"""
===============================================================================
                    ğŸ”¬ ç ”ç©¶æ™ºèƒ½ä½“ (Researcher Agent) v4.2 (Hardcore Edition)
===============================================================================
æ ¸å¿ƒç­–ç•¥ï¼š
1. æ™ºèƒ½èšåˆæœç´¢ï¼šExa AI (ä¼˜å…ˆ) + Tavily (å…œåº•)ï¼Œå…¨ç½‘æ·±åº¦æŒ–æ˜ã€‚
2. æ‰¹åˆ¤æ€§è¯„ä¼°è¿‡æ»¤å™¨ï¼šåœ¨ç¬”è®°æ•´ç†é˜¶æ®µï¼Œè‡ªåŠ¨è¯†åˆ«å¹¶æ ‡è®°â€œæ™ºå•†ç¨â€å·¥å…·ã€‚
3. åå¥—å£³æœºåˆ¶ï¼šå¼ºåˆ¶æå–åº•å±‚æŠ€æœ¯åŸç†ï¼Œæ‹’ç»è¥é”€è½¯æ–‡ã€‚
4. v4.2 æ–°å¢ï¼šFast Research æŒ‡å¼•è§£æ + ç²¾å‡†æœç´¢æŸ¥è¯¢ç”Ÿæˆ
===============================================================================
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import httpx
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from typing import Optional, List, Dict, Any
from pathlib import Path
from openai import OpenAI
from tavily import TavilyClient
from config import (
    DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL, 
    TAVILY_API_KEY, EXA_API_KEY, PERPLEXITY_API_KEY,
    PROXY_URL, REQUEST_TIMEOUT, get_research_notes_file, get_logger, retryable, track_cost
)


logger = get_logger(__name__)


class ResearcherAgent:
    """è‡ªåŠ¨åŒ–ç ”ç©¶æ™ºèƒ½ä½“ï¼šExa AI æœç´¢ + å†…å®¹èšåˆ + ç¬”è®°æ•´ç†"""
    
    def __init__(self):
        # åˆå§‹åŒ– DeepSeek å®¢æˆ·ç«¯
        proxy_url = PROXY_URL
        self.client = OpenAI(
            api_key=DEEPSEEK_API_KEY,
            base_url=DEEPSEEK_BASE_URL,
            http_client=httpx.Client(proxy=proxy_url, timeout=REQUEST_TIMEOUT)
        )
        
        # åˆå§‹åŒ–å„æœç´¢ API çŠ¶æ€
        self.tavily_key = TAVILY_API_KEY
        self.pplx_key = PERPLEXITY_API_KEY
        self.exa_key = EXA_API_KEY
        self.proxy_url = proxy_url
        
        self.pplx_enabled = bool(self.pplx_key and len(self.pplx_key) > 10)
        self.tavily_enabled = bool(self.tavily_key and len(self.tavily_key) > 10)
        self.exa_enabled = bool(self.exa_key and len(self.exa_key) > 10)
        
        logger.info("âœ… ResearcherAgent v4.3 åˆå§‹åŒ–å®Œæˆ (Priority: Perplexity -> Tavily -> Exa)")

    def search_perplexity(self, query: str) -> List[Dict[str, Any]]:
        """Perplexity API: è·å–æ¨¡å‹ç”Ÿæˆçš„æ‘˜è¦ä½œä¸ºæ ¸å¿ƒç ”ç©¶ç´ æ"""
        if not self.pplx_enabled: return []
        logger.info("ğŸ” [Step 1.1] Perplexity æ·±åº¦æœç´¢æ‘˜è¦...")
        url = "https://api.perplexity.ai/chat/completions"
        payload = {
            "model": "sonar",
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIç ”ç©¶åŠ©æ‰‹ã€‚è¯·é’ˆå¯¹ç”¨æˆ·çš„æŸ¥è¯¢æä¾›è¯¦ç»†ã€å‡†ç¡®ä¸”å¸¦æœ‰æ¥æºæ‘˜è¦çš„å›ç­”ã€‚è¾“å‡ºåº”åŒ…å«æ ¸å¿ƒæŠ€æœ¯ç‚¹ã€è¡Œä¸šè¶‹åŠ¿ã€çœŸå®æ¡ˆä¾‹ä»¥åŠç”¨æˆ·ç—›ç‚¹ã€‚"},
                {"role": "user", "content": query}
            ],
            "temperature": 0.2,
            "search_recency_filter": "week"
        }
        headers = {
            "Authorization": f"Bearer {self.pplx_key}",
            "Content-Type": "application/json"
        }
        try:
            with httpx.Client(timeout=45, proxy=self.proxy_url) as client:
                @retryable
                @track_cost(context="perplexity_research")
                def _post():
                    return client.post(url, json=payload, headers=headers)
                
                resp = _post()
                if resp.status_code != 200:
                    logger.warning(f"Perplexity æŠ¥é”™: {resp.status_code}")
                    return []
                
                data = resp.json()
                content = data['choices'][0]['message']['content']
                return [{
                    "url": "https://perplexity.ai",
                    "title": "Perplexity AI Research Summary",
                    "text": content,
                    "source": "Perplexity"
                }]
        except Exception as e:
            logger.error(f"Perplexity è°ƒç”¨å¤±è´¥: {e}")
            return []

    def search_exa(self, topic: str, queries: List[str]) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ Exa AI è¿›è¡Œé«˜çº§æœç´¢ (è‡ªåŠ¨åŒ…å«å†…å®¹)
        v4.2: å¢å¼ºæŸ¥è¯¢åˆ©ç”¨ï¼Œä½¿ç”¨ queries è¿›è¡Œå¤šæ‰¹æ¬¡ç²¾å‡†æœç´¢
        """
        if not self.exa_api_key:
            logger.warning("æœªé…ç½® EXA_API_KEYï¼Œè·³è¿‡ Exa æœç´¢")
            return []

        logger.info("ğŸ” [Step 1] Exa AI æ™ºèƒ½æœç´¢...")
        logger.info("   ğŸ“Œ ä¸»é¢˜: %s", topic)
        logger.info("   ğŸ”‘ æŸ¥è¯¢è¯: %s", queries[:5])
        
        all_results = []
        seen_urls = set()  # å»é‡
        headers = {
            "Authorization": f"Bearer {self.exa_api_key}",
            "Content-Type": "application/json"
        }
        
        # Exa API ç«¯ç‚¹
        url = "https://api.exa.ai/search"

        # å®šä¹‰æœç´¢æ‰¹æ¬¡
        # 1. ç¤¾äº¤åª’ä½“ä¸“é¡¹ (æŒ‡å®šåŸŸå)
        social_domains = [
            "mp.weixin.qq.com", "weibo.com", 
            "xiaohongshu.com", "v2ex.com", "juejin.cn"
        ]
        
        batches = [
            # Batch 1: é’ˆå¯¹ç¤¾äº¤åª’ä½“çš„ç²¾å‡†æœç´¢ (ä½¿ç”¨ä¸»é¢˜)
            {
                "query": f"{topic} æ·±åº¦è§£æ é¿å‘æŒ‡å— æ•™ç¨‹",
                "numResults": 5,
                "includeDomains": social_domains,
                "useAutoprompt": True,
                "contents": {"text": True}
            },
            # Batch 2: å…¨ç½‘é€šç”¨æœç´¢ (ä½¿ç”¨ä¸»é¢˜)
            {
                "query": topic,
                "numResults": 3,
                "useAutoprompt": True,
                "contents": {"text": True}
            }
        ]
        
        # v4.2: ä¸ºæ¯ä¸ªç²¾å‡†æŸ¥è¯¢è¯æ·»åŠ æœç´¢æ‰¹æ¬¡
        for q in queries[:4]:  # æœ€å¤šå–å‰4ä¸ªæŸ¥è¯¢è¯
            if q != topic and len(q) > 5:
                batches.append({
                    "query": q,
                    "numResults": 3,
                    "useAutoprompt": True,
                    "contents": {"text": True}
                })
        
        @retryable
        def _exa_post(client: httpx.Client, payload: dict, headers: dict):
            return client.post(url, json=payload, headers=headers)

        with httpx.Client(timeout=60, proxy=self.proxy_url) as client:
            for i, payload in enumerate(batches):
                try:
                    logger.info("ğŸš€ Exa Batch %s è¯·æ±‚ä¸­ (query: %s)...", i + 1, payload.get('query', '')[:30])
                    resp = _exa_post(client, payload, headers)
                    resp.raise_for_status()
                    data = resp.json()
                    
                    results = data.get("results", [])
                    for res in results:
                        res_url = res.get("url", "")
                        # v4.2: URL å»é‡
                        if res_url in seen_urls:
                            continue
                        seen_urls.add(res_url)
                        
                        all_results.append({
                            "url": res_url,
                            "title": res.get("title"),
                            "text": res.get("text", ""),
                            "source": "Exa"
                        })
                        logger.info("âœ“ [Exa] %s...", (res.get('title', 'Unknown') or '')[:40])
                        
                except Exception as e:
                    logger.error("âŒ Exa Batch %s å¤±è´¥: %s", i + 1, e)

        logger.info("   ğŸ“Š Exa å…±è·å– %d æ¡å»é‡ç»“æœ", len(all_results))
        return all_results

    def search_tavily_fallback(self, queries: List[str]) -> List[Dict[str, Any]]:
        """
        Tavily å¤‡ç”¨æœç´¢ (ä»…è·å– URLï¼Œæ— æ­£æ–‡)
        """
        if not self.tavily_enabled: return []
        logger.info("ğŸ”„ [Fallback] åˆ‡æ¢è‡³ Tavily å¹¶å‘æœç´¢...")
        
        from tavily import TavilyClient
        tavily_client = TavilyClient(api_key=self.tavily_key)
        
        all_results = []
        seen_urls = set()
        
        # æ„é€ æŸ¥è¯¢
        extended_queries = []
        for q in queries:
            extended_queries.append({"q": q, "type": "general"})
            extended_queries.append({"q": f"{q} site:mp.weixin.qq.com", "type": "wechat"})
            extended_queries.append({"q": f"{q} site:xiaohongshu.com", "type": "xhs"})
        
        @retryable
        def _tavily_search(query: str, limit: int):
            return tavily_client.search(
                query=query,
                search_depth="advanced",
                max_results=limit,
                days=30
            )

        def do_search(item):
            try:
                limit = 2 if item['type'] == "general" else 1
                resp = _tavily_search(item['q'], limit)
                return resp.get('results', [])
            except Exception as e:
                logger.warning("Tavily æœç´¢å¤±è´¥: %s", e)
                return []

        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(do_search, item) for item in extended_queries]
            for future in as_completed(futures):
                for res in future.result():
                    if res['url'] not in seen_urls and "pdf" not in res['url']:
                        seen_urls.add(res['url'])
                        all_results.append({
                            "url": res['url'],
                            "title": res['title'],
                            "text": "", # Tavily ä¸å«å…¨æ–‡ï¼Œéœ€åç»­çˆ¬å–
                            "source": "Tavily"
                        })
                        logger.info("âœ“ [Tavily] %s...", (res.get('title', '') or '')[:40])
        
        return all_results[:8]

    def scrape_missing_content(self, items: List[Dict[str, Any]]) -> None:
        """
        å¯¹ç¼ºå°‘æ­£æ–‡çš„æ¡ç›® (å¦‚æ¥è‡ª Tavily) è¿›è¡Œè¡¥å……çˆ¬å–
        ä½¿ç”¨ Jina Reader + Fallback
        """
        missing_items = [i for i in items if not i.get("text") or len(i.get("text")) < 200]
        if not missing_items:
            return
            
        logger.info("ğŸ“– [Step 2] è¡¥å……çˆ¬å– %s ä¸ªé¡µé¢ (Jina/Fallback)...", len(missing_items))
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        
        crawled_texts = []
        
        @retryable
        def _http_get(client: httpx.Client, url: str, headers: Optional[dict] = None):
            return client.get(url, headers=headers)

        with httpx.Client(timeout=60, proxy=self.proxy_url, follow_redirects=True) as client:
            for item in missing_items:
                url = item['url']
                logger.info("ğŸŒ çˆ¬å–: %s...", (item.get('title', '') or '')[:30])
                
                try:
                    # Jina
                    jina_resp = _http_get(client, f"https://r.jina.ai/{url}", headers=headers)
                    if jina_resp.status_code == 200 and len(jina_resp.text) > 500:
                        item['text'] = jina_resp.text
                        logger.info("âœ“ Jina æˆåŠŸ")
                        continue
                except:
                    pass
                
                try:
                    # Direct Fallback
                    raw_resp = _http_get(client, url, headers=headers)
                    if raw_resp.status_code == 200:
                        # æå…¶ç®€é™‹çš„æ–‡æœ¬æå–
                        from bs4 import BeautifulSoup
                        soup = BeautifulSoup(raw_resp.text, 'html.parser')
                        for s in soup(['script', 'style']): s.extract()
                        item['text'] = soup.get_text()[:10000]
                        logger.info("âœ“ ç›´è¿æˆåŠŸ")
                        continue
                except Exception as e:
                    pass

                # 3. Tavily å…œåº• (ä½œä¸ºæå–å™¨)
                try:
                    if self.tavily:
                        #ä»¥æ­¤ URL ä¸º query è¿›è¡Œæœç´¢ï¼Œå¹¶è¯·æ±‚ raw_content
                        tavily_resp = self.tavily.search(
                            query=url,
                            include_raw_content=True,
                            max_results=1
                        )
                        if tavily_resp and 'results' in tavily_resp and tavily_resp['results']:
                            raw_content = tavily_resp['results'][0].get('raw_content')
                            if raw_content:
                                item['text'] = raw_content[:10000]
                                logger.info("âœ“ Tavily å…œåº•æˆåŠŸ (Raw Content)")
                                continue
                except Exception as e:
                    logger.error("âŒ Tavily å…œåº•å¤±è´¥: %s", e)
                
                logger.error("âŒ æ‰€æœ‰è·å–æ‰‹æ®µå‡å¤±è´¥")

    def synthesize_notes(self, items: List[Dict[str, Any]], topic: str, strategic_intent: Optional[str] = None, imitation_source: str = "") -> str:
        """
        æ•´ç†æ‰€æœ‰ç´ æä¸ºç¬”è®° (å¸¦æ‰¹åˆ¤æ€§è¯„ä¼°è¿‡æ»¤å™¨)
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        1. å”¯æŠ€æœ¯è®ºï¼šåªæå–åº•å±‚æŠ€æœ¯/Prompt/æœ¬åœ°éƒ¨ç½²ç›¸å…³å†…å®¹ã€‚
        2. é„™è§†å¥—å£³ï¼šè¯†åˆ«å¹¶æ ‡è®°å›½å†…ä»˜è´¹å¥—å£³å·¥å…·ä¸ºâ€œé¿å‘é»‘åå•â€ã€‚
        """
        logger.info("ğŸ“ [Step 3] AI æ•´ç†ç¬”è®°...")
        
        # èšåˆæ‰€æœ‰æ–‡æœ¬
        combined_text = ""
        if imitation_source:
            combined_text += f"=== ä»¿å†™åŸæ–‡ç´ æ (é‡ç‚¹å‚è€ƒ) ===\n{imitation_source}\n\n"
        
        for item in items:
            text = item.get("text", "")
            if not text or len(text.strip()) < 50:
                logger.warning(f"âš ï¸ [å†…å®¹ç¼ºå¤±] å¿½ç•¥æ¡ç›®: {item.get('title', 'Unknown')} (æ— æ­£æ–‡)")
                continue
                
            if len(text) > 100:
                combined_text += f"\n{'='*50}\nSource: {item['url']}\nTitle: {item.get('title')}\n{'='*50}\n{text[:8000]}\n"
        
        if not combined_text:
            return "# ç ”ç©¶å¤±è´¥ï¼šæœªè·å–åˆ°æœ‰æ•ˆå†…å®¹"

        strategic_block = ("\n\n" + "="*20 + "\n" + "ã€é€‰é¢˜ç­–åˆ’ä¹¦ / æˆ˜ç•¥æ„å›¾ï¼ˆæœ€é«˜æŒ‡ä»¤ï¼‰ã€‘\n" + (strategic_intent or "") + "\n" + "="*20 + "\n") if strategic_intent else ""

        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šå†…å®¹ç ”ç©¶å‘˜å’Œèµ„æ·±æŠ€æœ¯åšä¸»ã€‚è¯·æ ¹æ®ä»¥ä¸‹å¤šç¯‡æ¥æºæ–‡ç« ï¼Œä¸ºå…¬ä¼—å·æ–‡ç« ã€Š{topic}ã€‹æ•´ç†ç´ æã€‚{strategic_block}
        
        âš ï¸ **æµé‡ä¸ç¤¾äº¤è°ƒç ”å¢å¼º (Social Packaging)**ï¼š
        1. **æœé›†çˆ†æ¬¾è§’åº¦**ï¼šé™¤äº†æŠ€æœ¯å®ç°ï¼Œå¿…é¡»æŒ–æ˜è¯¥è¯é¢˜åœ¨ç¤¾äº¤åª’ä½“ï¼ˆå°çº¢ä¹¦/å¾®åš/å…¬ä¼—å·ï¼‰ä¸Šçš„â€œçˆ†æ¬¾å› å­â€ã€‚
        2. **ç¥è¯„è®ºä¸åæ§½**ï¼šå¯»æ‰¾ç”¨æˆ·å¯¹è¯¥å·¥å…·/ç°è±¡çš„æœ€çœŸå®åæ§½ã€ç¥è¯„è®ºæˆ–ä½“æ„Ÿå˜åŒ–æè¿°ã€‚
        3. **è½¬å‘åŠ¨æœº**ï¼šåˆ†æä¸ºä»€ä¹ˆæ™®é€šäººä¼šæƒ³è½¬å‘è¿™ç¯‡æ–‡ç« ï¼Ÿï¼ˆæ˜¯å› ä¸ºèƒ½çœé’±ã€èƒ½è£…é€¼ã€è¿˜æ˜¯èƒ½é¿å‘ï¼Ÿï¼‰

        âš ï¸ **æ ¸å¿ƒä»·å€¼è§‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰**ï¼š
        1. **å”¯æŠ€æœ¯è®º**ï¼šæˆ‘ä»¬åªæ¨å´‡**åº•å±‚å¤§æ¨¡å‹ï¼ˆDeepSeek, ChatGPT, Claudeï¼‰** + **æç¤ºè¯å·¥ç¨‹**ã€‚
        2. **é„™è§†å¥—å£³**ï¼šæ‰€æœ‰å›½å†…å·ç§°â€œä¸€é”®é™é‡â€ã€â€œAIå†™ä½œç¥å™¨â€çš„ä»˜è´¹å·¥å…·ï¼ˆå¦‚**ç¬”çµã€å°å‘çŒ«ã€PaperYYã€å˜å˜**ç­‰ï¼‰ï¼Œä¸€å¾‹è§†ä¸º**â€œä½æŠ€æœ¯å«é‡çš„å¥—å£³äº§å“â€**ã€‚
        3. **è¯æ®æœé›†**ï¼šå¦‚æœç´ æä¸­å¹æ§è¿™äº›ä»˜è´¹å·¥å…·ï¼Œè¯·å¿½ç•¥å…¶è¥é”€è¯æœ¯ï¼Œé‡ç‚¹æŒ–æ˜å®ƒä»¬çš„ç¼ºç‚¹ï¼ˆå¦‚æ”¶è´¹è´µã€åº•å±‚æ¨¡å‹æœªçŸ¥ã€éšç§é£é™©ï¼‰ã€‚

        - ä½ çš„è¾“å‡ºå¿…é¡»æ˜¾å¼è¦†ç›–ç­–åˆ’ä¹¦ä¸­çš„ï¼š
          1) ä¸€å¥è¯å–ç‚¹ï¼ˆè¯»è€…è·å¾—æ„Ÿï¼‰
          2) å¿ƒç†é”šç‚¹ï¼ˆè¯»è€…ä¸ºä»€ä¹ˆä¼šç‚¹å¼€/ä¼šç„¦è™‘ä»€ä¹ˆï¼‰
          3) æ ¸å¿ƒçœ‹ç‚¹ï¼ˆæ–‡ç« å¿…é¡»è¦†ç›–çš„è¦ç‚¹/ç»“æ„ï¼‰
        - å¦‚æœç´ æä¸ç­–åˆ’ä¹¦å†²çªï¼šä¼˜å…ˆä¿ç•™â€œå¯è¯æ®æ”¯æŒâ€çš„å†…å®¹ï¼Œå¹¶åœ¨ç¬”è®°ä¸­æ ‡æ³¨â€œä¸ç­–åˆ’ä¹¦å‡è®¾ä¸ä¸€è‡´â€ã€‚

        âš ï¸ **ä¿¡æ¯ä¸è¶³å…œåº•ï¼ˆå¿…é¡»æ‰§è¡Œï¼‰**ï¼š
        - å¦‚æœâ€œé€‰é¢˜ç­–åˆ’ä¹¦â€é‡Œç‚¹åäº†æŸä¸ªå…·ä½“å·¥å…·/é¡¹ç›®ï¼ˆä¾‹å¦‚ block/gooseï¼‰ï¼Œä½†åœ¨ç´ æä¸­æœä¸åˆ°è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·åœ¨ç¬”è®°ä¸­æ˜ç¡®æ ‡æ³¨ï¼š**â€œä¿¡æ¯ä¸è¶³â€**ï¼Œå¹¶è§£é‡Šç¼ºå¤±ç‚¹ï¼ˆå¦‚ï¼šç¼ºå®˜æ–¹æ–‡æ¡£/ç¼ºçœŸå®ä½“éªŒ/ç¼ºè¿‘æœŸæ›´æ–°ï¼‰ã€‚
        - åŒæ—¶ï¼Œä½ å¿…é¡»ä¸»åŠ¨å¯»æ‰¾ä¸€ä¸ªâ€œåŒç±»å‹çš„ GitHub é«˜æ˜Ÿé¡¹ç›® / å®˜æ–¹æ›¿ä»£æ–¹æ¡ˆâ€ä½œä¸ºå¤‡é€‰ï¼Œå¹¶å†™æ¸…æ¥šï¼š
          1) ä¸ºä»€ä¹ˆå®ƒæ˜¯åŒç±»å‹
          2) å®ƒçš„æ ¸å¿ƒèƒ½åŠ›
          3) å®ƒä¸ç­–åˆ’ä¹¦ç›®æ ‡çš„åŒ¹é…åº¦ï¼ˆå–ç‚¹/é”šç‚¹/æ ¸å¿ƒçœ‹ç‚¹ï¼‰
        - ç›®çš„ï¼šä¸å…è®¸å†™ä½œç«¯å‡ºç°â€œå¼€å¤©çª—â€ã€‚
        
        è¯·æå–ï¼š
        1. **## ç¤¾äº¤è´§å¸ä¸èˆ†æƒ…åˆ†æ**ï¼šåŒ…å« 3 ä¸ªçˆ†æ¬¾è§’åº¦ã€ç”¨æˆ·ç¥è¯„è®º/åæ§½ã€ä»¥åŠè¯»è€…çš„è½¬å‘åŠ¨æœºåˆ†æã€‚
        2. **æ ¸å¿ƒè§‚ç‚¹ (æ ‡æ³¨æ¥æº)**ï¼šé‡ç‚¹å…³æ³¨â€œåœºæ™¯â€è€Œéâ€œå‚æ•°â€ã€‚
        3. **å…³é”®æ•°æ®/æ¡ˆä¾‹ (æ ‡æ³¨æ¥æº)**ï¼šç‰¹åˆ«æ˜¯é‚£äº›å…·æœ‰â€œè§†è§‰å†²å‡»åŠ›â€æˆ–â€œæˆå‰§æ€§ç»“æœâ€çš„æ¡ˆä¾‹ã€‚
        4. **é¿å‘é»‘åå•**ï¼šå°†æ‰€æœ‰â€œä»˜è´¹å¥—å£³å·¥å…·â€å½’å…¥æ­¤ç±»ï¼Œå¹¶è¯´æ˜ç†ç”±ï¼ˆæ™ºå•†ç¨ï¼‰ã€‚
        5. **çœŸæ­£çš„é«˜é˜¶ç©æ³•**ï¼šå¯»æ‰¾å…³äº**Promptä¼˜åŒ–ã€å¤šæ¨¡å‹äº¤å‰éªŒè¯ã€æœ¬åœ°éƒ¨ç½²(Ollama)**ç­‰ç¡¬æ ¸å†…å®¹ã€‚
        
        è¾“å‡ºä¸ºæ¸…æ™°çš„ Markdown æ ¼å¼ã€‚
        """

        try:
            @retryable
            @track_cost(context="synthesize_notes")
            def _chat_create():
                return self.client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": prompt},
                        {"role": "user", "content": f"ç´ æå†…å®¹ï¼š\n{combined_text[:60000]}"} # æ§åˆ¶æ€»é•¿åº¦
                    ],
                    temperature=0.3,
                    max_tokens=4000,
                    stream=True
                )

            response = _chat_create()

            logger.info("%s", "="*20 + " ç¬”è®°ç”Ÿæˆä¸­ " + "="*20)

            collected = []
            for chunk in response:
                content = chunk.choices[0].delta.content
                if content:
                    sys.stdout.write(content)
                    sys.stdout.flush()
                    collected.append(content)
            sys.stdout.write("\n")
            sys.stdout.flush()

            return "".join(collected)

        except Exception as e:
            logger.error("âŒ æ•´ç†å¤±è´¥: %s", e)
            return f"æ•´ç†å¤±è´¥: {e}"


    def _generate_search_queries_from_fast_research(self, fast_research: str, topic: str) -> List[str]:
        """
        v4.2: ä» Fast Research æŒ‡å¼•ä¸­æå–ç²¾å‡†æœç´¢æŸ¥è¯¢
        ä½¿ç”¨ LLM å°†ç»“æ„åŒ–æŒ‡å¼•è½¬æ¢ä¸ºå®é™…æœç´¢æŸ¥è¯¢
        """
        logger.info("ğŸ§  [Step 0] ä» Fast Research æŒ‡å¼•ç”Ÿæˆç²¾å‡†æœç´¢æŸ¥è¯¢...")
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæœç´¢æŸ¥è¯¢ç”Ÿæˆä¸“å®¶ã€‚æ ¹æ®ä»¥ä¸‹"ç ”ç©¶æŒ‡å¼•"ï¼Œç”Ÿæˆ 5-8 ä¸ªç²¾å‡†çš„æœç´¢æŸ¥è¯¢è¯ã€‚

ã€ç ”ç©¶æŒ‡å¼•ã€‘
{fast_research}

ã€æ–‡ç« ä¸»é¢˜ã€‘
{topic}

ã€è¾“å‡ºè¦æ±‚ã€‘
1. æ¯è¡Œä¸€ä¸ªæœç´¢æŸ¥è¯¢ï¼Œä¸è¦ç¼–å·
2. æŸ¥è¯¢è¦å…·ä½“ã€ç²¾å‡†ï¼Œèƒ½æ‰¾åˆ°é«˜è´¨é‡çš„æŠ€æœ¯æ–‡ç« /æ•™ç¨‹/è¯„æµ‹
3. ä¼˜å…ˆåŒ…å«ï¼šé¡¹ç›®åç§°ã€æŠ€æœ¯æœ¯è¯­ã€æ•™ç¨‹/è¯„æµ‹/å¯¹æ¯”ç­‰å…³é”®è¯
4. é¿å…è¿‡äºå®½æ³›çš„æŸ¥è¯¢

ç›´æ¥è¾“å‡ºæœç´¢æŸ¥è¯¢åˆ—è¡¨ï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼š"""

        try:
            @retryable
            @track_cost(context="generate_search_queries")
            def _chat_create():
                return self.client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.3,
                    max_tokens=500
                )
            
            response = _chat_create()
            queries_text = response.choices[0].message.content.strip()
            queries = [q.strip() for q in queries_text.split('\n') if q.strip() and len(q.strip()) > 3]
            
            logger.info("   âœ… ç”Ÿæˆäº† %d ä¸ªç²¾å‡†æœç´¢æŸ¥è¯¢", len(queries))
            for q in queries[:5]:
                logger.info("      - %s", q[:50])
            
            return queries
        except Exception as e:
            logger.error("   âŒ æŸ¥è¯¢ç”Ÿæˆå¤±è´¥: %s", e)
            return []

    def run(self, topic: str, queries: List[str], strategic_intent: Optional[str] = None, fast_research: Optional[str] = None, dry_run: bool = False) -> str:
        logger.info("%s", "="*60)
        logger.info("ğŸ”¬ ResearcherAgent v4.3 (Multi-Search)%s", " (ğŸ§ª DRY RUN)" if dry_run else "")
        logger.info("ğŸ“Œ é€‰é¢˜: %s", topic)
        logger.info("%s", "="*60)

        if dry_run:
            # ... (keep dry run logic)
            logger.info("ğŸ§ª [Mock] æ­£åœ¨ç”Ÿæˆæ¨¡æ‹Ÿç ”ç©¶ç¬”è®°...")
            mock_notes = f"""
## 1. ç¤¾äº¤è´§å¸ä¸èˆ†æƒ…åˆ†æ
- **çˆ†æ¬¾è§’åº¦**ï¼šCursor çš„éšè—è®¾ç½®æ˜¯å…¸å‹çš„â€œä¿¡æ¯å·®â€çº¢åˆ©ï¼Œæ™®é€šäººè¿˜åœ¨æœæŒ‡ä»¤ï¼Œé«˜æ‰‹å·²ç»åœ¨ç«‹è§„çŸ©ã€‚
- **ç¥è¯„è®º**ï¼š'å¼€äº†è¿™å‡ ä¸ªå¼€å…³ï¼ŒCursor ç»ˆäºä¸ä¹±åˆ æˆ‘ä»£ç äº†ï¼'
- **è½¬å‘åŠ¨æœº**ï¼šé¿å‘ã€çœé’±ã€ææ•ˆã€‚

## 2. æ ¸å¿ƒè§‚ç‚¹
- Cursor Rules (.mdc) æ˜¯æ§åˆ¶ AI çš„æ ¸å¿ƒã€‚
- MCP Server è®© AI å…·å¤‡å®æ—¶è”ç½‘èƒ½åŠ›ã€‚

## 3. å…³é”®æ•°æ®/æ¡ˆä¾‹
- å¼€å¯ Rules åï¼Œä»£ç é‡æ„å‡ºé”™ç‡é™ä½ 60%ã€‚

## 4. é¿å‘é»‘åå•
- ä¸¥ç¦æ— è„‘ Accept Allã€‚
- æ‹’ç»ä½¿ç”¨å›½å†…æ˜‚è´µçš„å¥—å£³å·¥å…·ã€‚

## 5. çœŸæ­£çš„é«˜é˜¶ç©æ³•
- ä½¿ç”¨ .cursor/rules å®šä¹‰é¡¹ç›®çº§è§„èŒƒã€‚
- æ¥å…¥ Perplexity MCP è·å–æœ€æ–° API æ–‡æ¡£ã€‚
"""
            # ä¿å­˜ Mock ç¬”è®°
            notes_file = get_research_notes_file()
            with open(notes_file, "w", encoding="utf-8") as f:
                intent_section = f"\n\n## ğŸ¯ æˆ˜ç•¥æ„å›¾æ‘˜è¦\n\n{strategic_intent.strip()}\n" if strategic_intent else ""
                f.write(f"# ğŸ”¬ è‡ªåŠ¨ç ”ç©¶ç¬”è®° v4.3 (ğŸ§ª Mock)\n\n**é€‰é¢˜**: {topic}\n**æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n{intent_section}\n---\n\n{mock_notes}")
            logger.info("ğŸ“ [Mock] ç¬”è®°å·²ä¿å­˜: %s", notes_file)
            return mock_notes

        # v4.2: å¦‚æœæœ‰ Fast Research æŒ‡å¼•ï¼Œç”Ÿæˆæ›´ç²¾å‡†çš„æœç´¢æŸ¥è¯¢
        if fast_research:
            generated_queries = self._generate_search_queries_from_fast_research(fast_research, topic)
            if generated_queries:
                queries = generated_queries + queries  # åˆå¹¶ï¼šç²¾å‡†æŸ¥è¯¢ä¼˜å…ˆ
                queries = list(dict.fromkeys(queries))[:10]  # å»é‡ï¼Œé™åˆ¶æ•°é‡

        results = []
        
        # 1. é¦–é€‰ Perplexity è·å–æ‘˜è¦
        pplx_results = self.search_perplexity(topic)
        if pplx_results:
            results.extend(pplx_results)
            logger.info("   âœ… å·²è·å– Perplexity ç ”ç©¶æ‘˜è¦")

        # 2. æ— è®ºæ˜¯å¦æœ‰ pplxï¼Œéƒ½é€šè¿‡ Tavily æˆ– Exa è·å–æ›´å¤šå‚è€ƒé“¾æ¥å’Œæ­£æ–‡
        # ä¼˜å…ˆ Tavily (å› ä¸ºå¿«ä¸”ç¨³)ï¼ŒExa ä½œä¸ºå…œåº•
        search_results = []
        if self.tavily_enabled:
            search_results = self.search_tavily_fallback(queries)
        
        # 3. å¦‚æœ Tavily å¤±è´¥æˆ–æ²¡ç»“æœï¼Œå°è¯• Exa å…œåº•
        if not search_results and self.exa_enabled:
            search_results = self.search_exa(topic, queries)
            
        results.extend(search_results)

        if not results:
            logger.warning("âš ï¸ æ‰€æœ‰æœç´¢é€šé“å‡æœªæ‰¾åˆ°æœ‰æ•ˆå†…å®¹")
            return ""

        # 4. è¡¥å……çˆ¬å– (é’ˆå¯¹ Tavily æ¥æºæˆ– Exa æ²¡æŠ“åˆ°æ­£æ–‡çš„)
        # æ³¨æ„ï¼šPerplexity ç»“æœå·²ç»è‡ªå¸¦ content (text å­—æ®µ)ï¼Œä¸éœ€è¦çˆ¬å–
        self.scrape_missing_content(results)

        # v5.2: æ£€æŸ¥æ˜¯å¦æœ‰ä»¿å†™åŸæ–‡ç´ æï¼Œå¦‚æœæœ‰ï¼Œå°†å…¶åŠ å…¥ç ”ç©¶èƒŒæ™¯
        imitation_source = ""
        from config import get_stage_dir
        source_file = Path(get_stage_dir("research")) / "imitation_source.txt"
        if source_file.exists():
            try:
                imitation_source = source_file.read_text(encoding="utf-8")
                logger.info("   ğŸ“„ å‘ç°ä»¿å†™åŸæ–‡ç´ æï¼Œå·²åŠ å…¥ç ”ç©¶èƒŒæ™¯")
            except Exception as e:
                logger.warning("   âš ï¸ è¯»å–ä»¿å†™ç´ æå¤±è´¥: %s", e)

        # 5. æ•´ç†ç¬”è®°
        notes = self.synthesize_notes(results, topic, strategic_intent=strategic_intent, imitation_source=imitation_source)

        # ä¿å­˜
        notes_file = get_research_notes_file()
        with open(notes_file, "w", encoding="utf-8") as f:
            intent_section = f"\n\n## ğŸ¯ æˆ˜ç•¥æ„å›¾æ‘˜è¦\n\n{strategic_intent.strip()}\n" if strategic_intent else ""
            f.write(f"# ğŸ”¬ è‡ªåŠ¨ç ”ç©¶ç¬”è®° v4.3\n\n**é€‰é¢˜**: {topic}\n**æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n{intent_section}\n---\n\n{notes}")

        logger.info("ğŸ“ ç¬”è®°å·²ä¿å­˜: %s", notes_file)
        return notes

def main():
    agent = ResearcherAgent()
    agent.run("DeepSeek V3 éšè—åŠŸèƒ½", ["DeepSeek æ•™ç¨‹"])

if __name__ == "__main__":
    main()
