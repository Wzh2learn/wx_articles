"""
ğŸš€ å…¨ç½‘é€‰é¢˜é›·è¾¾ (Trend Hunter Agent) v4.0 (Hardcore Edition)
æ ¸å¿ƒç­–ç•¥ï¼š
1. ä¸‰çº§å®¹é”™æœºåˆ¶ï¼šJina Primary -> Jina Backup (RSS) -> Tavily Searchï¼Œç¡®ä¿æ•°æ®æºç¨³å®šã€‚
2. éšæœºåŒ–æ‰«æï¼šBè·¯(æ•ˆç‡)ä¸Cè·¯(é¿å‘)é‡‡ç”¨éšæœºæŠ½å–ç­–ç•¥ï¼Œé¿å…é‡å¤ã€‚
3. ä¸¥æ ¼å»é‡ï¼šåŸºäºå†å²è®°å½•çš„è‡ªåŠ¨å»é‡ä¸æ–°è¯æ‰¶æŒæœºåˆ¶ã€‚
"""
import sys, os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import time
import json
import re
import httpx
import random
from pathlib import Path
from difflib import SequenceMatcher
from json_repair import repair_json
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from typing import List, Dict, Optional, Any, Tuple
from bs4 import BeautifulSoup
from openai import OpenAI
from config import (
    DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL, PROXY_URL, REQUEST_TIMEOUT,
    TAVILY_API_KEY, PERPLEXITY_API_KEY, EXA_API_KEY, get_topic_report_file, get_today_dir,
    get_stage_dir, get_research_notes_file, get_history_file, get_logger, retryable,
    track_cost, WATCHLIST, TREND_SOURCES, OPERATIONAL_PHASE, PHASE_CONFIG,
    EFFICIENCY_KEYWORDS, PAIN_KEYWORDS, RADAR_QUERIES,
    MAX_CONCURRENT_FETCHES, FETCH_TIMEOUT_SECONDS
)


logger = get_logger(__name__)


def log_print(*args, **kwargs):
    end = kwargs.get("end", "\n")
    flush = kwargs.get("flush", False)
    msg = " ".join(str(a) for a in args)

    if end == "" or flush:
        sys.stdout.write(msg + end)
        if flush:
            sys.stdout.flush()
        return

    if "âŒ" in msg:
        logger.error(msg)
    elif "âš ï¸" in msg or "ğŸ›¡ï¸" in msg:
        logger.warning(msg)
    else:
        logger.info(msg)

# ================= å†å²è®°å½•ç®¡ç† =================

def load_history():
    """åŠ è½½æœ€è¿‘ 7 å¤©çš„å†å²é€‰é¢˜"""
    history_file = get_history_file()
    if not os.path.exists(history_file):
        return []
    
    try:
        with open(history_file, "r", encoding="utf-8") as f:
            history = json.load(f)
            
        # è¿‡æ»¤å‡ºæœ€è¿‘ 7 å¤©çš„
        recent_history = []
        today = datetime.now()
        for item in history:
            date_str = item.get("date")
            try:
                item_date = datetime.strptime(date_str, "%Y-%m-%d")
                if (today - item_date).days <= 7:
                    recent_history.append(item)
            except:
                continue
        return recent_history
    except Exception:
        return []

def save_topic_to_history(topic, angle):
    """ä¿å­˜é€‰ä¸­é€‰é¢˜åˆ°å†å²è®°å½•"""
    history_file = get_history_file()
    history = []
    if os.path.exists(history_file):
        try:
            with open(history_file, "r", encoding="utf-8") as f:
                history = json.load(f)
        except:
            pass
            
    new_entry = {
        "date": datetime.now().strftime("%Y-%m-%d"),
        "topic": topic,
        "angle": angle
    }
    history.append(new_entry)
    
    # åªä¿ç•™æœ€è¿‘ 30 æ¡
    if len(history) > 30:
        history = history[-30:]
        
    with open(history_file, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)
    log_print(f"   ğŸ’¾ å†å²è®°å½•å·²æ›´æ–°: {topic}")

# ================= å»é‡ä¸ç›¸ä¼¼åº¦è¾…åŠ© =================

def _max_similarity_to_history(title: str, history_items: List[Dict[str, str]]) -> float:
    """è®¡ç®—æ ‡é¢˜ä¸å†å²è®°å½•çš„æœ€å¤§ç›¸ä¼¼åº¦"""
    if not title or not history_items:
        return 0.0
    sims = []
    for h in history_items:
        hist_title = (h.get("topic") or "").strip()
        if not hist_title:
            continue
        sims.append(SequenceMatcher(None, title.lower(), hist_title.lower()).ratio())
    return max(sims) if sims else 0.0

def _dedup_search_plan(search_plan: List[Dict[str, str]], history_items: List[Dict[str, str]], threshold: float = 0.82) -> List[Dict[str, str]]:
    """
    æ ¹æ®å†å²é€‰é¢˜åšç®€å•å»é‡ï¼Œè‹¥å…¨éƒ¨è¢«åˆ¤å®šä¸ºé‡å¤ï¼Œåˆ™å¼ºåˆ¶ä¿ç•™ç›¸ä¼¼åº¦æœ€ä½çš„ä¸€ä¸ªï¼Œé¿å…é¥¥é¥¿ã€‚
    """
    if not search_plan:
        return search_plan
    scored = []
    for item in search_plan:
        title = (item.get("event") or "").strip()
        max_sim = _max_similarity_to_history(title, history_items)
        new_item = dict(item)
        new_item["_max_sim"] = max_sim
        scored.append(new_item)
    deduped = [i for i in scored if i["_max_sim"] < threshold]
    if not deduped and scored:
        fallback = min(scored, key=lambda x: x["_max_sim"])
        log_print(f"âš ï¸ All topics were flagged as duplicates. Force-keeping the least similar one: {fallback.get('event', '(unknown)')}")
        deduped = [fallback]
    # ç§»é™¤å†…éƒ¨è¯„åˆ†å­—æ®µ
    for i in deduped:
        i.pop("_max_sim", None)
    return deduped

# ================= é…ç½®åŒº =================

CURRENT_CONFIG = PHASE_CONFIG[OPERATIONAL_PHASE]

# ================= æœç´¢å·¥å…· (å¤šçº§é™çº§) =================

class WebSearchTool:
    def __init__(self):
        self.tavily_key = TAVILY_API_KEY
        self.pplx_key = PERPLEXITY_API_KEY
        self.exa_key = EXA_API_KEY
        
        self.pplx_enabled = bool(self.pplx_key and len(self.pplx_key) > 10)
        self.tavily_enabled = bool(self.tavily_key and len(self.tavily_key) > 10)
        self.exa_enabled = bool(self.exa_key and len(self.exa_key) > 10)
        
        self.enabled = self.pplx_enabled or self.tavily_enabled or self.exa_enabled
        
        if self.pplx_enabled: log_print("   âœ… Perplexity API å·²å°±ç»ª (é¦–é€‰)")
        if self.tavily_enabled: log_print("   âœ… Tavily API å·²å°±ç»ª (å¤‡é€‰)")
        if self.exa_enabled: log_print("   âœ… Exa AI API å·²å°±ç»ª (å…œåº•)")

    def search(self, query, max_results=5, include_answer=True, topic=None, days=3):
        """
        å¤šçº§æœç´¢é™çº§é€»è¾‘: Perplexity -> Tavily -> Exa
        """
        if not self.enabled: return []

        # 1. é¦–é€‰ Perplexity
        if self.pplx_enabled:
            results = self._search_perplexity(query)
            if results: return results

        # 2. å¤‡é€‰ Tavily
        if self.tavily_enabled:
            results = self._search_tavily(query, max_results, include_answer, topic, days)
            if results: return results

        # 3. å…œåº• Exa
        if self.exa_enabled:
            results = self._search_exa(query, max_results)
            if results: return results

        return []

    def _search_perplexity(self, query):
        """Perplexity API: è·å–æ¨¡å‹ç”Ÿæˆçš„æ‘˜è¦ä½œä¸ºæ ¸å¿ƒç ”ç©¶ç´ æ"""
        log_print(f"   ğŸ” Perplexity æœç´¢: {query}")
        url = "https://api.perplexity.ai/chat/completions"
        payload = {
            "model": "sonar",
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIç ”ç©¶åŠ©æ‰‹ã€‚è¯·é’ˆå¯¹ç”¨æˆ·çš„æŸ¥è¯¢æä¾›è¯¦ç»†ã€å‡†ç¡®ä¸”å¸¦æœ‰æ¥æºæ‘˜è¦çš„å›ç­”ã€‚"},
                {"role": "user", "content": query}
            ],
            "temperature": 0.2,
            "top_p": 0.9,
            "search_domain_filter": None,
            "return_images": False,
            "return_related_questions": False,
            "search_recency_filter": "week",
            "top_k": 0,
            "stream": False,
            "presence_penalty": 0,
            "frequency_penalty": 1
        }
        headers = {
            "Authorization": f"Bearer {self.pplx_key}",
            "Content-Type": "application/json"
        }
        
        try:
            proxies = PROXY_URL if PROXY_URL else None
            with httpx.Client(timeout=45, proxy=proxies, trust_env=False) as client:
                @retryable
                @track_cost(context="perplexity_search")
                def _post():
                    return client.post(url, json=payload, headers=headers)
                
                resp = _post()
                if resp.status_code != 200:
                    log_print(f"      âš ï¸ Perplexity æŠ¥é”™: {resp.status_code}")
                    return None
                
                data = resp.json()
                content = data['choices'][0]['message']['content']
                # å°†ç”Ÿæˆçš„æ‘˜è¦ä½œä¸ºç¬¬ä¸€ä¸ªç»“æœè¿”å›ï¼Œbody è®¾ä¸ºå…¨æ–‡
                return [{"title": "Perplexity AI Summary", "body": content, "url": "https://perplexity.ai"}]
        except Exception as e:
            log_print(f"      âŒ Perplexity è°ƒç”¨å¤±è´¥: {e}")
            return None

    def _search_tavily(self, query, max_results=5, include_answer=False, topic=None, days=3):
        """åŸæœ‰çš„ Tavily æœç´¢é€»è¾‘"""
        log_print(f"   ğŸ” Tavily æœç´¢ (æœ€è¿‘{days}å¤©): {query}")
        url = "https://api.tavily.com/search"
        payload = {
            "api_key": self.tavily_key,
            "query": query,
            "search_depth": "advanced",
            "max_results": max_results,
            "include_answer": include_answer,
            "days": days
        }
        if topic: payload["topic"] = topic
            
        try:
            proxies = PROXY_URL if PROXY_URL else None
            with httpx.Client(timeout=30, proxy=proxies, trust_env=False) as client:
                @retryable
                def _post():
                    resp = client.post(url, json=payload)
                    if resp.status_code in [429, 432]:
                        log_print(f"      âš ï¸ Tavily é¢åº¦å—é™ ({resp.status_code})")
                        return resp
                    resp.raise_for_status()
                    return resp

                resp = _post()
                if resp.status_code != 200: return None
                
                data = resp.json()
                results = []
                if data.get('answer'):
                    results.append({"title": "Tavily AI Summary", "body": data['answer'], "url": ""})
                for r in data.get('results', []):
                    results.append({
                        "title": r.get('title', ''),
                        "body": r.get('content', ''),
                        "url": r.get('url', '')
                    })
                return results
        except Exception as e:
            log_print(f"      âŒ Tavily å¤±è´¥: {e}")
            return None

    def _search_exa(self, query, max_results=5):
        """Exa AI (åŸ Metaphor) å…œåº•æœç´¢"""
        log_print(f"   ğŸ” Exa AI å…œåº•æœç´¢: {query}")
        url = "https://api.exa.ai/search"
        headers = {
            "x-api-key": self.exa_key,
            "Content-Type": "application/json"
        }
        payload = {
            "query": query,
            "useAutoprompt": True,
            "numResults": max_results,
            "type": "neural"
        }
        try:
            proxies = PROXY_URL if PROXY_URL else None
            with httpx.Client(timeout=30, proxy=proxies, trust_env=False) as client:
                @retryable
                @track_cost(context="exa_search")
                def _post():
                    return client.post(url, json=payload, headers=headers)
                
                resp = _post()
                if resp.status_code != 200:
                    log_print(f"      âš ï¸ Exa AI æŠ¥é”™: {resp.status_code}")
                    return None
                
                data = resp.json()
                results = []
                for r in data.get('results', []):
                    results.append({
                        "title": r.get('title', ''),
                        "body": r.get('text', '') or r.get('snippet', ''),
                        "url": r.get('url', '')
                    })
                return results
        except Exception as e:
            log_print(f"      âŒ Exa AI è°ƒç”¨å¤±è´¥: {e}")
            return None

# ================= è¾…åŠ©å‡½æ•° =================

def get_github_trending():
    log_print("   ğŸ” GitHub Trending (Weekly)...")
    url = "https://github.com/trending?since=weekly" # å…¨è¯­è¨€ Weeklyï¼ŒèŒƒå›´æ›´å¹¿
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        with httpx.Client(proxy=PROXY_URL, timeout=15) as client:
            @retryable
            def _get():
                return client.get(url, headers=headers)

            resp = _get()
        soup = BeautifulSoup(resp.text, 'html.parser')
        repos = soup.select('article.Box-row')
        results = []
        for repo in repos[:5]:
            name_tag = repo.select_one('h1 a') or repo.select_one('h2 a')
            if not name_tag: continue
            name = name_tag.get_text(strip=True).replace('\n', '').replace(' ', '')
            desc = repo.select_one('p').get_text(strip=True) if repo.select_one('p') else ""
            # è¿‡æ»¤æ‰éAI/å·¥å…·ç±»çš„ä»“åº“(ç®€å•å…³é”®è¯è¿‡æ»¤)
            results.append(f"- {name}: {desc}")
        return results
    except Exception as e:
        return [f"- GitHub æŠ“å–å¤±è´¥: {e}"]

# ================= çƒ­æ¦œåŠ¨æ€æŠ“å– =================

def _fetch_single_source(
    source: Dict[str, str],
    search_tool: Optional["WebSearchTool"]
) -> Optional[str]:
    """
    æŠ“å–å•ä¸ªçƒ­æ¦œæºï¼ˆä¾›å¹¶å‘è°ƒç”¨ï¼‰ã€‚
    éš”ç¦»å¼‚å¸¸ï¼Œä¿è¯å•æºå¤±è´¥ä¸å½±å“æ•´ä½“ã€‚
    """
    try:
        return _fetch_with_fallback(
            source["primary"],
            source["backup"],
            source["name"],
            search_tool
        )
    except Exception as e:
        log_print(f"      âš ï¸ [{source['name']}] æŠ“å–å¼‚å¸¸: {e}")
        return None


# ================= è¶‹åŠ¿å‘ç°å™¨ (Trending Discoverer) =================

class TrendingDiscoverer:
    """
    v4.5: å¤–éƒ¨è¶‹åŠ¿æ¥å…¥å¼•æ“ (QQæµè§ˆå™¨ Agent / TrendRadar æ€è·¯)
    è´Ÿè´£è°ƒç”¨ç¬¬ä¸‰æ–¹ API æˆ–èšåˆå¹³å°çƒ­æœï¼Œå‘ç°é WATCHLIST å†…çš„çˆ†æ¬¾è¯é¢˜
    """
    def __init__(self, search_tool: Optional["WebSearchTool"] = None):
        self.search_tool = search_tool
        self.logger = get_logger(__name__)

    def discover_external_hotspots(self) -> List[str]:
        """
        ä»å¤–éƒ¨èšåˆæºå‘ç°çƒ­ç‚¹ã€‚
        v4.6: å¢åŠ  Product Hunt, Hacker News å’Œ V2EX çš„å®æ—¶ä¿¡å·æ¢æµ‹ (å¹¶å‘ä¼˜åŒ–)
        """
        self.logger.info("   ğŸ” [TrendingDiscoverer] æ­£åœ¨æ¢æµ‹å…¨ç½‘ç¤¾äº¤åª’ä½“ä¸å³æ—¶çƒ­æœ...")
        
        # 1. å®æ—¶ä¿¡å· (åˆ©ç”¨ Jina Reader æé€Ÿæ‰«æ)
        realtime_urls = [
            "https://www.producthunt.com",
            "https://news.ycombinator.com",
            "https://www.v2ex.com/?tab=hot"
        ]
        
        # 2. ä¼ ç»Ÿçƒ­æœæ¢æµ‹ (Perplexity/Tavily)
        search_queries = [
            "å¾®åšçƒ­æœæ¦œ site:s.weibo.com",
            "ç™¾åº¦çƒ­æœ å®æ—¶",
            "å°çº¢ä¹¦ çˆ†æ¬¾ è¯é¢˜",
            "ä»Šæ—¥å¤´æ¡ çƒ­ç‚¹æ–°é—»"
        ]
        
        results = []
        
        # å¹¶å‘æ‰§è¡Œ Jina æŠ“å–
        def fetch_jina(url):
            try:
                headers = {"x-no-cache": "true"}
                with httpx.Client(proxy=PROXY_URL, timeout=15) as client:
                    resp = client.get(f"https://r.jina.ai/{url}", headers=headers)
                    if resp.status_code == 200:
                        text_clean = resp.text[:1000].replace('\n', ' ')
                        return f"Source[{url}]: {text_clean}"
            except:
                pass
            return None

        # å¹¶å‘æ‰§è¡Œæœç´¢æ¢æµ‹
        def fetch_search(q):
            if self.search_tool and self.search_tool.enabled:
                res = self.search_tool.search(q, max_results=2, days=1)
                return [f"{r['title']}: {r['body'][:100]}" for r in res]
            return []

        with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_FETCHES) as executor:
            # æäº¤ Jina ä»»åŠ¡
            jina_futures = {executor.submit(fetch_jina, url): url for url in realtime_urls}
            # æäº¤æœç´¢ä»»åŠ¡
            search_futures = {executor.submit(fetch_search, q): q for q in search_queries}
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(jina_futures):
                res = future.result()
                if res: results.append(res)
                
            for future in as_completed(search_futures):
                res = future.result()
                if res: results.extend(res)
        
        return results

def fetch_dynamic_trends(
    client: OpenAI,
    search_tool: Optional["WebSearchTool"] = None
) -> List[str]:
    """
    ä»çƒ­æ¦œç½‘ç«™å¹¶å‘æŠ“å–å®æ—¶å…³é”®è¯ï¼ˆä¸‰çº§å®¹é”™æœºåˆ¶ï¼‰
    1. Jina Primary -> 2. Jina Backup (RSS) -> 3. Tavily Search
    
    ä½¿ç”¨ ThreadPoolExecutor å®ç°å¤šæºå¹¶å‘ï¼Œæ˜¾è‘—æå‡é‡‡é›†æ•ˆç‡ã€‚
    å•ä¸ªæºçš„è¶…æ—¶/å¤±è´¥ä¸ä¼šé˜»å¡å…¶ä»–æºã€‚
    """
    log_print("   ğŸŒ [çƒ­æ¦œæŠ“å–] ä»å…¨ç½‘çƒ­æ¦œè·å–å®æ—¶è¶‹åŠ¿ (å¹¶å‘æ¨¡å¼)...")
    
    # æ•°æ®æºé…ç½®
    sources = TREND_SOURCES
    
    # ===== Phase 0: å¤–éƒ¨è¶‹åŠ¿æ¢æµ‹ (TrendingDiscoverer) =====
    discoverer = TrendingDiscoverer(search_tool)
    external_hotspots = discoverer.discover_external_hotspots()
    if external_hotspots:
        log_print(f"   ğŸ“¡ [å¤–éƒ¨æ¢æµ‹] è·å–åˆ° {len(external_hotspots)} æ¡å…¨ç½‘åŸå§‹çƒ­ç‚¹")
    
    # ===== Phase 1: å¹¶å‘æŠ“å–æ‰€æœ‰æº =====
    source_contents: Dict[str, Optional[str]] = {}
    
    with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_FETCHES) as executor:
        future_to_source = {
            executor.submit(_fetch_single_source, src, search_tool): src
            for src in sources
        }
        
        for future in as_completed(future_to_source):
            src = future_to_source[future]
            try:
                content = future.result(timeout=FETCH_TIMEOUT_SECONDS)
                source_contents[src["name"]] = content
            except Exception as e:
                log_print(f"      âš ï¸ [{src['name']}] å¹¶å‘ä»»åŠ¡å¼‚å¸¸: {e}")
                source_contents[src["name"]] = None
    
    log_print(f"   ğŸ“Š æŠ“å–å®Œæˆ: {sum(1 for v in source_contents.values() if v)}/{len(sources)} ä¸ªæºæˆåŠŸ")
    
    # ===== Phase 2: ä¸²è¡Œæå–å…³é”®è¯ï¼ˆLLM è°ƒç”¨ä¸å®œè¿‡åº¦å¹¶å‘ï¼‰ =====
    all_keywords: List[str] = []
    
    # æ³¨å…¥å¤–éƒ¨çƒ­ç‚¹è¿›è¡Œå…³è”åˆ†æ
    if external_hotspots:
        external_context = "\n".join(external_hotspots)
        prompt = f"""
        è¿™æ˜¯å½“å‰å…¨ç½‘ç¤¾äº¤åª’ä½“çš„çƒ­æœæ‘˜è¦ï¼š
        {external_context}
        
        è¯·ä½œä¸º Agentï¼Œæ‰§è¡Œä»¥ä¸‹åŠ¨ä½œï¼š
        1. æŒ–æ˜ä¸Šè¿°çƒ­ç‚¹ä¸­ï¼Œ**å“ªäº›å¯ä»¥ä¸ AI ç»“åˆ**ï¼Ÿï¼ˆä¾‹å¦‚ï¼š'æ˜¥è¿' -> 'AI æŠ¢ç¥¨/æ”»ç•¥', 'è°ƒä¼‘' -> 'AI è‡ªåŠ¨åŒ–åŠå…¬')
        2. ç»™å‡º 2-3 ä¸ªæœ€å…·â€œæµé‡çˆ†å‘åŠ›â€çš„ AI å…³è”è¯ã€‚
        3. åªè¿”å›å…·ä½“åè¯ï¼Œç”¨è‹±æ–‡é€—å·åˆ†éš”ã€‚å¦‚æœæ²¡æœ‰åˆé€‚çš„ï¼Œè¿”å› NONEã€‚
        """
        try:
            @retryable
            @track_cost(context="discover_ai_hotspots")
            def _chat_hotspots():
                return client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ“…é•¿å°†ç¤¾ä¼šçƒ­ç‚¹ä¸ AI æŠ€æœ¯å¼ºå…³è”çš„å†…å®¹ç­–ç•¥ä¸“å®¶ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3
                )
            resp = _chat_hotspots()
            hot_keywords = [k.strip() for k in resp.choices[0].message.content.split(',') if k.strip() and "NONE" not in k.upper()]
            if hot_keywords:
                log_print(f"   ğŸ”¥ [Agent å…³è”] ä»å…¨ç½‘çƒ­æœé”å®š AI ç»“åˆç‚¹: {hot_keywords}")
                all_keywords.extend(hot_keywords)
        except Exception as e:
            log_print(f"      âš ï¸ å¤–éƒ¨çƒ­ç‚¹å…³è”åˆ†æå¤±è´¥: {e}")

    for src in sources:
        content = source_contents.get(src["name"])
        if content:
            keywords = _extract_keywords_from_single_source(
                client,
                content,
                src["name"],
                src["tag"]
            )
            all_keywords.extend(keywords)
    
    if not all_keywords:
        log_print("      âš ï¸ æ‰€æœ‰çƒ­æ¦œæºæå–å…³é”®è¯å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨")
        return []
    
    # å»é‡å¹¶é™åˆ¶æ•°é‡
    unique_keywords = list(dict.fromkeys(all_keywords))[:10]
    log_print(f"   ğŸ”¥ [çƒ­æ¦œæ±‡æ€»] å®æ—¶å…³é”®è¯: {unique_keywords}")
    return unique_keywords


def _fetch_with_fallback(
    primary_url: str,
    backup_url: str,
    source_name: str,
    search_tool: Optional["WebSearchTool"] = None
) -> Optional[str]:
    """
    ä¸‰çº§è·å–ç­–ç•¥ï¼šJina Primary -> Jina Backup -> Tavily Search
    """
    jina_base = "https://r.jina.ai/"
    
    # 1. å°è¯• Jina Primary
    content = _fetch_via_jina(jina_base + primary_url, source_name, "primary")
    if content and len(content) >= 500:
        return content
    
    # 2. å°è¯• Jina Backup (RSS)
    if backup_url:
        log_print(f"      ğŸ”„ [{source_name}] Primary å¤±è´¥ï¼Œå°è¯• Backup (RSS)...")
        content = _fetch_via_jina(jina_base + backup_url, source_name, "backup")
        if content and len(content) >= 500:
            return content

    # 3. å°è¯• Tavily ç»ˆææ•‘æ´
    if search_tool and search_tool.enabled:
        log_print(f"      ğŸ›¡ï¸ [{source_name}] å¯ç”¨ Tavily ç»ˆææ•‘æ´...")
        # æ„é€ æœç´¢è¯
        query = f"{source_name} çƒ­é—¨ AI ç§‘æŠ€å†…å®¹ {datetime.now().strftime('%Y-%m-%d')}"
        results = search_tool.search(query, max_results=3, days=3)
        if results:
            # æ‹¼æ¥ Tavily çš„æœç´¢ç»“æœä½œä¸ºä¼ªé€ çš„"ç½‘é¡µå†…å®¹"
            combined_text = "\n".join([f"Title: {r['title']}\nSnippet: {r['body']}" for r in results])
            log_print(f"      âœ… [{source_name}] Tavily æ•‘æ´æˆåŠŸ: æŠ“å– {len(results)} æ¡ç»“æœ")
            return combined_text
            
    log_print(f"      âŒ [{source_name}] æ‰€æœ‰é€šé“å‡å¤±è´¥")
    return None


def _fetch_via_jina(url: str, source_name: str, url_type: str) -> Optional[str]:
    """
    é€šè¿‡ Jina Reader API è·å–ç½‘é¡µå†…å®¹
    """
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "x-no-cache": "true"  # å¼ºåˆ¶ Jina Reader æŠ“å–æœ€æ–°é¡µé¢ï¼Œä¸è¿”å›ç¼“å­˜
        }
        with httpx.Client(proxy=PROXY_URL, timeout=30) as client:
            @retryable
            def _get():
                return client.get(url, headers=headers)

            resp = _get()
            
            if resp.status_code != 200:
                log_print(f"      âš ï¸ [{source_name}] {url_type} çŠ¶æ€ç : {resp.status_code}")
                return None
            
            content = resp.text
            if len(content) < 500:
                log_print(f"      âš ï¸ [{source_name}] {url_type} å†…å®¹è¿‡çŸ­: {len(content)} å­—ç¬¦")
                return None
            
            log_print(f"      âœ… [{source_name}] {url_type} æˆåŠŸ: {len(content)} å­—ç¬¦")
            return content[:8000]  # é™åˆ¶é•¿åº¦ï¼Œé¿å… token è¿‡å¤š
            
    except httpx.TimeoutException:
        log_print(f"      âš ï¸ [{source_name}] {url_type} è¶…æ—¶")
        return None
    except Exception as e:
        log_print(f"      âš ï¸ [{source_name}] {url_type} å¼‚å¸¸: {e}")
        return None


def _extract_keywords_from_single_source(
    client: OpenAI,
    content: str,
    name: str,
    tag: str
) -> List[str]:
    """
    ä½¿ç”¨ LLM ä»å•ä¸ªçƒ­æ¦œæºä¸­æå–å…³é”®è¯ï¼ˆå¸¦ä¸¥æ ¼é™å™ªè¿‡æ»¤ï¼‰
    """
    if not content:
        return []
    
    # é™åˆ¶å†…å®¹é•¿åº¦ï¼ˆä¿ç•™è¾ƒå¤šå†…å®¹ä»¥è¦†ç›–çƒ­æ¦œå‰50åï¼‰
    content_truncated = content[:8000]
    
    prompt = f"""
è¿™æ˜¯ã€{name}ã€‘ä»Šå¤©çš„çƒ­æ¦œæˆ–æœç´¢æ‘˜è¦ã€‚
è¯·ä»ä¸­æå– 2-3 ä¸ªæœ€ç¬¦åˆ"{tag}"é¢†åŸŸçš„å…·ä½“æŠ€æœ¯åè¯æˆ–äº§å“åç§°ã€‚

âš ï¸ å…³é”®è¿‡æ»¤è§„åˆ™ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š
1. ğŸ”´ **ç»å¯¹æ’é™¤åº•å±‚æŠ€æœ¯**ï¼šä¸¥ç¦æå– åç«¯æ¡†æ¶(Spring Boot/Django)ã€æ•°æ®åº“(Redis/SQL)ã€è¿ç»´(K8s/Docker)ã€åº•å±‚é©±åŠ¨(CUDA/NATS)ã€ç¼–ç¨‹è¯­è¨€ç‰ˆæœ¬(Java 21/Vite 8)ã€‚**æˆ‘ä»¬åªè¦ç»™å°ç™½ç”¨çš„å·¥å…·ï¼**
2. ğŸŸ¢ **åªä¿ç•™åº”ç”¨å±‚**ï¼š
   - AI åº”ç”¨/å¤§æ¨¡å‹ (DeepSeek, Kimi, Claude 4.5, Sora)
   - æ•ˆç‡å·¥å…· (Notion, Cursor, Obsidian, Arcæµè§ˆå™¨)
   - è½åœ°ç©æ³• (AIåšPPT, æ™ºèƒ½ä½“å¼€å‘, æœ¬åœ°éƒ¨ç½²)
   - è¡Œä¸šçƒ­ç‚¹ (AIçœ¼é•œ, å…·èº«æ™ºèƒ½)
   - ç¤¾äº¤çˆ†æ¬¾ (AI æ‰©å›¾, è¯ä»¶ç…§, è¯­éŸ³å…‹éš†, æ‰‹æœº Agent è‡ªåŠ¨åŒ–)
3. æ’é™¤å¨±ä¹æ˜æ˜Ÿå’Œç¤¾ä¼šæ–°é—»ã€‚
4. å¦‚æœé¡µé¢æ˜¯ RSS XML æ ¼å¼ï¼Œè¯·å¿½ç•¥ XML æ ‡ç­¾ï¼Œåªæå– Title ä¸­çš„æŠ€æœ¯åè¯ã€‚
5. è¿”å›æ ¼å¼ï¼šåªè¿”å›åè¯ï¼Œç”¨è‹±æ–‡é€—å·åˆ†éš”ã€‚å¦‚æœä¸ç¡®å®šæˆ–æ— ç›¸å…³å†…å®¹ï¼Œè¿”å› "NONE"ã€‚
6. ä¼˜å…ˆæå–**çŸ¥åç§‘æŠ€å…¬å¸**ï¼ˆå¦‚æ·±åº¦æ±‚ç´¢ï¼Œæ™ºè°±, å­—èŠ‚, è…¾è®¯ã€é˜¿é‡Œã€OpenAIï¼ŒGoogle ï¼ŒClaude ï¼ŒBing ï¼Œæœˆä¹‹æš—é¢ï¼Œè®¯é£ï¼Œç™¾åº¦ï¼Œå¾®è½¯ï¼Œè‹¹æœï¼Œå°çº¢ä¹¦ï¼‰å‘å¸ƒçš„**æ–°äº§å“åç§°**ï¼ˆå¦‚ AutoGLM, Soraï¼‰ï¼Œä»¥åŠ**åœ¨ç¤¾äº¤åª’ä½“ï¼ˆå°çº¢ä¹¦/å¾®åš/æŠ–éŸ³ï¼‰ä¸Šç–¯ä¼ çš„ AI ç©æ³•**ã€‚

ç¤ºä¾‹ï¼š
âŒ é”™è¯¯ï¼šSpring Boot, MySQL, React Hooks
âœ… æ­£ç¡®ï¼šDeepSeek, Cursor, ç§˜å¡”æœç´¢
"""
    
    try:
        @retryable
        @track_cost(context="extract_keywords")
        def _chat_create():
            return client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ•é”çš„æŠ€æœ¯è¶‹åŠ¿æ•æ‰‹ï¼Œæ“…é•¿ä»æ‚ä¹±çš„ç½‘é¡µå†…å®¹ä¸­æå–æœ‰ä»·å€¼çš„æŠ€æœ¯å…³é”®è¯ï¼Œå¹¶è¿‡æ»¤æ‰æ— å…³çš„å¨±ä¹å…«å¦ã€‚"},
                    {"role": "user", "content": f"ã€{name} çƒ­æ¦œå†…å®¹ã€‘\n{content_truncated}\n\n{prompt}"}
                ],
                temperature=0.2
            )

        response = _chat_create()
        result = response.choices[0].message.content.strip()
        
        # å¤„ç† NONE æƒ…å†µ
        if result.upper() == "NONE" or "NONE" in result.upper():
            log_print(f"      â­ï¸ [{name}] æ— ç›¸å…³æŠ€æœ¯å†…å®¹ï¼Œè·³è¿‡")
            return []
        
        # æ¸…æ´—å¹¶è¿”å›
        keywords = [k.strip() for k in result.split(',') if k.strip() and len(k.strip()) < 30]
        keywords = keywords[:3]  # æ¯ä¸ªæºæœ€å¤š3ä¸ª
        
        if keywords:
            log_print(f"      ğŸ“Œ [{name}] æå–: {keywords}")
        return keywords
        
    except Exception as e:
        log_print(f"      âš ï¸ [{name}] å…³é”®è¯æå–å¤±è´¥: {e}")
        return []


def extract_hot_entities(client: OpenAI, search_results: List[Dict[str, Any]]) -> List[str]:
    """ä»æœç´¢ç»“æœä¸­æå– 2-3 ä¸ªçƒ­é—¨æŠ€æœ¯åè¯"""
    if not search_results: return []
    
    text = "\n".join([f"- {r['title']}" for r in search_results[:10]]) # é™åˆ¶è¾“å…¥é•¿åº¦
    prompt = """
    è¯·ä»ä¸Šè¿°æ–°é—»æ ‡é¢˜ä¸­ï¼Œæå– 2-3 ä¸ªå½“å‰æœ€ç«çš„ AI æŠ€æœ¯æˆ–äº§å“åç§°ã€‚
    è¦æ±‚ï¼š
    1. åªè¿”å›å…·ä½“åè¯ï¼Œå¦‚ "DeepSeek V3", "MCP", "Sora 2.0"ã€‚
    2. ä¸è¦è¿”å›é€šç”¨è¯ï¼ˆå¦‚ "AI", "LLM", "Technology"ï¼‰ã€‚
    3. è¾“å‡ºæ ¼å¼ï¼šç”¨è‹±æ–‡é€—å·åˆ†éš”ï¼Œä¸è¦å…¶ä»–åºŸè¯ã€‚
    """
    try:
        @retryable
        @track_cost(context="extract_hot_entities")
        def _chat_create():
            return client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ•é”çš„æŠ€æœ¯è¶‹åŠ¿æ•æ‰‹ã€‚"},
                    {"role": "user", "content": f"ã€æ–°é—»æ ‡é¢˜åˆ—è¡¨ã€‘\n{text}\n\n{prompt}"}
                ],
                temperature=0.1
            )

        response = _chat_create()
        content = response.choices[0].message.content.strip()
        # ç®€å•æ¸…ç†
        entities = [e.strip() for e in content.split(',') if e.strip() and len(e.strip()) < 20]
        return entities[:3]
    except Exception as e:
        log_print(f"      âš ï¸ çƒ­ç‚¹æå–å¤±è´¥: {e}")
        return []

# ================= æ ¸å¿ƒé€»è¾‘ =================

def _robust_json_parse(content: str) -> Any:
    """
    v4.1: é²æ£’ JSON è§£æå™¨
    æ— è®º LLM è¾“å‡ºå¸¦ä¸å¸¦ Markdown ä»£ç å—ï¼Œæˆ–è€… JSON ç¼ºäº†é€—å·å¼•å·ï¼Œéƒ½èƒ½æ­£ç¡®è§£æ
    """
    if not content:
        return []
    
    # 1. å°è¯•ç›´æ¥è§£æï¼ˆæœ€ä¼˜æƒ…å†µï¼šå¹²å‡€çš„ JSONï¼‰
    try:
        return json.loads(content)
    except json.JSONDecodeError:
        pass
    
    # 2. ç”¨æ­£åˆ™æå–ç¬¬ä¸€ä¸ª JSON å¯¹è±¡/æ•°ç»„
    json_pattern = r'(\{.*\}|\[.*\])'
    match = re.search(json_pattern, content, re.DOTALL)
    
    if match:
        raw_json = match.group(1)
        try:
            # 3. ä½¿ç”¨ json_repair ä¿®å¤å¹¶è§£æ
            repaired = repair_json(raw_json, return_objects=True)
            log_print(f"      ğŸ”§ JSON å·²è‡ªåŠ¨ä¿®å¤")
            return repaired
        except Exception as e:
            log_print(f"      âš ï¸ JSON ä¿®å¤å¤±è´¥: {e}")
    
    # 4. ç»ˆæå›é€€ï¼šæ•´ä½“ä¿®å¤
    try:
        repaired = repair_json(content, return_objects=True)
        return repaired
    except Exception as e:
        log_print(f"      âŒ JSON è§£æå½»åº•å¤±è´¥: {e}")
        return []


def get_plan_prompt(history_text: str = "", directed_topic: Optional[str] = None) -> str:
    """åŠ¨æ€ç”Ÿæˆè§„åˆ’æç¤ºè¯ï¼Œæ³¨å…¥å½“å‰æ—¥æœŸã€å†å²è®°å½•å’Œç”¨æˆ·æ„å›¾"""
    today = datetime.now().strftime('%Y-%m-%d')
    
    intent_instruction = ""
    if directed_topic:
        intent_instruction = f"""
    ğŸ‘¤ **ç”¨æˆ·æ ¸å¿ƒæŒ‡ä»¤**ï¼š
    ç”¨æˆ·æŒ‡å®šäº†ä¸»é¢˜ã€{directed_topic}ã€‘ã€‚
    1. ä½ ç”Ÿæˆçš„ 3 ä¸ªé€‰é¢˜ä¸­ï¼Œ**å¿…é¡»åŒ…å«**è‡³å°‘ 1 ä¸ªä¸ã€{directed_topic}ã€‘æ·±åº¦ç›¸å…³çš„é€‰é¢˜ï¼ˆä½œä¸º A æ–¹æ¡ˆï¼‰ã€‚
    2. åŒæ—¶ï¼Œè¯·ä»æƒ…æŠ¥æ± ä¸­æŒ–æ˜å¦å¤– 1-2 ä¸ª**é«˜æ½œè´¨**çš„éšæœºçƒ­ç‚¹æˆ–å…³è”è¯é¢˜ï¼ˆä½œä¸º Plan B/Cï¼‰ï¼Œä¸ç”¨æˆ·æŒ‡å®šä¸»é¢˜è¿›è¡Œ"ä»·å€¼PK"ã€‚
    3. å¦‚æœå‘ç°ã€{directed_topic}ã€‘ç›®å‰æ¯«æ— æ–°æ„ï¼ˆæ— æ–°é—»ã€æ— ç—›ç‚¹ï¼‰ï¼Œä½ å¯ä»¥"æŠ—æ—¨"ï¼Œå…¨æ¨å…¶ä»–æ›´æœ‰ä»·å€¼çš„çƒ­ç‚¹ï¼Œä½†å¿…é¡»åœ¨åˆ†æä¸­è¯´æ˜ç†ç”±ã€‚
    """
    
    return f"""
    ğŸ“… ä»Šå¤©æ˜¯ {today}ã€‚ä½ å¿…é¡»åªå…³æ³¨æœ€è¿‘ 3-7 å¤©å†…å‘ç”Ÿçš„ AI åœˆæœ€æ–°å¤§äº‹ä»¶ã€‚
    â— ç»å¯¹ç¦æ­¢æŠ¥é“ 2024 å¹´æˆ–æ›´æ—©çš„æ—§é—»ï¼ˆå¦‚ DeepSeek R1ã€GPT-4 å‘å¸ƒç­‰å†å²äº‹ä»¶ï¼‰ã€‚
    {intent_instruction}

    ã€å†å²å‘æ–‡è®°å½• (æœ€è¿‘7å¤©)ã€‘
{history_text}
âš ï¸ æŸ¥é‡æŒ‡ä»¤ï¼šå¦‚æœä¸Šè¿°å†å²è®°å½•ä¸­å·²å­˜åœ¨ç›¸ä¼¼é€‰é¢˜ï¼Œè¯·å¿…é¡»è°ƒæ•´åˆ‡å…¥è§’åº¦ï¼ˆä¾‹å¦‚ï¼šä»"æ–°é—»æŠ¥é“"è½¬å‘"æ·±åº¦å®æµ‹"æˆ–"é¿å‘æŒ‡å—"ï¼‰ã€‚å¦‚æœæ— æ³•å·®å¼‚åŒ–ï¼Œè¯·ç›´æ¥ä¸¢å¼ƒè¯¥é€‰é¢˜ã€‚

ä½ æ˜¯â€œç‹å¾€AIâ€çš„é¦–å¸­å†…å®¹ç­–ç•¥å®˜ã€‚
è¯·åŸºäºã€å…¨ç½‘æƒ…æŠ¥ã€‘å’Œã€å¿ƒç†å­¦ç­–ç•¥ã€‘ï¼ŒæŒ–æ˜ 3 ä¸ªæœ€å…·â€œçˆ†æ¬¾æ½œè´¨â€çš„é€‰é¢˜æ–¹å‘ã€‚

## ä»·å€¼å…¬å¼ (æµé‡é£æš´ç‰ˆ)
**é€‰é¢˜ä»·å€¼** = (ç¤¾ä¼šçƒ­åº¦ Ã— å¥½å¥‡å¿ƒ) + (æƒ…ç»ªå…±é¸£ Ã— å‚ä¸åº¦) - è®¤çŸ¥é—¨æ§›

## ç­–ç•¥ä¼˜å…ˆçº§ (TRAFFIC_STORM)
1. **å¤§ä¼—ä½“æ„Ÿä¼˜å…ˆ**ï¼šæ¯”èµ·"æ¨¡å‹å‚æ•°"ï¼Œç”¨æˆ·æ›´å…³å¿ƒ"æˆ‘æ‰‹æœºä¸Šçš„ AI å˜èªæ˜äº†"ã€"AI å¸®æˆ‘çœäº† 500 å—"ã€‚
2. **æƒ…ç»ªä»·å€¼ä¼˜å…ˆ**ï¼šå¯»æ‰¾é‚£äº›èƒ½å¼•å‘"å§æ§½"ã€"ç¦»è°±"ã€"çœŸé¦™"ã€"ç»ˆäºç­‰åˆ°"æ„Ÿå¹çš„è¯é¢˜ã€‚
3. **ç¤¾äº¤è´§å¸ä¼˜å…ˆ**ï¼šè®©ç”¨æˆ·è½¬åˆ°æœ‹å‹åœˆæ˜¾å¾—è‡ªå·±"æ‡‚ç§‘æŠ€"ã€"ä¼šçœé’±"ã€"èµ°åœ¨æ—¶ä»£å‰æ²¿"ã€‚

## å¿ƒç†å­¦ä¸‰è·¯ç­–ç•¥ï¼ˆæµé‡åŠ å¼ºç‰ˆï¼‰
1. **Aè·¯ - é”šç‚¹æ•ˆåº” (å€ŸåŠ¿é¡¶æµ)**ï¼šå€ŸåŠ© DeepSeek/Cursor/Gemini ç­‰é¡¶æµäº§å“çš„çŸ¥ååº¦ï¼Œå…³æ³¨å…¶"éšè—åŠŸèƒ½"æˆ–"æœ€æ–°ç©æ³•"ã€‚ç”¨æˆ·çœ‹åˆ°ç†Ÿæ‚‰çš„åå­—æ›´å®¹æ˜“ç‚¹å‡»ã€‚
2. **Bè·¯ - å³æ—¶æ»¡è¶³ (æ•ˆèƒ½ç¥å™¨)**ï¼šå¯»æ‰¾çœŸæ­£çš„"æ•ˆç‡ç¥å™¨"ï¼Œä¸»æ‰“"3åˆ†é’Ÿä¸Šæ‰‹"ã€"ä¸‹ç­æ—©èµ°1å°æ—¶"ã€‚è®©ç”¨æˆ·è§‰å¾—"çœ‹å®Œå°±èƒ½ç”¨"ã€‚
3. **Cè·¯ - æŸå¤±åŒæ¶ (é¿å‘/è®¤çŸ¥)**ï¼š
   - é¿å‘ç±»ï¼šå¯»æ‰¾"æ™ºå•†ç¨"ã€"ç¿»è½¦ç°åœº"ã€"å¹³æ›¿"ï¼Œè§¦å‘ç”¨æˆ·å®³æ€•è¸©å‘çš„å¿ƒç†ã€‚
   - è®¤çŸ¥ç±»ï¼šè§£è¯»æ–°è¶‹åŠ¿ã€æ–°ç¡¬ä»¶ï¼ˆå¦‚ AI è€³æœºã€æ‰‹æœºæ™ºèƒ½ä½“ï¼‰ï¼Œè®©ç”¨æˆ·å®³æ€•"è½åäºæ—¶ä»£"ã€‚

è¾“å…¥æ•°æ®ï¼š
- é•¿æœŸå…³æ³¨å“ç±»åŠ¨æ€
- æœ¬å‘¨çƒ­é—¨å·¥å…·/æ•™ç¨‹
- ç”¨æˆ·åæ§½ä¸ç—›ç‚¹
- å¤§å‚æ–°å‘å¸ƒåŠ¨æ€

å†³ç­–æ ‡å‡†ï¼š
- âœ… **ä¿ç•™**ï¼šDeepSeek éšè—ç©æ³•ï¼ˆé”šç‚¹ï¼‰ã€å…è´¹ç”»æ¶æ„å›¾ï¼ˆå³æ—¶æ»¡è¶³ï¼‰ã€Cursor æ”¶è´¹é¿å‘ï¼ˆæŸå¤±åŒæ¶ï¼‰ã€Google AI è€³æœºä½“éªŒï¼ˆè®¤çŸ¥å‡çº§ï¼‰ã€‚
- âŒ **å‰”é™¤**ï¼šçº¯æ¯ç‡¥çš„èèµ„æ–°é—»ã€è¿‡äºå­¦æœ¯çš„è®ºæ–‡è§£è¯»ã€æ¯«æ— æ–°æ„çš„"æ­£ç¡®çš„åºŸè¯"ã€å†·é—¨æ— åå°å·¥å…·ã€‚

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼ JSONï¼‰ï¼š
[
    {{
        "event": "é€‰é¢˜æ ¸å¿ƒè¯ (å¦‚: DeepSeek)",
        "angle": "åˆ‡å…¥è§’åº¦ (å¦‚: éšè—ç©æ³• / é¿å‘æŒ‡å— / æ·±åº¦è¯„æµ‹)",
        "news_query": "åŠŸèƒ½æ€§æœç´¢è¯ (å¦‚: DeepSeek V3 file upload)",
        "social_query": "æƒ…ç»ªæ€§æœç´¢è¯ (å¦‚: DeepSeek æŠ¥é”™ / DeepSeek ä¸å¥½ç”¨)"
    }},
    ...
]
"""

# ä¿ç•™å†å²å…¼å®¹æ€§
PLAN_PROMPT = get_plan_prompt()

def step1_broad_scan_and_plan(
    client: OpenAI,
    search_tool: "WebSearchTool",
    directed_topic: Optional[str] = None
) -> List[Dict[str, str]]:
    """
    Step 1: å¹¿åŸŸä»·å€¼æ‰«æ (å¿ƒç†å­¦ä¸‰è·¯ç­–ç•¥ + å…¨ç½‘é›·è¾¾)
    æ··åˆæ¨¡å¼ï¼šå¦‚æœä¼ å…¥ directed_topicï¼Œå°†å…¶ä½œä¸º A è·¯æ ¸å¿ƒï¼ŒåŒæ—¶ä¿ç•™ B/C è·¯éšæœºæ¢ç´¢
    """
    log_print(f"\nğŸ“¡ [Step 1] å¹¿åŸŸä»·å€¼æ‰«æ (ç­–ç•¥: {CURRENT_CONFIG['name']})...")
    if directed_topic:
        log_print(f"   ğŸ¯ [æ··åˆæ¨¡å¼] æ ¸å¿ƒä¸»é¢˜: ã€Œ{directed_topic}ã€ + å…¨ç½‘éšæœºæ‰«æ")
    
    pre_scan_results: List[Dict[str, Any]] = []
    
    # === Phase 0: å…¨ç½‘é›·è¾¾ (Global Radar) ===
    # ç ´é™¤ä¿¡æ¯èŒ§æˆ¿ï¼Œä¸»åŠ¨å—…æ¢ä¸åœ¨ WATCHLIST é‡Œçš„æ–°é»‘é©¬
    log_print(f"   ğŸŒ‘ [Phase 0] å…¨ç½‘é›·è¾¾æ‰«æ (å‘ç°æ–°ç‰©ç§)...")
    for q in RADAR_QUERIES:
        res = search_tool.search(q, max_results=2, topic="news", days=1) # åªçœ‹24å°æ—¶å†…
        pre_scan_results.extend(res)

    # === Phase 0.5: çƒ­ç‚¹æå– ===
    hot_entities = extract_hot_entities(client, pre_scan_results)
    if hot_entities:
        log_print(f"   ğŸ”¥ [é›·è¾¾é”å®š] çªå‘çƒ­ç‚¹: {hot_entities}")

    # === Phase 0.6: çƒ­æ¦œåŠ¨æ€è¶‹åŠ¿ ===
    fresh_keywords = []
    try:
        fresh_keywords = fetch_dynamic_trends(client, search_tool)
    except Exception as e:
        log_print(f"      âš ï¸ çƒ­æ¦œæŠ“å–å¼‚å¸¸ï¼Œè·³è¿‡: {e}")
    
    # === Aè·¯: é¡¶æµé”šç‚¹ (Watchlist + Hotspots + Fresh) ===
    if directed_topic:
        # å®šå‘æ¨¡å¼ï¼šæ ¸å¿ƒæ˜¯ directed_topicï¼Œä½†ä¹Ÿæ¥çº³çªå‘çƒ­ç‚¹
        targets = [directed_topic]
        # é€‚å½“åŠ å…¥çƒ­ç‚¹ï¼ˆå¦‚æœæœ‰é‡å¤§çªå‘ï¼‰ï¼Œä½†ä¹Ÿå¯èƒ½è¢« LLM è¿‡æ»¤
        for h in hot_entities:
            if h.lower() not in directed_topic.lower():
                targets.append(h)
        targets = targets[:4] # ä¿æŒèšç„¦
        
        # === v4.9: å®šå‘æœç´¢å¢å¼º - æ‰©å±•æœ€æ–°åŠŸèƒ½å…³é”®è¯ ===
        log_print(f"   ğŸ” [å®šå‘å¢å¼º] æ‰©å±•æœç´¢: {directed_topic} + æœ€æ–°åŠŸèƒ½/æ›´æ–°...")
        # é’ˆå¯¹å¸¸è§äº§å“çš„æœ€æ–°åŠŸèƒ½æœç´¢æ‰©å±•
        PRODUCT_FEATURE_EXPANSIONS = {
            "coze": ["Coze Studio", "Coze IDE", "Coze å¯¹è¯ç”Ÿæˆå·¥ä½œæµ", "Coze è‡ªåŠ¨åˆ›å»º Agent"],
            "cursor": ["Cursor Composer", "Cursor Agent", "Cursor æ–°åŠŸèƒ½"],
            "deepseek": ["DeepSeek V3.2", "DeepSeek Reasoner", "DeepSeek æ–°åŠŸèƒ½"],
            "kimi": ["Kimi é•¿æ–‡æœ¬", "Kimi æ–°åŠŸèƒ½", "Kimi k2"],
        }
        # æŸ¥æ‰¾åŒ¹é…çš„äº§å“æ‰©å±•
        for product, expansions in PRODUCT_FEATURE_EXPANSIONS.items():
            if product in directed_topic.lower():
                for exp in expansions:
                    exp_query = f"{exp} æœ€æ–° åŠŸèƒ½ æ›´æ–° 2025"
                    res = search_tool.search(exp_query, max_results=2, topic="news", days=7)
                    pre_scan_results.extend(res)
                    log_print(f"      â†’ æ‰©å±•æœç´¢: {exp_query} ({len(res)} ç»“æœ)")
                break
    else:
        # éšæœºæ¨¡å¼
        targets = random.sample(WATCHLIST, 3)
        # å°†çƒ­æ¦œå…³é”®è¯åŠ å…¥ targets (æœ€é«˜ä¼˜å…ˆçº§)
        for fk in fresh_keywords:
            if not any(fk.lower() in t.lower() for t in targets):
                targets.insert(0, fk)
        # å°†çƒ­ç‚¹åŠ å…¥ targets (ä¼˜å…ˆä¾¦å¯Ÿ)
        for h in hot_entities:
            if not any(h.lower() in t.lower() for t in targets):
                targets.insert(0, h)
        targets = targets[:6]

    log_print(f"   ğŸ¯ [Aè·¯-é”šç‚¹] æ‰«æç›®æ ‡: {targets}")
    for t in targets:
        # æ¿€æ´»åƒµå°¸å…³é”®è¯ï¼šåŒæ—¶æœ"éšè—åŠŸèƒ½"å’Œ"æœ€æ–°æ›´æ–°"
        queries = [
            f"{t} éšè—åŠŸèƒ½ ç©æ³• æ•™ç¨‹ 2025",
            f"{t} new features latest update", # è‹±æ–‡æœæ›´æ–°å¾€å¾€æ›´å‡†
            f"{t} æœ€æ–°åŠŸèƒ½ ä¸Šçº¿ å‘å¸ƒ 2025"  # v4.9: å¢åŠ æœ€æ–°åŠŸèƒ½æœç´¢
        ]
        for q in queries:
            res = search_tool.search(q, max_results=2, topic="news", days=7)  # v4.9: å¢åŠ ç»“æœæ•°å’Œæ—¶é—´èŒƒå›´
            pre_scan_results.extend(res)
        
    # === Bè·¯: éšæœºæ”¶ç›Šåœºæ™¯ (Life Hack) ===
    log_print(f"   âš¡ [Bè·¯-æ”¶ç›Š] æ‰«ææ•ˆç‡ç¥å™¨...")
    selected_efficiency = random.sample(EFFICIENCY_KEYWORDS, 3)
    if directed_topic:
        # æ··åˆæ¨¡å¼ï¼šåŠ å…¥å®šå‘ä¸»é¢˜çš„æ•ˆç‡åœºæ™¯
        selected_efficiency.insert(0, f"{directed_topic} æ•ˆç‡ç¥å™¨")
        
    log_print(f"      ğŸ² éšæœºæŠ½å–: {selected_efficiency}")
    for kw in selected_efficiency:
        # Bè·¯: å¼ºåˆ¶è¿½åŠ é«˜è´¨é‡ä¿¡æºï¼Œè¿‡æ»¤ SEO åƒåœ¾
        q = f"{kw} æ¨è site:sspai.com OR site:36kr.com OR site:v2ex.com OR site:mp.weixin.qq.com"
        res = search_tool.search(q, max_results=2, days=3)
        pre_scan_results.extend(res)
        
    # === Cè·¯: éšæœºé¿å‘åœºæ™¯ (Pain Points) ===
    log_print(f"   ğŸ›¡ï¸ [Cè·¯-æŸå¤±] æ‰«æé¿å‘/åæ§½...")
    selected_pain = random.sample(PAIN_KEYWORDS, 3)
    if directed_topic:
        # æ··åˆæ¨¡å¼ï¼šåŠ å…¥å®šå‘ä¸»é¢˜çš„é¿å‘åœºæ™¯
        selected_pain.insert(0, f"{directed_topic} é¿å‘ åæ§½")
        
    log_print(f"      ğŸ² éšæœºæŠ½å–: {selected_pain}")
    for kw in selected_pain:
        # Cè·¯: å¼ºåˆ¶è¿½åŠ ç¤¾åŒºä¿¡æº
        q = f"{kw} åæ§½ é¿å‘ site:v2ex.com OR site:reddit.com OR site:mp.weixin.qq.com"
        res = search_tool.search(q, max_results=2, days=3)
        pre_scan_results.extend(res)
    
    pre_scan_text = "\n".join([f"- {r['title']}: {r['body'][:80]}" for r in pre_scan_results])
    
    # 2. æ™ºèƒ½ç­›é€‰ä¸è§„åˆ’
    log_print(f"   ğŸ“ æƒ…æŠ¥èšåˆå®Œæ¯•ï¼ŒDeepSeek æ­£åœ¨åº”ç”¨å¿ƒç†å­¦ç­–ç•¥é€‰é¢˜...")
    
    # åŠ è½½å†å²è®°å½•
    history = load_history()
    history_text = "\n".join([f"- {h['date']}: {h['topic']} ({h['angle']})" for h in history])
    if not history_text: history_text = "æ— ï¼ˆè¿™æ˜¯ç¬¬ä¸€ç¯‡ï¼‰"

    try:
        @retryable
        @track_cost(context="step1_broad_scan_and_plan")
        def _chat_create():
            return client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": get_plan_prompt(history_text, directed_topic)},
                    {"role": "user", "content": f"ã€æ··åˆæƒ…æŠ¥æ± ã€‘\n{pre_scan_text}"}
                ],
                temperature=0.7,
                response_format={ "type": "json_object" }
            )

        response = _chat_create()
        content = response.choices[0].message.content
        
        # v4.1: ä½¿ç”¨ json_repair å¢å¼ºé²æ£’æ€§
        search_plan = _robust_json_parse(content)
        if isinstance(search_plan, dict) and "events" in search_plan:
            search_plan = search_plan["events"]
        
        # v4.4: å»é‡å¹¶é˜²é¥¥é¥¿ï¼Œè‹¥å…¨é‡å¤åˆ™å¼ºåˆ¶ä¿ç•™ç›¸ä¼¼åº¦æœ€ä½çš„ä¸€ä¸ª
        history_items = load_history()
        search_plan = _dedup_search_plan(search_plan, history_items)
        
        log_print(f"   ğŸ§  é€‰é¢˜æ–¹å‘å·²é”å®š: {[i['event'] + '-' + i['angle'] for i in search_plan]}\n")
        return search_plan
    except Exception as e:
        log_print(f"   âŒ è§„åˆ’å¤±è´¥: {e}")
        return [{"event": "DeepSeek", "angle": "é¿å‘", "news_query": "DeepSeek V3", "social_query": "DeepSeek å¹»è§‰"}]

def _clean_text(text: Optional[str], max_len: int = 100) -> str:
    """æ¸…æ´—æ–‡æœ¬ï¼šç§»é™¤å¤šä½™ç©ºç™½ã€HTMLæ ‡ç­¾ã€æˆªæ–­é•¿åº¦"""
    if not text:
        return ""
    # ç§»é™¤å¤šä½™ç©ºç™½å’Œæ¢è¡Œ
    text = re.sub(r'\s+', ' ', text).strip()
    # ç§»é™¤å¸¸è§ HTML æ ‡ç­¾æ®‹ç•™
    text = re.sub(r'<[^>]+>', '', text)
    # æˆªæ–­å¹¶æ·»åŠ çœç•¥å·
    if len(text) > max_len:
        text = text[:max_len] + "..."
    return text

def step2_deep_scan(
    search_plan: List[Dict[str, str]],
    search_tool: "WebSearchTool",
    directed_topic: Optional[str] = None
) -> str:
    """
    Step 2: æ·±åº¦éªŒè¯ (é‡ç¤¾äº¤/ç—›ç‚¹)
    è¾“å‡ºæ ¼å¼ï¼šæ¸…æ™°çš„ Markdown åˆ—è¡¨ï¼ŒåŒ…å«æ‘˜è¦å’Œæ¥æº URL
    """
    log_print("ğŸ“¡ [Step 2] å¯åŠ¨æ·±åº¦ä»·å€¼éªŒè¯...\n")
    all_results = []
    
    w_news = CURRENT_CONFIG['weights']['news']
    w_social = CURRENT_CONFIG['weights']['social']
    
    for item in search_plan:
        event = item.get("event", "æœªçŸ¥")
        angle = item.get("angle", "é€šç”¨")
        news_q = item.get("news_query", "")
        social_q = item.get("social_query", "")

        is_core = False
        if directed_topic and event:
            dt = str(directed_topic).lower()
            ev = str(event).lower()
            is_core = (dt in ev) or (ev in dt)

        # é˜²å¹²æ‰°ï¼šå®šå‘æ¨¡å¼ä¸‹ï¼ŒæŠŠæ›´å¤šæ£€ç´¢é¢åº¦ç•™ç»™æ ¸å¿ƒä¸»é¢˜ï¼›éæ ¸å¿ƒä¸»é¢˜é™é…é¢
        social_max_results = 4
        news_max_results = 2
        if directed_topic:
            social_max_results = 4 if is_core else 2
            news_max_results = 2 if is_core else 1
        
        log_print(f"   ğŸ” æ­£åœ¨æ·±æŒ–: ã€{event}ã€‘ ({angle}æ–¹å‘)")
        event_data = [f"### ğŸ¯ é€‰é¢˜: {event} ({angle})"]
        
        # 1. ç¤¾äº¤/ç—›ç‚¹æœç´¢ (æ ¸å¿ƒ)
        if social_q:
            log_print(f"      ğŸ’¬ ç¤¾äº¤èˆ†æƒ… (æƒé‡ {w_social}): {social_q}")
            full_social_q = f"{social_q} site:mp.weixin.qq.com OR site:xiaohongshu.com OR site:bilibili.com"
            res = search_tool.search(full_social_q, max_results=social_max_results)
            if res:
                event_data.append(f"\n**ğŸ’¬ ç”¨æˆ·åé¦ˆ** ({social_q})")
                for r in res:
                    title = _clean_text(r.get('title', 'æ— æ ‡é¢˜'), 50)
                    body = _clean_text(r.get('body', ''), 100)
                    url = r.get('url', '')
                    if url:
                        event_data.append(f"- **{title}**: {body} [[æ¥æº]({url})]")
                    else:
                        event_data.append(f"- **{title}**: {body}")
                
        # 2. å®˜æ–¹éªŒè¯ (è¾…åŠ©)
        if news_q:
            log_print(f"      ğŸ”¥ å®˜æ–¹éªŒè¯ (æƒé‡ {w_news}): {news_q}")
            res = search_tool.search(news_q, max_results=news_max_results)
            if res:
                event_data.append(f"\n**ğŸ“° å®˜æ–¹ä¿¡æ¯** ({news_q})")
                for r in res:
                    title = _clean_text(r.get('title', 'æ— æ ‡é¢˜'), 60)
                    url = r.get('url', '')
                    if url:
                        event_data.append(f"- {title} [[æ¥æº]({url})]")
                    else:
                        event_data.append(f"- {title}")
        
        all_results.append("\n".join(event_data))
        log_print("")
        time.sleep(1)

    # GitHub è¡¥å…… (Weekly)
    log_print(f"   ğŸ’» GitHub Weekly Trending...")
    github_res = get_github_trending()
    all_results.append("### ğŸ’» GitHub Weekly Trending\n" + "\n".join(github_res))
    
    return "\n\n---\n\n".join(all_results)

def step3_final_decision(
    scan_data: str,
    client: OpenAI,
    history_text: str = "æ— ï¼ˆè¿™æ˜¯ç¬¬ä¸€ç¯‡ï¼‰",
    directed_topic: Optional[str] = None
) -> str:
    """Step 3: å†³ç­–ï¼ˆå¸¦å»é‡å’Œæ–°è¯æ‰¶æŒ + ç”¨æˆ·æ„å›¾åŠ æƒï¼‰"""
    log_print("\n" + "="*50 + "\nğŸ“ DeepSeek ä¸»ç¼–å®¡æ ¸ä¸­...\n" + "="*50)
    
    # æ„é€ ç”¨æˆ·æ„å›¾æç¤º
    user_intent_prompt = ""
    if directed_topic:
        user_intent_prompt = f"""
    ğŸ‘¤ **ç”¨æˆ·æ„å›¾ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰**ï¼š
    ç”¨æˆ·æ˜ç¡®å¸Œæœ›å†™å…³äºã€{directed_topic}ã€‘çš„å†…å®¹ã€‚
    **å†³ç­–åŸåˆ™**ï¼š
    1. é»˜è®¤ä¼˜å…ˆï¼šåœ¨åŒç­‰ä»·å€¼ä¸‹ï¼Œä¼˜å…ˆé€‰æ‹©ä¸ã€{directed_topic}ã€‘ç›¸å…³çš„é€‰é¢˜ã€‚
    2. å…è®¸æŠ—æ—¨ï¼šåªæœ‰å½“æ‰«æåˆ°çš„å…¶ä»–çƒ­ç‚¹ï¼ˆå¦‚çªå‘é‡å¤§æŠ€æœ¯æ›´æ–°ï¼‰å…·æœ‰**æé«˜çš„çˆ†æ¬¾æ½œè´¨**æ—¶ï¼Œä½ æ‰å»ºè®®æ”¾å¼ƒç”¨æˆ·æŒ‡å®šä¸»é¢˜ã€‚
    3. æ··åˆç­–ç•¥ï¼šå¦‚æœå¯èƒ½ï¼Œå°è¯•å°†ã€{directed_topic}ã€‘ä¸å…¶ä»–çƒ­ç‚¹ç»“åˆï¼ˆä¾‹å¦‚ "ç”¨ {directed_topic} è§£å†³è¿™ä¸ªæ–°çƒ­ç‚¹é—®é¢˜"ï¼‰ã€‚
    """

    prompt = f"""
    {EDITOR_PROMPT}
    {user_intent_prompt}
    
    âŒ **ä¸¥æ ¼å»é‡**ï¼šä»¥ä¸‹æ˜¯æœ€è¿‘å·²å†™è¿‡çš„é€‰é¢˜ï¼š
    {history_text}
    
    **ç»å¯¹ç¦æ­¢**å†æ¬¡é€‰æ‹©ä¸ä¸Šè¿°æå…¶ç›¸ä¼¼çš„é€‰é¢˜ï¼å¿…é¡»æ¢ä¸ªå·¥å…·æˆ–æ¢ä¸ªè§’åº¦ï¼
    
    âœ¨ **æ‰¶æŒæ–°è¯**ï¼šè¯·ä¼˜å…ˆå…³æ³¨æƒ…æŠ¥ä¸­æåˆ°çš„ã€ç”Ÿåƒ»æŠ€æœ¯åè¯ã€‘ï¼ˆå¦‚ AutoGLM, Dayflow ç­‰ï¼‰ï¼Œå¦‚æœå®ƒä»¬æœ‰ä»·å€¼ï¼Œä¼˜å…ˆå…¥é€‰ã€‚
    
    å½“å‰ç­–ç•¥ï¼šã€{CURRENT_CONFIG['name']}ã€‘
    {CURRENT_CONFIG['prompt_suffix']}
    """
    
    try:
        # å•æ¬¡æ‰«æç”¨ chat æ¨¡å‹ï¼ˆå¿«ã€ä¾¿å®œï¼‰ï¼Œç»¼åˆå†³ç­–æ‰ç”¨ reasoner
        @retryable
        @track_cost(context="step3_final_decision")
        def _chat_create():
            return client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": f"ã€æ·±åº¦éªŒè¯æƒ…æŠ¥ã€‘\n{scan_data}"}
                ],
                stream=True
            )

        response = _chat_create()

        log_print("\n" + "="*20 + " é€‰é¢˜æŠ¥å‘Š " + "="*20 + "\n")
        collected = []
        for chunk in response:
            if chunk.choices[0].delta.content:
                c = chunk.choices[0].delta.content
                log_print(c, end="", flush=True)
                collected.append(c)
        return "".join(collected)
    except Exception as e:
        log_print(f"âŒ å†³ç­–å¤±è´¥: {e}")
        return f"å¤±è´¥: {e}"

EDITOR_PROMPT = """
ä½ å«"ç‹å¾€AI"ï¼Œä¸“æ³¨ AI å·¥ä½œæµçš„ç¡¬æ ¸åšä¸»ã€‚
è¯·ç­›é€‰ 3 ä¸ªã€ä»·å€¼æœ€é«˜ã€‘çš„é€‰é¢˜ï¼Œ**å¿…é¡»è¦†ç›–è‡³å°‘ 2 ç§å¿ƒç†ç­–ç•¥**ä»¥ä¿è¯å¤šæ ·æ€§ã€‚

## ä»·å€¼å…¬å¼
**é€‰é¢˜ä»·å€¼** = (ä¿¡æ¯å·® Ã— è®¤çŸ¥å†²å‡») + (ç—›ç‚¹å¼ºåº¦ Ã— è§£å†³æ•ˆç‡) - é˜…è¯»é—¨æ§›

## å¿ƒç†å­¦ç­–ç•¥ï¼ˆ3 ä¸ªé€‰é¢˜å¿…é¡»è¦†ç›–è‡³å°‘ 2 è·¯ï¼‰
1. **é”šç‚¹æ•ˆåº” (å€ŸåŠ¿é¡¶æµ)**ï¼šå€ŸåŠ© DeepSeek/Cursor/Gemini ç­‰é¡¶æµäº§å“çš„çŸ¥ååº¦ï¼Œç”¨æˆ·æ›´å®¹æ˜“ç‚¹å‡»ã€‚
2. **å³æ—¶æ»¡è¶³ (æ•ˆèƒ½ç¥å™¨)**ï¼šè®©ç”¨æˆ·è§‰å¾—"çœ‹å®Œå°±èƒ½ç”¨"ï¼Œè·å¾—æ­£åé¦ˆã€‚å¦‚"3åˆ†é’Ÿå­¦ä¼š"ã€"å…è´¹ç™½å«–"ã€‚
3. **æŸå¤±åŒæ¶ (é¿å‘/è®¤çŸ¥)**ï¼šè§¦å‘ç”¨æˆ·"å®³æ€•è¸©å‘"æˆ–"å®³æ€•è½å"çš„å¿ƒç†ã€‚å¦‚"ç¿»è½¦ç°åœº"ã€"æ–°è¶‹åŠ¿è§£è¯»"ã€‚

ğŸ›¡ï¸ **è´¨é‡è¿‡æ»¤çº¢çº¿**ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š
1. **æ‹’ç»ä½è´¨å†…å®¹**ï¼šå‰”é™¤æ¯«æ— æ–°æ„çš„"æ­£ç¡®çš„åºŸè¯"å’Œå†·é—¨æ— åå°å·¥å…·ã€‚
2. **å¤§å‚æ–°åŠ¨ä½œä¼˜å…ˆ**ï¼šGoogleã€OpenAIã€DeepSeekã€Anthropic ç­‰å¤§å‚çš„æ–°å‘å¸ƒã€æ–°åŠŸèƒ½ä¼˜å…ˆçº§æœ€é«˜ã€‚
3. **å‰æ²¿è¶‹åŠ¿ä¼˜å…ˆ**ï¼šæ–°çš„ Agent ç©æ³•ã€æ–°çš„å¼€æºé»‘é©¬é¡¹ç›®ã€æ–°çš„ç¡¬ä»¶ä½“éªŒï¼ˆå¦‚ AI è€³æœº/æ‰‹æœºï¼‰å€¼å¾—å…³æ³¨ã€‚

å†³ç­–é€»è¾‘ï¼š
1. **æ—¢è¦å®æ“ä¹Ÿè¦è®¤çŸ¥**ï¼šä¸è¦åªç›¯ç€"çœæ—¶é—´"çš„å°å·¥å…·ã€‚å¦‚æœæœ‰ä¸€ä¸ªæ–°çš„æŠ€æœ¯è¶‹åŠ¿ï¼Œå³ä½¿æš‚æ—¶ä¸èƒ½ä¸‹è½½ï¼Œåªè¦èƒ½å¸¦æ¥"è®¤çŸ¥éœ‡æ’¼"ï¼Œä¹Ÿæ˜¯å¥½é€‰é¢˜ã€‚
2. **æ‹’ç»è¿‡åº¦è¥é”€**ï¼šå‰”é™¤é‚£äº›åªæœ‰è¥é”€å™±å¤´æ²¡æœ‰å®è´¨å†…å®¹çš„å·¥å…·ã€‚
3. **å…³è”çƒ­ç‚¹**ï¼šå¦‚æœæ¶‰åŠ WATCHLIST ä¸­çš„äº§å“ï¼ŒåŠ åˆ†ã€‚

è¾“å‡ºæ ¼å¼ï¼š
### é€‰é¢˜ 1ï¼š[æ ‡é¢˜] (éœ€æå…·å¸å¼•åŠ›)
* **å¿ƒç†é”šç‚¹**ï¼š[é”šç‚¹æ•ˆåº” / å³æ—¶æ»¡è¶³ / æŸå¤±åŒæ¶]
* **æ ¸å¿ƒä»·å€¼**ï¼š[ç”¨æˆ·çœ‹å®Œèƒ½å¾—åˆ°ä»€ä¹ˆï¼Ÿæ–°çŸ¥ï¼ŸæŠ€èƒ½ï¼Ÿé¿å‘ï¼Ÿ]
* **çƒ­åº¦è¯„çº§**ï¼š[â­â­â­â­â­]
* **æ¨èç†ç”±**ï¼š[ä¸ºä»€ä¹ˆè¿™ä¸ªé€‰é¢˜ç°åœ¨å€¼å¾—å†™ï¼Ÿ]
---
## ä»Šæ—¥ä¸»æ¨
å‘Šè¯‰æˆ‘ä¸å†™ä¼šåæ‚”çš„é‚£ä¸ª (ä»·å€¼æœ€é«˜çš„)ï¼Œå¹¶è¯´æ˜å®ƒå‘½ä¸­äº†å“ªä¸ªå¿ƒç†é”šç‚¹ã€‚
"""

def auto_init_workflow() -> None:
    """è‡ªåŠ¨åˆå§‹åŒ–åç»­å·¥ä½œæµæ–‡ä»¶å¤¹å’Œæ–‡ä»¶"""
    log_print("\nâš™ï¸ æ­£åœ¨åˆå§‹åŒ–åç»­å·¥ä½œæµ...")
    
    # 1. é¢„åˆ›å»ºæ‰€æœ‰é˜¶æ®µæ–‡ä»¶å¤¹
    from config import get_stage_dir, get_research_notes_file
    stages = ["research", "drafts", "publish", "assets"]
    for stage in stages:
        path = get_stage_dir(stage)
        log_print(f"   ğŸ“‚ ç›®å½•å°±ç»ª: {path}")
        
    # 2. åˆ›å»ºç©ºç™½ç ”ç©¶ç¬”è®°
    notes_file = get_research_notes_file()
    if not os.path.exists(notes_file):
        with open(notes_file, "w", encoding="utf-8") as f:
            f.write("# ç ”ç©¶ç¬”è®°\n\nè¯´æ˜ï¼šæ­¤æ–‡ä»¶é€šå¸¸ç”± `python run.py research` è‡ªåŠ¨ç”Ÿæˆã€‚\nå¦‚éœ€äººå·¥è¡¥å……ï¼Œè¯·åœ¨æ­¤å¤„è¿½åŠ ä½ çš„å…³é”®å‘ç°ä¸å¼•ç”¨é“¾æ¥ã€‚\n")
        log_print(f"   ğŸ“„ ç¬”è®°æ–‡ä»¶å·²åˆ›å»º: {notes_file}")
    
    # 3. æç¤ºä¸‹ä¸€æ­¥
    log_print("\nğŸ’¡ ä¸‹ä¸€æ­¥ï¼š")
    log_print("   - å¯ç»§ç»­è¿è¡Œ hunt è·å–æ›´å¤šé€‰é¢˜")
    log_print("   - æˆ–è¿è¡Œ `python run.py final` ç»¼åˆæ‰€æœ‰æŠ¥å‘Šï¼Œè·å¾— 3 ä¸ªæç¤ºè¯")

def save_report(raw_data: str, analysis: str, directed_topic: Optional[str] = None) -> None:
    filename = get_topic_report_file()
    mode_info = f"å®šå‘æœç´¢: {directed_topic}" if directed_topic else CURRENT_CONFIG['name']
    content = f"# ğŸš€ é€‰é¢˜é›·è¾¾æŠ¥å‘Š v4.0 ({mode_info})\n\n**æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n**ç­–ç•¥**: {CURRENT_CONFIG['strategy']}\n\n## æ·±åº¦éªŒè¯æƒ…æŠ¥\n\n{raw_data}\n\n---\n\n## é€‰é¢˜åˆ†æ\n\n{analysis}"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(content)
    log_print(f"\n\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜: {filename}")
    
    # ä¿å­˜åè‡ªåŠ¨åˆå§‹åŒ–å·¥ä½œæµ
    auto_init_workflow()

def main(topic=None, dry_run=False):
    """
    é€‰é¢˜é›·è¾¾ä¸»å…¥å£
    å‚æ•°:
        topic: å¯é€‰ï¼ŒæŒ‡å®šæœç´¢ä¸»é¢˜ã€‚
        dry_run: èŠ‚æµæ¨¡å¼ï¼Œä¸è°ƒç”¨ APIã€‚
    """
    mode_text = f"å®šå‘æœç´¢: {topic}" if topic else "å…¨ç½‘é›·è¾¾"
    if dry_run:
        mode_text += " (ğŸ§ª DRY RUN)"
    
    log_print("\n" + "="*60 + f"\nğŸš€ é€‰é¢˜é›·è¾¾ v4.0 ({mode_text}) - ç‹å¾€AI\n" + "="*60 + "\n")
    
    if dry_run:
        log_print("ğŸ§ª [Mock] æ­£åœ¨ç”Ÿæˆæ¨¡æ‹Ÿçƒ­ç‚¹æŠ¥å‘Š...")
        raw_data = "Source[Mock]: Trending AI news about Google Ears and Agentic workflow."
        analysis = """
### é€‰é¢˜ 1ï¼šGoogle AIè€³æœºæ·±åº¦è¯„æµ‹ï¼šå®ƒçœŸçš„èƒ½â€œå¬æ‡‚â€ä½ çš„å·¥ä½œæµå—ï¼Ÿ
* **å¿ƒç†é”šç‚¹**ï¼šé”šç‚¹æ•ˆåº”
* **æ ¸å¿ƒä»·å€¼**ï¼šæŠ¢å AIç¡¬ä»¶é¦–å‘è¯„æµ‹è®¤çŸ¥ã€‚
* **çƒ­åº¦è¯„çº§**ï¼šâ­â­â­â­â­
* **æ¨èç†ç”±**ï¼šGoogleæœ€æ–°ç¡¬ä»¶åŠ¨å‘ã€‚

---
## ä»Šæ—¥ä¸»æ¨
Google AIè€³æœºè¯„æµ‹ï¼Œå‘½ä¸­äº†é”šç‚¹æ•ˆåº”ã€‚
"""
        save_report(raw_data, analysis, directed_topic=topic)
        log_print("\nâœ… [Mock] é€‰é¢˜é›·è¾¾å®Œæˆï¼")
        return

    search_tool = WebSearchTool()
    
    with httpx.Client(proxy=PROXY_URL, timeout=REQUEST_TIMEOUT) as http_client:
        client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL, http_client=http_client)
        
        # åŠ è½½å†å²è®°å½•ç”¨äºå»é‡
        history = load_history()
        history_text = "\n".join([f"- {h['date']}: {h['topic']} ({h['angle']})" for h in history])
        if not history_text: history_text = "æ— ï¼ˆè¿™æ˜¯ç¬¬ä¸€ç¯‡ï¼‰"
        
        # 1. å¹¿åŸŸæ‰«æ / å®šå‘æœç´¢
        search_plan = step1_broad_scan_and_plan(client, search_tool, directed_topic=topic)
        
        # 2. æ·±åº¦éªŒè¯
        raw_data = step2_deep_scan(search_plan, search_tool, directed_topic=topic)
        
        # 3. å†³ç­–ï¼ˆä¼ å…¥å†å²è®°å½•ç”¨äºå»é‡ï¼‰
        analysis = step3_final_decision(raw_data, client, history_text, directed_topic=topic)
        
        # 4. ä¿å­˜
        save_report(raw_data, analysis, directed_topic=topic)
    
    log_print("\nâœ… é€‰é¢˜é›·è¾¾å®Œæˆï¼")

def _extract_topic_frequencies(reports_content: str) -> Dict[str, Tuple[int, float, str]]:
    """
    v4.8: ä»å¤šä»½æŠ¥å‘Šä¸­æå–å…³é”®è¯å‡ºç°é¢‘ç‡ï¼Œå¸¦é¢†åŸŸæƒé‡
    è¿”å›: {keyword: (raw_count, weighted_score, category)}
    
    é¢†åŸŸæ•æ„Ÿåº¦æƒé‡ (PRPè¦æ±‚):
    - ç¡¬æ ¸æŠ€æœ¯ç±»: 2.0
    - æŠ•èµ„é‡‘èç±»: 0.5
    - é€šç”¨åœºæ™¯ç±»: 1.0 (ä½†åœ¨æ´å¯Ÿä¸­æ ‡è®°ä¸º"ä¿®é¥°è¯­")
    """
    from collections import Counter
    
    # åˆ†ç±»å…³é”®è¯åŠå…¶æƒé‡
    KEYWORD_CATEGORIES = {
        # === ç¡¬æ ¸æŠ€æœ¯ç±» (æƒé‡ 2.0) ===
        "tech": {
            "weight": 2.0,
            "keywords": [
                "DeepSeek", "Cursor", "Gemini", "Claude", "GPT", "Kimi", "Copilot",
                "Windsurf", "Bolt", "Lovable", "ç§˜å¡”", "è±†åŒ…", "é€šä¹‰", "æ™ºè°±", "AutoGLM",
                "Coze", "Agent", "æ™ºèƒ½ä½“", "MCP", "RAG", "Workflow", "å·¥ä½œæµ",
                "æœ¬åœ°éƒ¨ç½²", "Ollama", "vLLM", "Prompt", "æç¤ºè¯",
                "æ¶æ„å›¾", "æµç¨‹å›¾", "æ€ç»´å¯¼å›¾", "æ–‡æ¡£åˆ†æ", "ä»£ç ç”Ÿæˆ",
                "API", "SDK", "å¼€æº", "GitHub"
            ]
        },
        # === æŠ•èµ„é‡‘èç±» (æƒé‡ 0.5) ===
        "finance": {
            "weight": 0.5,
            "keywords": [
                "è‚¡ä»·", "ä¸Šå¸‚", "è´¢æŠ¥", "æš´æ¶¨", "æš´è·Œ", "å¸‚å€¼", "èèµ„", "IPO",
                "æŠ•èµ„", "è‚¡ç¥¨", "éŸ­èœ", "å‰²éŸ­èœ", "ç‚’è‚¡"
            ]
        },
        # === é€šç”¨åœºæ™¯ç±» (æƒé‡ 1.0ï¼Œä½†æ ‡è®°ä¸ºä¿®é¥°è¯­) ===
        "generic": {
            "weight": 1.0,
            "keywords": [
                "å…è´¹", "å¹³æ›¿", "ç™½å«–", "é¿å‘", "ç¿»è½¦", "æ•™ç¨‹", "çˆ†æ¬¾",
                "å®æ—¶ç¿»è¯‘", "AI è€³æœº", "æ‰‹æœºåŠ©æ‰‹"
            ]
        }
    }
    
    results = {}
    content_lower = reports_content.lower()
    
    for category, config in KEYWORD_CATEGORIES.items():
        weight = config["weight"]
        for kw in config["keywords"]:
            count = content_lower.count(kw.lower())
            if count > 0:
                weighted_score = count * weight
                results[kw] = (count, weighted_score, category)
    
    # æŒ‰åŠ æƒåˆ†æ•°æ’åºï¼Œè¿”å›å‰10
    sorted_results = dict(sorted(results.items(), key=lambda x: x[1][1], reverse=True)[:10])
    return sorted_results


def _generate_topic_insights(freq: Dict[str, Tuple[int, float, str]], reports_count: int) -> str:
    """
    v4.8: æ ¹æ®é¢‘ç‡ç»Ÿè®¡ç”Ÿæˆé€‰é¢˜æ´å¯Ÿï¼Œå¸¦é¢†åŸŸæ ‡ç­¾
    freq: {keyword: (raw_count, weighted_score, category)}
    """
    if not freq:
        return "æš‚æ— é«˜é¢‘å…³é”®è¯ç»Ÿè®¡ã€‚"
    
    CATEGORY_LABELS = {
        "tech": "ğŸ”§ ç¡¬æ ¸æŠ€æœ¯",
        "finance": "ğŸ’° é‡‘èç±»(é™æƒ)",
        "generic": "ğŸ“ ä¿®é¥°è¯­"
    }
    
    insights = []
    insights.append(f"ğŸ“Š **å…³é”®è¯çƒ­åº¦ç»Ÿè®¡ v4.8** (æ¥è‡ª {reports_count} ä»½æŠ¥å‘Šï¼Œå·²åº”ç”¨é¢†åŸŸæƒé‡)ï¼š")
    insights.append("   âš ï¸ æ³¨æ„ï¼šæŠ€æœ¯ç±»å…³é”®è¯æƒé‡Ã—2.0ï¼Œé‡‘èç±»Ã—0.5ï¼Œä¿®é¥°è¯­ä»…ä¾›å‚è€ƒ")
    insights.append("")
    
    for kw, (raw_count, weighted_score, category) in freq.items():
        label = CATEGORY_LABELS.get(category, "")
        
        if category == "tech":
            if weighted_score >= 6:
                insights.append(f"   ğŸ”¥ğŸ”¥ğŸ”¥ **{kw}** [{label}]: {raw_count}æ¬¡ â†’ åŠ æƒ{weighted_score:.1f} (æé«˜ä¼˜å…ˆçº§)")
            elif weighted_score >= 4:
                insights.append(f"   ğŸ”¥ğŸ”¥ **{kw}** [{label}]: {raw_count}æ¬¡ â†’ åŠ æƒ{weighted_score:.1f} (é«˜ä¼˜å…ˆçº§)")
            else:
                insights.append(f"   ğŸ”¥ **{kw}** [{label}]: {raw_count}æ¬¡ â†’ åŠ æƒ{weighted_score:.1f}")
        elif category == "finance":
            insights.append(f"   âš ï¸ **{kw}** [{label}]: {raw_count}æ¬¡ â†’ åŠ æƒ{weighted_score:.1f} (éœ€è½¬åŒ–ä¸ºæŠ€æœ¯è§†è§’)")
        else:  # generic
            insights.append(f"   ğŸ“ {kw} [{label}]: {raw_count}æ¬¡ (ä»…ä½œä¿®é¥°ï¼Œä¸ä½œä¸ºæ ¸å¿ƒé€‰é¢˜ä¾æ®)")
    
    return "\n".join(insights)


def final_summary(dry_run=False):
    """ç»¼åˆå½“å¤©æ‰€æœ‰æŠ¥å‘Šï¼Œç»™å‡ºæœ€ç»ˆé€‰é¢˜æ¨èå’Œä¸‰ä¸ªæç¤ºè¯"""
    import glob
    from config import get_today_dir
    
    title_text = "ğŸ¯ ç»¼åˆé€‰é¢˜å†³ç­– v5.0 - å•é€‰é¢˜ç¡®å®šæ¨¡å¼"
    if dry_run:
        title_text += " (ğŸ§ª DRY RUN)"
        
    log_print("\n" + "="*60)
    log_print(title_text)
    log_print("="*60 + "\n")
    
    # 1. è¯»å–å½“å¤©æ‰€æœ‰æŠ¥å‘Š
    topics_dir = os.path.join(get_today_dir(), "1_topics")
    reports = glob.glob(os.path.join(topics_dir, "report_*.md"))
    
    if not reports:
        if dry_run:
            log_print("ğŸ§ª [Mock] æœªæ‰¾åˆ°æŠ¥å‘Šï¼Œç”Ÿæˆæ¨¡æ‹Ÿæœ€ç»ˆå†³ç­–...")
            final_report = os.path.join(topics_dir, "FINAL_DECISION.md")
            mock_decision = """
### ğŸ† ä»Šæ—¥æœ€ç»ˆé€‰é¢˜
**æ ‡é¢˜**ï¼šåˆ«ä¹±ç”¨Cursoräº†ï¼è¿™5ä¸ªéšè—è®¾ç½®ï¼Œè®©ä½ çš„AIç¼–ç¨‹æ•ˆç‡ç¿»å€
**å¿ƒç†é”šç‚¹**ï¼šæŸå¤±åŒæ¶
**ä¸€å¥è¯å–ç‚¹**ï¼šæŒæ¡éšè—è®¾ç½®ï¼Œæ•ˆç‡ç¿»å€ã€‚
**å…³é”®è¯**ï¼šCursor, AIç¼–ç¨‹

### ğŸ“¡ æç¤ºè¯ 1ï¼šFast Research
```
1. æœç´¢ Cursor æœ€æ–°éšè—è®¾ç½®ã€‚
```
"""
            with open(final_report, "w", encoding="utf-8") as f:
                f.write(f"# ğŸ† ä»Šæ—¥æœ€ç»ˆé€‰é¢˜å†³ç­– (ğŸ§ª Mock)\n\n{mock_decision}")
            save_topic_to_history("Cursor æ•ˆç‡è®¾ç½®", "Mock å†³ç­–")
            log_print("\nâœ… [Mock] ç»¼åˆé€‰é¢˜å®Œæˆï¼")
            return
        log_print("âŒ ä»Šæ—¥æš‚æ— æŠ¥å‘Šï¼Œè¯·å…ˆè¿è¡Œ `python run.py hunt`")
        return
    
    # é€»è¾‘ä¿®æ­£ï¼šæŒ‰æ—¶é—´æ’åºï¼ˆæ–‡ä»¶ååç¼€ï¼‰ï¼Œè¯†åˆ«æœ€åä¸€ä»½æŠ¥å‘Š
    sorted_reports = sorted(reports)
    latest_report_path = sorted_reports[-1]
    
    log_print(f"ğŸ“Š æ‰¾åˆ° {len(reports)} ä»½æŠ¥å‘Šï¼Œæœ€æ–°æŠ¥å‘Šä¸º: {os.path.basename(latest_report_path)}")
    
    all_content = []
    latest_recommendation = ""
    directed_topics = []  # {topic: recommendation}
    directed_recommendations = {}  # v4.9: å­˜å‚¨æ¯ä¸ªå®šå‘ä¸»é¢˜çš„ä¸»æ¨å†…å®¹
    
    for r in sorted_reports:
        name = os.path.basename(r)
        with open(r, "r", encoding="utf-8") as f:
            content = f.read()
            all_content.append(f"=== {name} ===\n{content}")
            
            # æå–æŠ¥å‘Šä¸­çš„å®šå‘æœç´¢ä¸»é¢˜
            directed_match = re.search(r'# ğŸš€ é€‰é¢˜é›·è¾¾æŠ¥å‘Š v4.0 \(å®šå‘æœç´¢: (.*?)\)', content)
            if directed_match:
                d_topic = directed_match.group(1).strip()
                if d_topic not in directed_topics:
                    directed_topics.append(d_topic)
                
                # v4.9: æå–è¯¥å®šå‘æŠ¥å‘Šçš„â€œä»Šæ—¥ä¸»æ¨â€ä½œä¸ºé€‰é¢˜é”šç‚¹
                rec_match = re.search(r'## ä»Šæ—¥ä¸»æ¨\s*(.*?)(?:\n\n|\n##|$)', content, re.DOTALL)
                if rec_match:
                    directed_recommendations[d_topic] = rec_match.group(1).strip()
            
            # å¦‚æœæ˜¯æœ€æ–°æŠ¥å‘Šï¼Œå°è¯•æå–â€œä»Šæ—¥ä¸»æ¨â€
            if r == latest_report_path:
                match = re.search(r'## ä»Šæ—¥ä¸»æ¨\s*(.*?)(?:\n\n|\n##|$)', content, re.DOTALL)
                if match:
                    latest_recommendation = match.group(1).strip()

    combined = "\n\n".join(all_content)
    
    # === v4.1: é¢„å¤„ç† - å…³é”®è¯é¢‘ç‡åˆ†æ ===
    topic_freq = _extract_topic_frequencies(combined)
    topic_insights = _generate_topic_insights(topic_freq, len(reports))

    # === v4.9: å¢å¼º Prompt - å®šå‘é”šç‚¹ä¸å‘æ•£ç­–ç•¥ ===
    weighted_instruction = ""
    
    if directed_topics:
        # æ„å»ºå®šå‘ä¸»é¢˜åŠå…¶å¯¹åº”çš„æ¨èé€‰é¢˜
        directed_anchor_text = ""
        for dt in directed_topics:
            rec = directed_recommendations.get(dt, "æœªæå–åˆ°å…·ä½“æ¨è")
            directed_anchor_text += f"\n    - **å®šå‘ä¸»é¢˜**: {dt}\n      **è¯¥æŠ¥å‘Šä¸»æ¨**: {rec[:200]}..."
        
        weighted_instruction += f"""
    ğŸ¯ **å®šå‘é€‰é¢˜é”šç‚¹ (ANCHOR - æœ€é«˜ä¼˜å…ˆçº§)**ï¼š
    ç”¨æˆ·é€šè¿‡ `-t` å‚æ•°æ˜ç¡®æŒ‡å®šäº†ä»¥ä¸‹ä¸»é¢˜ï¼Œè¿™æ˜¯ä»Šæ—¥é€‰é¢˜çš„ã€Œæ ¸å¿ƒé”šç‚¹ã€ï¼š
    {directed_anchor_text}
    
    ğŸš¨ **å¼ºåˆ¶è¦æ±‚ (ä¸å¯è¿å)**ï¼š
    1. **ç¬¬ä¸€é€‰é¢˜å¿…é¡»æ˜¯å®šå‘ä¸»é¢˜çš„ã€Œç²¾ç¡®æ‰§è¡Œç‰ˆã€**ï¼šåŸºäºä¸Šè¿°æŠ¥å‘Šä¸»æ¨å†…å®¹ï¼Œè¾“å‡ºå¯ç›´æ¥ä½¿ç”¨çš„çˆ†æ¬¾æ ‡é¢˜ã€‚
    2. **ç¬¬äºŒã€ç¬¬ä¸‰é€‰é¢˜å¿…é¡»å›´ç»•å®šå‘ä¸»é¢˜å‘æ•£**ï¼šå…è®¸åç§»è§’åº¦ï¼ˆå¦‚é¿å‘ã€å®æµ‹ã€è¿›é˜¶ï¼‰ï¼Œä½†ä¸èƒ½åç¦»ç”¨æˆ·çš„æœç´¢æ„å›¾ã€‚
    3. **ç¦æ­¢è¾“å‡ºä¸å®šå‘ä¸»é¢˜æ— å…³çš„é€‰é¢˜**ï¼šå³ä½¿å…¶ä»–çƒ­ç‚¹å¾ˆç«ï¼Œä¹Ÿä¸èƒ½å–ä»£å®šå‘é€‰é¢˜çš„ä½ç½®ã€‚
    """

    if latest_recommendation and not directed_topics:
        # åªæœ‰åœ¨æ²¡æœ‰å®šå‘ä¸»é¢˜æ—¶ï¼Œæ‰ä½¿ç”¨æœ€æ–°æŠ¥å‘Šçš„ä¸»æ¨ä½œä¸ºå‚è€ƒ
        weighted_instruction += f"""
    â­ **æœ€æ–°æƒ…æŠ¥å‚è€ƒ (Reference)**ï¼š
    æœ€è¿‘çš„ä¸€ä»½æŠ¥å‘Šå»ºè®®ï¼šã€{latest_recommendation[:200]}ã€‘
    """

    FINAL_PROMPT = f"""
    {weighted_instruction}
    
    ä½ æ˜¯"ç‹å¾€AI"ï¼Œä¸€ä¸ª**ä¸“æ³¨ç¡¬æ ¸ AI å·¥ä½œæµä¸ææ•ˆæŠ€å·§**çš„æŠ€æœ¯åšä¸»ï¼Œä¸æ˜¯é‡‘èåˆ†æå¸ˆï¼Œä¹Ÿä¸æ˜¯æ–°é—»æ¬è¿å·¥ã€‚
    ä½ çš„ä»»åŠ¡ï¼šç»¼åˆåˆ†æä»Šå¤©çš„æ‰€æœ‰é€‰é¢˜æŠ¥å‘Šï¼Œé€‰å‡ºã€1ä¸ªæœ€ç»ˆé€‰é¢˜ã€‘ï¼Œå¹¶è¾“å‡º3ä¸ªç»“æ„åŒ–æç¤ºè¯ã€‚
    
    ## ğŸ§  æ ¸å¿ƒäººè®¾ä¸ä»·å€¼è§‚ (ä¸å¯è¿èƒŒ)
    1. **å”¯æŠ€æœ¯è®º**ï¼šå³ä½¿æ˜¯åˆ†æå…¬å¸ä¸Šå¸‚æˆ–å¤§å‚åŠ¨ä½œï¼Œè½è„šç‚¹ä¹Ÿå¿…é¡»æ˜¯**åº•å±‚æŠ€æœ¯ã€å·¥ä½œæµå˜é©ã€Prompt æŠ€å·§**ï¼Œè€Œéè‚¡ä»·ã€è´¢æŠ¥æˆ–å…«å¦ã€‚
    2. **æ‹’ç»æŠ•æœº**ï¼šç¦æ­¢ç”Ÿæˆçº¯ç²¹çš„æŠ•èµ„ç†æµ‹ã€è‚¡å¸‚åˆ†æå†…å®¹ã€‚å¦‚æœæ¶‰åŠåˆ°å£ä»ç§‘æŠ€ç­‰å…¬å¸ï¼Œå¿…é¡»è½¬åŒ–ä¸ºâ€œå›½äº§ GPU çš„æŠ€æœ¯ç”Ÿæ€æŒ‘æˆ˜â€æˆ–â€œAI ç®—åŠ›éƒ¨ç½²é¿å‘â€ã€‚
    3. **ç¡¬æ ¸ä¼˜å…ˆ**ï¼šCoze, Cursor, Agent å·¥ä½œæµ, æœ¬åœ°éƒ¨ç½²ç­‰å†…å®¹çš„æƒé‡æ°¸è¿œé«˜äºç®€å•çš„â€œAI è¶£é—»â€ã€‚
    
    ## ğŸ”¥ ç³»ç»Ÿé¢„å¤„ç†ï¼šå…³é”®è¯çƒ­åº¦åˆ†æ
    {topic_insights}
    
    âš ï¸ **é‡è¦å†³ç­–åŸåˆ™**ï¼š
    1. **å®šå‘æŒ‡ä»¤ç»å¯¹ä¼˜å…ˆ**ï¼šå¦‚æœç”¨æˆ·æŒ‡å®šäº†ä¸»é¢˜ï¼ˆè§ä¸Šæ–‡ï¼‰ï¼Œå¿…é¡»ä¼˜å…ˆä»¥æ­¤ä¸ºä¸­å¿ƒè¿›è¡Œå‘æ•£ã€‚
    2. **æ—¶æ•ˆæ€§ä¸ä»·å€¼å¹³è¡¡**ï¼šè¶Šæ™šç”Ÿæˆçš„æŠ¥å‘Šæƒé‡è¶Šé«˜ï¼Œä½†â€œç¡¬æ ¸ä»·å€¼â€æ˜¯å”¯ä¸€çš„ä¸€ç¥¨å¦å†³æƒã€‚
    3. **å…³é”®è¯é˜²å¹²æ‰°**ï¼šä¸è¦è¢«â€œé¿å‘â€ã€â€œå…è´¹â€ç­‰é€šç”¨é«˜é¢‘è¯å¸¦åï¼Œå®ƒä»¬åªæ˜¯ä¿®é¥°è¯­ï¼Œæ ¸å¿ƒå¿…é¡»æ˜¯å…·ä½“çš„æŠ€æœ¯æˆ–äº§å“ã€‚

## ä»·å€¼å…¬å¼ (å¿ƒç†å­¦é©±åŠ¨)
**é€‰é¢˜ä»·å€¼** = (ä¿¡æ¯å·® Ã— è®¤çŸ¥å†²å‡») + (ç—›ç‚¹å¼ºåº¦ Ã— è§£å†³æ•ˆç‡) - é˜…è¯»é—¨æ§›

å¿ƒç†å­¦ç­–ç•¥ï¼ˆä¸‰é€‰ä¸€ï¼Œä½†å¯æ··æ­ï¼‰ï¼š
1. **é”šç‚¹æ•ˆåº” (å€ŸåŠ¿)**ï¼šå€ŸåŠ©é¡¶æµäº§å“çš„çŸ¥ååº¦ï¼Œç”¨æˆ·æ›´å®¹æ˜“ç‚¹å‡»ï¼ˆå¦‚ "DeepSeek éšè—ç©æ³•"ï¼‰
2. **å³æ—¶æ»¡è¶³ (æ•ˆèƒ½)**ï¼šè®©ç”¨æˆ·è§‰å¾—"çœ‹å®Œå°±èƒ½ç”¨"ï¼Œè·å¾—å³æ—¶æ­£åé¦ˆï¼ˆå¦‚ "3åˆ†é’Ÿå­¦ä¼š"ï¼‰
3. **æŸå¤±åŒæ¶ (é¿å‘)**ï¼šè®©ç”¨æˆ·å®³æ€•é”™è¿‡æˆ–è¸©å‘ï¼Œè§¦å‘ç´§è¿«æ„Ÿï¼ˆå¦‚ "åˆ«å†è¢«å‘äº†"ï¼‰

## é€‰é¢˜æ ‡å‡† (æŒ‰ä¼˜å…ˆçº§)
1. **é«˜é¢‘çƒ­ç‚¹ä¼˜å…ˆ**ï¼šå¤šæ¬¡å‡ºç°åœ¨æŠ¥å‘Šä¸­çš„å…³é”®è¯è¯´æ˜çƒ­åº¦æŒç»­ï¼Œä¼˜å…ˆé€‰æ‹©
2. **å¤§å‚åŠ¨ä½œä¼˜å…ˆ**ï¼šGoogle/OpenAI/DeepSeek ç­‰å¤§å‚çš„æ–°å‘å¸ƒã€æ–°åŠŸèƒ½ä¼˜å…ˆçº§æœ€é«˜
3. **ä»·å€¼å¤šå…ƒ**ï¼š
   - **è®¤çŸ¥ç±»**ï¼šè§£è¯»æ–°è¶‹åŠ¿ã€æ–°ç¡¬ä»¶ï¼ˆå¦‚ AI è€³æœºã€æ‰‹æœºæ™ºèƒ½ä½“ï¼‰ï¼Œæ»¡è¶³æ±‚çŸ¥æ¬²
   - **å®æ“ç±»**ï¼šçœŸæ­£çš„æ•ˆç‡ç¥å™¨ï¼ˆå¦‚ å…è´¹ç”»å›¾ï¼‰ï¼Œæ»¡è¶³å³æ—¶æ»¡è¶³å¿ƒç†
   - **é¿å‘ç±»**ï¼šç¿»è½¦ç°åœºã€æ™ºå•†ç¨æ­ç§˜ï¼Œæ»¡è¶³æŸå¤±åŒæ¶å¿ƒç†
4. **æ‹’ç»å¹³åº¸**ï¼šå‰”é™¤é‚£äº›"çœ‹èµ·æ¥æœ‰ç”¨ä½†å®é™…æ²¡å•¥ç”¨"çš„å·¥å…·

## è¾“å‡ºæ ¼å¼ (v5.0: å•é€‰é¢˜ç¡®å®šæ¨¡å¼)

### ğŸ† ä»Šæ—¥æœ€ç»ˆé€‰é¢˜ (THE ONE)
**æ ‡é¢˜**ï¼š[çˆ†æ¬¾æ ‡é¢˜ï¼Œ15-25å­—ï¼Œå¿…é¡»ç´§æ‰£ç”¨æˆ·çš„å®šå‘æœç´¢æ„å›¾]
**å¿ƒç†é”šç‚¹**ï¼š[é”šç‚¹æ•ˆåº” / å³æ—¶æ»¡è¶³ / æŸå¤±åŒæ¶ï¼Œé€‰ä¸€ä¸ªä¸»æ‰“]
**ä¸€å¥è¯å–ç‚¹**ï¼š[ç”¨æˆ·çœ‹å®Œèƒ½å¾—åˆ°ä»€ä¹ˆï¼Ÿè®¤çŸ¥å‡çº§ï¼Ÿè§£å†³ç—›ç‚¹ï¼Ÿé¿å¼€é™·é˜±ï¼Ÿ]
**å…³é”®è¯**ï¼š[3-5ä¸ªæœç´¢å…³é”®è¯ï¼Œç”¨äºåç»­ç´ ææœé›†]

### ğŸ’¡ å¤‡é€‰è§’åº¦ (ä¾›äººå·¥è°ƒæ•´å‚è€ƒï¼Œä¸ä½œä¸ºä¸»è¾“å‡º)
- **è§’åº¦A**ï¼š[ä»é¿å‘/ç¿»è½¦åˆ‡å…¥çš„æ ‡é¢˜æ€è·¯]
- **è§’åº¦B**ï¼š[ä»æ•ˆç‡æå‡/å³æ—¶æ»¡è¶³åˆ‡å…¥çš„æ ‡é¢˜æ€è·¯]

---

### ğŸ“¡ æç¤ºè¯ 1ï¼šFast Research (ç”¨äºè‡ªåŠ¨ç ”ç©¶ / research é˜¶æ®µ)
```
[è¯·ç”¨ä¸­æ–‡ï¼Œå‘Šè¯‰ Researcher éœ€è¦æœç´¢å“ªäº›å…·ä½“å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š
- å®˜æ–¹æ–‡æ¡£/å‘å¸ƒä¼š/Demo
- è¡Œä¸šä¸“å®¶çš„æ·±åº¦è§£è¯»/è¯„æµ‹
- ç”¨æˆ·çš„çœŸå®ä½“éªŒ/åæ§½
- ç«å“å¯¹æ¯”
æ ¼å¼è¦æ±‚ï¼šåˆ†æ¡åˆ—å‡ºï¼Œæ¯æ¡ä¸€ä¸ªæ˜ç¡®çš„æœç´¢ä»»åŠ¡]
```

### ğŸ¨ æç¤ºè¯ 2ï¼šè§†è§‰è„šæœ¬ (ç”¨äºé…å›¾æ–¹æ¡ˆ)
```
[è¯·ç”¨ä¸­æ–‡ï¼Œå»ºè®®éœ€è¦å‡†å¤‡çš„é…å›¾ï¼ŒåŒ…æ‹¬ï¼š
- å…³é”®æˆªå›¾ (å¦‚ï¼šæ–°åŠŸèƒ½ç•Œé¢ã€Demoæ¼”ç¤º)
- å¯¹æ¯”å›¾ (æ–°æ—§å¯¹æ¯”ã€ç«å“å¯¹æ¯”)
- æ¦‚å¿µå›¾ (å¦‚æœæ˜¯æŠ½è±¡æ¦‚å¿µï¼Œå¦‚ä½•å¯è§†åŒ–)
- å°é¢å›¾é£æ ¼å»ºè®® (é«˜å¤§ä¸Šã€ç§‘æŠ€æ„Ÿæˆ–æç®€é£)]
```

### ğŸ¨ è§†è§‰é…å›¾æŒ‡å— (Visual Guide)
**è¯´æ˜**ï¼šè¯·ä¸ºäººå·¥é…å›¾æä¾›è¯¦ç»†çš„ç”»é¢å»ºè®®ï¼Œå¸®åŠ©åšä¸»å¿«é€Ÿäº§å‡ºé«˜è´¨é‡ç´ æã€‚
[è¯·ç”¨ä¸­æ–‡åˆ—å‡ºä¸å°‘äº 3 å¼ å…³é”®é…å›¾çš„å»ºè®®ï¼š

å°é¢å›¾ï¼š[ç”»é¢æè¿°ï¼Œå¦‚ï¼šç§‘æŠ€æ„Ÿæµå…‰èƒŒæ™¯ï¼Œçªå‡ºæ ¸å¿ƒå…³é”®è¯]

å†…é¡µå›¾1ï¼š[æè¿°]

å†…é¡µå›¾2ï¼š[æè¿°]

å†…é¡µå›¾3ï¼š[æè¿°] ]
"""

    with httpx.Client(proxy=PROXY_URL, timeout=REQUEST_TIMEOUT) as http_client:
        client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL, http_client=http_client)
        
        try:
            @retryable
            @track_cost(context="final_summary")
            def _chat_create():
                return client.chat.completions.create(
                    model="deepseek-reasoner",
                    messages=[
                        {"role": "system", "content": FINAL_PROMPT},
                        {"role": "user", "content": f"ä»¥ä¸‹æ˜¯ä»Šæ—¥çš„æ‰€æœ‰é€‰é¢˜æŠ¥å‘Šï¼Œè¯·ç»¼åˆåˆ†æåç»™å‡ºæœ€ç»ˆæ¨èï¼š\n\n{combined}"}
                    ],
                    stream=True
                )

            response = _chat_create()
            
            log_print("\n" + "="*60)
            log_print("ğŸ† æœ€ç»ˆé€‰é¢˜æ¨è")
            log_print("="*60 + "\n")
            
            collected = []
            for chunk in response:
                if chunk.choices[0].delta.content:
                    c = chunk.choices[0].delta.content
                    log_print(c, end="", flush=True)
                    collected.append(c)
            
            # ä¿å­˜ç»¼åˆæŠ¥å‘Š
            final_report = os.path.join(topics_dir, "FINAL_DECISION.md")
            content_str = ''.join(collected)
            with open(final_report, "w", encoding="utf-8") as f:
                f.write(f"# ğŸ† ä»Šæ—¥æœ€ç»ˆé€‰é¢˜å†³ç­–\n\n**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n**ç»¼åˆæŠ¥å‘Šæ•°**: {len(reports)}\n\n{content_str}")
            
            log_print(f"\n\nğŸ“ ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: {final_report}")

            # === è‡ªåŠ¨æ›´æ–°å†å²è®°å½• (Memory Update) ===
            try:
                # ä¼˜åŒ–æ­£åˆ™ï¼šå…¼å®¹ä¸­è‹±æ–‡å†’å·ã€å¿½ç•¥å‰åç©ºæ ¼ã€å¤šè¡ŒåŒ¹é…
                # æ¨¡å¼1: **æ ‡é¢˜**: xxx
                title_pattern1 = r'\*\*æ ‡é¢˜\*\*\s*[:ï¼š]\s*(.+)'
                # æ¨¡å¼2: ### é€‰é¢˜ 1ï¼šxxx
                title_pattern2 = r'###\s*é€‰é¢˜\s*\d+\s*[:ï¼š]\s*(.+)'
                
                final_topic = None
                
                # å°è¯•åŒ¹é…
                match1 = re.search(title_pattern1, content_str)
                if match1:
                    final_topic = match1.group(1).strip()
                else:
                    match2 = re.search(title_pattern2, content_str)
                    if match2:
                        final_topic = match2.group(1).strip()
                
                if final_topic:
                    save_topic_to_history(final_topic, "ç»¼åˆå†³ç­–")
                else:
                    # Fallback: å°è¯•æå–ç¬¬ä¸€è¡Œæœ‰æ•ˆæ–‡æœ¬
                    lines = [l.strip() for l in content_str.split('\n') if l.strip() and not l.startswith('#')]
                    if lines:
                        fallback_title = lines[0][:50]  # å–å‰50å­—ç¬¦
                        save_topic_to_history(fallback_title, "ç»¼åˆå†³ç­–")
                        log_print(f"âš ï¸ ä½¿ç”¨ Fallback æ ‡é¢˜: {fallback_title}")
                    else:
                        log_print("âš ï¸ è­¦å‘Š: æ— æ³•ä»æŠ¥å‘Šä¸­æå–æœ€ç»ˆé€‰é¢˜æ ‡é¢˜ï¼Œå†å²è®°å½•æœªæ›´æ–°ã€‚")
                        log_print(f"   è°ƒè¯•ä¿¡æ¯: å†…å®¹å‰200å­— -> {content_str[:200].replace(chr(10), ' ')}")
            
            except Exception as e:
                 log_print(f"âš ï¸ å†å²è®°å½•æ›´æ–°å¤±è´¥: {e}")
            
        except Exception as e:
            log_print(f"âŒ ç»¼åˆåˆ†æå¤±è´¥: {e}")

    log_print("\nâœ… ç»¼åˆé€‰é¢˜å®Œæˆï¼")


# =============================================================================
# ä»¿å†™æ¨¡å¼ (Imitate Mode) v1.0
# =============================================================================

def _parse_reference_file(file_path: str) -> str:
    """
    è§£æå‚è€ƒæ–‡ç« æ–‡ä»¶ï¼Œæ”¯æŒ HTML/MD/TXT ç­‰æ ¼å¼
    è¿”å›çº¯æ–‡æœ¬å†…å®¹
    """
    import os
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"å‚è€ƒæ–‡ç« ä¸å­˜åœ¨: {file_path}")
    
    ext = os.path.splitext(file_path)[1].lower()
    
    with open(file_path, "r", encoding="utf-8") as f:
        raw_content = f.read()
    
    # HTML æ ¼å¼ï¼šæå–çº¯æ–‡æœ¬
    if ext in [".html", ".htm"]:
        soup = BeautifulSoup(raw_content, "html.parser")
        # ç§»é™¤ script å’Œ style æ ‡ç­¾
        for tag in soup(["script", "style", "nav", "header", "footer"]):
            tag.decompose()
        text = soup.get_text(separator="\n", strip=True)
        # æ¸…ç†å¤šä½™ç©ºè¡Œ
        lines = [line.strip() for line in text.split("\n") if line.strip()]
        return "\n".join(lines)
    
    # Markdown æˆ–çº¯æ–‡æœ¬ï¼šç›´æ¥è¿”å›
    return raw_content


def _fetch_url_content(url: str) -> str:
    """
    ä½¿ç”¨ Jina Reader æŠ“å– URL å†…å®¹
    """
    jina_url = f"https://r.jina.ai/{url}"
    log_print(f"   ğŸŒ æ­£åœ¨é€šè¿‡ Jina Reader æŠ“å–: {url}")
    
    try:
        with httpx.Client(proxy=PROXY_URL, timeout=REQUEST_TIMEOUT) as client:
            resp = client.get(jina_url)
            resp.raise_for_status()
            content = resp.text
            
            # ä¿å­˜åˆ°æœ¬åœ°ç¼“å­˜ä»¥ä¾¿è°ƒè¯•
            from config import get_today_dir
            import os
            cache_dir = os.path.join(get_today_dir(), "temp")
            os.makedirs(cache_dir, exist_ok=True)
            cache_file = os.path.join(cache_dir, "last_imitate_raw.md")
            with open(cache_file, "w", encoding="utf-8") as f:
                f.write(content)
            log_print(f"   âœ… æŠ“å–æˆåŠŸï¼Œå·²ç¼“å­˜è‡³: {cache_file}")
            
            return content
    except Exception as e:
        log_print(f"   âŒ URL æŠ“å–å¤±è´¥: {e}")
        raise


def imitate_mode(reference_input: str, dry_run: bool = False):
    """
    ä»¿å†™æ¨¡å¼ v1.1ï¼šæ”¯æŒæœ¬åœ°æ–‡ä»¶æˆ– URL
    
    æµç¨‹ï¼š
    1. è·å–å‚è€ƒå†…å®¹ï¼ˆæœ¬åœ°è§£ææˆ– URL æŠ“å–ï¼‰
    2. LLM åˆ†æï¼šæå–ä¸»é¢˜ã€å…³é”®è¯ã€ç»“æ„ã€ç´ æç±»å‹
    3. ç”Ÿæˆæœç´¢è®¡åˆ’
    4. æ‰§è¡Œæœç´¢å¹¶ç”Ÿæˆ report_*.md
    """
    log_print("\n" + "="*60)
    log_print("ğŸ“ ä»¿å†™æ¨¡å¼ v1.1 - çˆ†æ¬¾å†…å®¹åˆ†æä¸åˆ›ä½œ")
    log_print("="*60 + "\n")
    
    # 1. è·å–å‚è€ƒå†…å®¹
    article_content = ""
    is_url = reference_input.startswith(("http://", "https://"))
    
    if is_url:
        log_print(f"ğŸ”— æ£€æµ‹åˆ° URL è¾“å…¥: {reference_input}")
        try:
            article_content = _fetch_url_content(reference_input)
        except:
            return
    else:
        log_print(f"ğŸ“– æ­£åœ¨è§£ææœ¬åœ°å‚è€ƒæ–‡ç« : {reference_input}")
        try:
            article_content = _parse_reference_file(reference_input)
        except Exception as e:
            log_print(f"   âŒ è§£æå¤±è´¥: {e}")
            return

    try:
        # é™åˆ¶é•¿åº¦é¿å… Token æº¢å‡º
        if len(article_content) > 15000:
            article_content = article_content[:15000] + "\n\n...(å†…å®¹å·²æˆªæ–­)"
            log_print(f"   âš ï¸ å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­è‡³ 15000 å­—ç¬¦")
        log_print(f"   âœ… å†…å®¹å°±ç»ªï¼Œå…± {len(article_content)} å­—ç¬¦")
    except Exception as e:
        log_print(f"   âŒ å†…å®¹å¤„ç†å¤±è´¥: {e}")
        return
    
    # 2. LLM åˆ†ææ–‡ç« 
    log_print("\nğŸ§  DeepSeek æ­£åœ¨åˆ†ææ–‡ç« ç»“æ„ä¸ä¸»é¢˜...")
    
    ANALYZE_PROMPT = """ä½ æ˜¯å†…å®¹åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹çˆ†æ¬¾æ–‡ç« ï¼Œæå–å…³é”®ä¿¡æ¯ç”¨äºä»¿å†™ã€‚

## åˆ†æè¦æ±‚
è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼ˆä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ï¼‰ï¼š

```json
{
    "topic": "æ ¸å¿ƒä¸»é¢˜ï¼ˆä¸€å¥è¯ï¼Œå¦‚ï¼šCursor çš„éšè—æ•ˆç‡æŠ€å·§ï¼‰",
    "keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3", "å…³é”®è¯4", "å…³é”®è¯5"],
    "structure": "æ–‡ç« ç»“æ„ï¼ˆå¦‚ï¼šç—›ç‚¹å¼•å…¥-æ–¹æ¡ˆä»‹ç»-æ¡ˆä¾‹æ¼”ç¤º-æ€»ç»“è¡ŒåŠ¨ï¼‰",
    "material_types": ["ç´ æç±»å‹1", "ç´ æç±»å‹2"],
    "psychology": "å¿ƒç†é”šç‚¹ï¼ˆé”šç‚¹æ•ˆåº”/å³æ—¶æ»¡è¶³/æŸå¤±åŒæ¶ï¼‰",
    "search_queries": ["æœç´¢è¯1", "æœç´¢è¯2", "æœç´¢è¯3"]
}
```

## å­—æ®µè¯´æ˜
- topic: è¿™ç¯‡æ–‡ç« çš„æ ¸å¿ƒä¸»é¢˜æ˜¯ä»€ä¹ˆ
- keywords: 5-8ä¸ªå…³é”®è¯ï¼Œç”¨äºåç»­æœç´¢ç›¸å…³ç´ æ
- structure: æ–‡ç« çš„é€»è¾‘ç»“æ„
- material_types: æ–‡ç« ç”¨åˆ°çš„ç´ æç±»å‹ï¼ˆå¦‚ï¼šå®˜æ–¹æ–‡æ¡£ã€ç”¨æˆ·æ¡ˆä¾‹ã€ç«å“å¯¹æ¯”ã€æˆªå›¾æ¼”ç¤ºï¼‰
- psychology: è¿™ç¯‡æ–‡ç« ä¸»è¦è¿ç”¨äº†å“ªç§å¿ƒç†å­¦ç­–ç•¥
- search_queries: 3ä¸ªæœ€æœ‰ä»·å€¼çš„æœç´¢è¯ï¼Œç”¨äºæ‰¾åˆ°ç±»ä¼¼ç´ æ"""

    with httpx.Client(proxy=PROXY_URL, timeout=REQUEST_TIMEOUT) as http_client:
        client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL, http_client=http_client)
        
        try:
            @retryable
            @track_cost(context="imitate_analyze")
            def _analyze():
                return client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": ANALYZE_PROMPT},
                        {"role": "user", "content": f"è¯·åˆ†æä»¥ä¸‹æ–‡ç« ï¼š\n\n{article_content}"}
                    ],
                    response_format={"type": "json_object"}
                )
            
            response = _analyze()
            analysis_text = response.choices[0].message.content
            
            # è§£æ JSON
            try:
                analysis = json.loads(analysis_text)
            except:
                analysis = json.loads(repair_json(analysis_text))
            
            log_print(f"   âœ… åˆ†æå®Œæˆ")
            log_print(f"   ğŸ“Œ ä¸»é¢˜: {analysis.get('topic', 'æœªçŸ¥')}")
            log_print(f"   ğŸ·ï¸ å…³é”®è¯: {', '.join(analysis.get('keywords', []))}")
            log_print(f"   ğŸ§  å¿ƒç†é”šç‚¹: {analysis.get('psychology', 'æœªçŸ¥')}")
            
            if dry_run:
                log_print("\nğŸ§ª [Dry Run] åˆ†æç»“æœé¢„è§ˆ:")
                log_print(json.dumps(analysis, ensure_ascii=False, indent=2))
                log_print("\nğŸ§ª [Dry Run] ä»¿å†™æ¨¡å¼éªŒè¯æˆåŠŸï¼Œä¸æ‰§è¡Œå®é™…æ“ä½œã€‚")
                return
            
            # 3. v5.2: è·³è¿‡ Hunt æ‰«æï¼Œç›´æ¥ç”Ÿæˆæœ€ç»ˆå†³ç­–
            log_print(f"\nğŸš€ [æé€Ÿä»¿å†™] è·³è¿‡æ‰«æï¼Œæ­£åœ¨ç›´æ¥ç”Ÿæˆæœ€ç»ˆå†³ç­–...")
            
            from config import get_stage_dir, get_research_notes_file
            topics_dir = Path(get_stage_dir("topics"))
            final_report = topics_dir / "FINAL_DECISION.md"
            
            # ä¿å­˜åŸæ–‡ç´ æåˆ° research ç›®å½•ï¼Œä¾›åç»­ draft å‚è€ƒ
            research_dir = Path(get_stage_dir("research"))
            source_file = research_dir / "imitation_source.txt"
            with open(source_file, "w", encoding="utf-8-sig") as f:
                f.write(article_content)
            log_print(f"   ğŸ“¥ å·²ä¿å­˜ä»¿å†™åŸæ–‡ç´ æ: {source_file}")

            # æ„å»ºç¬¦åˆ FINAL_DECISION.md æ ¼å¼çš„å†…å®¹
            # é€‰é¢˜ 1 ä¸ºç²¾ç¡®ä»¿å†™ç‰ˆ
            topic_title = analysis.get("topic", "æœªå‘½åä»¿å†™é€‰é¢˜")
            keywords = ", ".join(analysis.get("keywords", []))
            psychology = analysis.get("psychology", "é”šç‚¹æ•ˆåº”")
            
            # ä¸º Researcher ç”Ÿæˆä»»åŠ¡æè¿°
            search_queries = analysis.get("search_queries", [])
            research_tasks = "\n".join([f"{i+1}. {q}" for i, q in enumerate(search_queries)])
            
            final_decision_content = f"""# ğŸ† ä»Šæ—¥æœ€ç»ˆé€‰é¢˜å†³ç­– (æé€Ÿä»¿å†™æ¨¡å¼)

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}
**ä»¿å†™æ¥æº**: {reference_input if is_url else os.path.basename(reference_input)}

## è¾“å‡ºæ ¼å¼ (v5.0: å•é€‰é¢˜ç¡®å®šæ¨¡å¼)

### ğŸ† ä»Šæ—¥æœ€ç»ˆé€‰é¢˜ (THE ONE)
**æ ‡é¢˜**ï¼š{topic_title}
**å¿ƒç†é”šç‚¹**ï¼š{psychology}
**ä¸€å¥è¯å–ç‚¹**ï¼šåŸºäºæ·±åº¦ä»¿å†™åˆ†æï¼Œæ—¨åœ¨é‡ç°åŸæ–‡çš„çˆ†æ¬¾ç»“æ„ä¸æƒ…ç»ªä»·å€¼ã€‚
**å…³é”®è¯**ï¼š{keywords}

### ğŸ’¡ å¤‡é€‰è§’åº¦ (ä»¿å†™æ¨¡å¼ä¸æä¾›å¤‡é€‰)
- **è§’åº¦A**ï¼šä»¿å†™æ¨¡å¼ä¸‹ï¼Œç³»ç»Ÿå…¨åŠ›èšç„¦äºå”¯ä¸€ç›®æ ‡ã€‚

---

### ğŸ“¡ æç¤ºè¯ 1ï¼šFast Research (ç”¨äºè‡ªåŠ¨ç ”ç©¶ / research é˜¶æ®µ)
```
è¯·ä½œä¸ºç ”ç©¶å‘˜ï¼Œå›´ç»•é€‰é¢˜ã€Š{topic_title}ã€‹è¿›è¡Œæ·±åº¦æœç´¢å’Œç´ ææœé›†ï¼š
1. æ ¸å¿ƒæœç´¢ä»»åŠ¡ï¼š
{research_tasks}
2. ç»“æ„å‚è€ƒï¼š
{analysis.get("structure", "ä¿æŒåŸæ–‡é€»è¾‘ç»“æ„")}
3. é‡ç‚¹ï¼šç»“åˆä»¿å†™åŸæ–‡ä¸­çš„ç´ æç±»å‹ï¼ˆ{", ".join(analysis.get("material_types", []))}ï¼‰ï¼Œæœé›†æœ€æ–°çš„å¯æ›¿ä»£ç´ æã€‚
```

### ğŸ¨ æç¤ºè¯ 2ï¼šè§†è§‰è„šæœ¬ (ç”¨äºé…å›¾æ–¹æ¡ˆ)
```
1. å‚è€ƒåŸæ–‡çš„è§†è§‰é£æ ¼ï¼Œä¸ºã€Š{topic_title}ã€‹å‡†å¤‡é…å›¾å»ºè®®ã€‚
2. é‡ç‚¹å±•ç¤ºï¼šæ–°ç‰ˆåŠŸèƒ½çš„å®é™…ç•Œé¢ã€æ“ä½œæµç¨‹ã€‚
```

### ğŸ¨ è§†è§‰é…å›¾æŒ‡å— (Visual Guide)
**è¯´æ˜**ï¼šè¯·ä¸ºäººå·¥é…å›¾æä¾›è¯¦ç»†çš„ç”»é¢å»ºè®®ã€‚
å°é¢å›¾ï¼š[ç§‘æŠ€æ„Ÿæµå…‰èƒŒæ™¯ï¼Œçªå‡ºä¸»é¢˜ï¼š{topic_title}]
å†…é¡µå›¾1ï¼š[åŠŸèƒ½æ“ä½œæˆªå›¾æ¼”ç¤º]
å†…é¡µå›¾2ï¼š[æ•ˆæœå¯¹æ¯”å›¾]
"""
            with open(final_report, "w", encoding="utf-8-sig") as f:
                f.write(final_decision_content)
            
            # æ›´æ–°å†å²è®°å½•
            save_topic_to_history(topic_title, f"ä»¿å†™: {psychology}")
            
            log_print(f"âœ… æé€Ÿä»¿å†™å®Œæˆï¼FINAL_DECISION.md å·²ç”Ÿæˆã€‚")
            log_print(f"ğŸ’¡ ä¸‹ä¸€æ­¥ï¼šç›´æ¥è¿è¡Œ `python main.py research` å¼€å§‹æ·±åº¦ç ”ç©¶ã€‚")
            
        except Exception as e:
            log_print(f"âŒ ä»¿å†™æ¨¡å¼è¿è¡Œå¤±è´¥: {e}")
            import traceback
            log_print(traceback.format_exc())


if __name__ == "__main__":
    main()
