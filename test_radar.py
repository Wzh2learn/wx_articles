"""
ğŸ§ª é€‰é¢˜é›·è¾¾å•å…ƒæµ‹è¯•è„šæœ¬
åªæµ‹è¯• fetch_dynamic_trends å‡½æ•°ï¼Œä¸æ¶ˆè€— Tavily é¢åº¦ï¼Œä¸è¿è¡Œå®Œæ•´æµç¨‹ã€‚
"""
import sys
import os
import httpx
from openai import OpenAI

# å¯¼å…¥é…ç½®
from config import DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL, PROXY_URL, REQUEST_TIMEOUT, get_logger

# æ¨¡æ‹Ÿç¯å¢ƒå¼•å…¥
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from agents.trend_hunter import fetch_dynamic_trends, WebSearchTool

logger = get_logger(__name__)

def test_radar():
    logger.info("ğŸ§ª æ­£åœ¨å¯åŠ¨é›·è¾¾å•å…ƒæµ‹è¯•...")
    logger.info("ğŸ”Œ ä»£ç†é…ç½®: %s", PROXY_URL)
    logger.info("ğŸ”‘ API Key: %s******", (DEEPSEEK_API_KEY[:5] if DEEPSEEK_API_KEY else ""))

    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    try:
        if PROXY_URL:
            http_client = httpx.Client(proxy=PROXY_URL, timeout=REQUEST_TIMEOUT)
        else:
            http_client = httpx.Client(timeout=REQUEST_TIMEOUT)

        client = OpenAI(
            api_key=DEEPSEEK_API_KEY, 
            base_url=DEEPSEEK_BASE_URL, 
            http_client=http_client
        )
    except Exception as e:
        logger.error("âŒ å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: %s", e)
        return

    # åˆå§‹åŒ– Tavily å·¥å…·
    search_tool = WebSearchTool()

    # è¿è¡ŒæŠ“å–
    try:
        keywords = fetch_dynamic_trends(client, search_tool)

        logger.info("%s", "="*50)
        logger.info("ğŸ‰ æµ‹è¯•ç»“æœæŠ¥å‘Š")
        logger.info("%s", "="*50)
        
        if keywords:
            logger.info("âœ… æˆåŠŸæ•è· %s ä¸ªçƒ­è¯:", len(keywords))
            for i, kw in enumerate(keywords, 1):
                logger.info("   %s. %s", i, kw)
        else:
            logger.warning("âš ï¸ æœªæ•è·åˆ°ä»»ä½•å…³é”®è¯ (è¯·æ£€æŸ¥ç½‘ç»œæˆ– Jina æœåŠ¡çŠ¶æ€)")
            
    except Exception as e:
        logger.error("âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: %s", e)
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_radar()