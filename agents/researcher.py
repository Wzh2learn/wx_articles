"""
===============================================================================
                    ğŸ”¬ ç ”ç©¶æ™ºèƒ½ä½“ (Researcher Agent) v4.0 (Hardcore Edition)
===============================================================================
æ ¸å¿ƒç­–ç•¥ï¼š
1. æ™ºèƒ½èšåˆæœç´¢ï¼šExa AI (ä¼˜å…ˆ) + Tavily (å…œåº•)ï¼Œå…¨ç½‘æ·±åº¦æŒ–æ˜ã€‚
2. æ‰¹åˆ¤æ€§è¯„ä¼°è¿‡æ»¤å™¨ï¼šåœ¨ç¬”è®°æ•´ç†é˜¶æ®µï¼Œè‡ªåŠ¨è¯†åˆ«å¹¶æ ‡è®°â€œæ™ºå•†ç¨â€å·¥å…·ã€‚
3. åå¥—å£³æœºåˆ¶ï¼šå¼ºåˆ¶æå–åº•å±‚æŠ€æœ¯åŸç†ï¼Œæ‹’ç»è¥é”€è½¯æ–‡ã€‚
===============================================================================
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import httpx
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from typing import Optional, List, Dict, Any
from openai import OpenAI
from tavily import TavilyClient
from config import (
    DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL, 
    TAVILY_API_KEY, EXA_API_KEY,
    PROXY_URL, REQUEST_TIMEOUT, get_research_notes_file, get_logger, retryable
)


logger = get_logger(__name__)


class ResearcherAgent:
    """è‡ªåŠ¨åŒ–ç ”ç©¶æ™ºèƒ½ä½“ï¼šExa AI æœç´¢ + å†…å®¹èšåˆ + ç¬”è®°æ•´ç†"""
    
    def __init__(self):
        # åˆå§‹åŒ– DeepSeek å®¢æˆ·ç«¯
        # ä½¿ç”¨ç»Ÿä¸€é…ç½®ä¸­çš„ä»£ç†ï¼›å¦‚ä¸éœ€è¦ä»£ç†è¯·åœ¨ config.py ä¸­å°† PROXY_URL è®¾ä¸º None
        proxy_url = PROXY_URL
        self.client = OpenAI(
            api_key=DEEPSEEK_API_KEY,
            base_url=DEEPSEEK_BASE_URL,
            http_client=httpx.Client(proxy=proxy_url, timeout=REQUEST_TIMEOUT)
        )
        
        # åˆå§‹åŒ– Tavily (å¤‡ç”¨)
        self.tavily = TavilyClient(api_key=TAVILY_API_KEY)
        
        self.exa_api_key = EXA_API_KEY
        self.proxy_url = proxy_url
        
        logger.info("âœ… ResearcherAgent v4.0 åˆå§‹åŒ–å®Œæˆ (Exa + Tavily)")

    def search_exa(self, topic: str, queries: List[str]) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ Exa AI è¿›è¡Œé«˜çº§æœç´¢ (è‡ªåŠ¨åŒ…å«å†…å®¹)
        """
        if not self.exa_api_key:
            logger.warning("æœªé…ç½® EXA_API_KEYï¼Œè·³è¿‡ Exa æœç´¢")
            return []

        logger.info("ğŸ” [Step 1] Exa AI æ™ºèƒ½æœç´¢ (Topic: %s)...", topic)
        
        all_results = []
        headers = {
            "Authorization": f"Bearer {self.exa_api_key}",
            "Content-Type": "application/json"
        }
        
        # Exa API ç«¯ç‚¹
        url = "https://api.exa.ai/search"

        # å®šä¹‰æœç´¢æ‰¹æ¬¡
        # 1. ç¤¾äº¤åª’ä½“ä¸“é¡¹ (æŒ‡å®šåŸŸå)
        social_domains = [
            "mp.weixin.qq.com", "zhihu.com", "weibo.com", 
            "xiaohongshu.com", "v2ex.com", "juejin.cn"
        ]
        
        batches = [
            # Batch 1: é’ˆå¯¹ç¤¾äº¤åª’ä½“çš„ç²¾å‡†æœç´¢
            {
                "query": f"{topic} æ·±åº¦è§£æ é¿å‘æŒ‡å— æ•™ç¨‹",
                "numResults": 8,
                "includeDomains": social_domains,
                "useAutoprompt": True, # è®© Exa ä¼˜åŒ–æŸ¥è¯¢
                "contents": {"text": True} # ç›´æ¥è·å–æ­£æ–‡
            },
            # Batch 2: å…¨ç½‘é€šç”¨æœç´¢ (å¯»æ‰¾æœ€æ–°/é«˜è´¨é‡é•¿æ–‡)
            {
                "query": topic,
                "numResults": 5,
                "useAutoprompt": True,
                "contents": {"text": True}
            }
        ]
        
        @retryable
        def _exa_post(client: httpx.Client, payload: dict, headers: dict):
            return client.post(url, json=payload, headers=headers)

        with httpx.Client(timeout=60, proxy=self.proxy_url) as client:
            for i, payload in enumerate(batches):
                try:
                    logger.info("ğŸš€ Exa Batch %s è¯·æ±‚ä¸­...", i + 1)
                    resp = _exa_post(client, payload, headers)
                    resp.raise_for_status()
                    data = resp.json()
                    
                    results = data.get("results", [])
                    for res in results:
                        all_results.append({
                            "url": res.get("url"),
                            "title": res.get("title"),
                            "text": res.get("text", ""), # Exa ç›´æ¥è¿”å›çš„æ­£æ–‡
                            "source": "Exa"
                        })
                        logger.info("âœ“ [Exa] %s...", (res.get('title', 'Unknown') or '')[:40])
                        
                except Exception as e:
                    logger.error("âŒ Exa Batch %s å¤±è´¥: %s", i + 1, e)

        return all_results

    def search_tavily_fallback(self, queries: List[str]) -> List[Dict[str, Any]]:
        """
        Tavily å¤‡ç”¨æœç´¢ (ä»…è·å– URLï¼Œæ— æ­£æ–‡)
        """
        logger.info("ğŸ”„ [Fallback] åˆ‡æ¢è‡³ Tavily å¹¶å‘æœç´¢...")
        
        all_results = []
        seen_urls = set()
        
        # æ„é€ æŸ¥è¯¢
        extended_queries = []
        for q in queries:
            extended_queries.append({"q": q, "type": "general"})
            extended_queries.append({"q": f"{q} site:mp.weixin.qq.com", "type": "wechat"})
            extended_queries.append({"q": f"{q} site:zhihu.com", "type": "zhihu"})
        
        @retryable
        def _tavily_search(query: str, limit: int):
            return self.tavily.search(
                query=query,
                search_depth="advanced",
                max_results=limit,
                days=30
            )

        def do_search(item):
            try:
                limit = 2 if item['type'] == "general" else 1
                resp = _tavily_search(item['q'], limit)
                return resp.get('results', [])
            except Exception as e:
                logger.warning("Tavily æœç´¢å¤±è´¥: %s", e)
                return []

        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(do_search, item) for item in extended_queries]
            for future in as_completed(futures):
                for res in future.result():
                    if res['url'] not in seen_urls and "pdf" not in res['url']:
                        seen_urls.add(res['url'])
                        all_results.append({
                            "url": res['url'],
                            "title": res['title'],
                            "text": "", # Tavily ä¸å«å…¨æ–‡ï¼Œéœ€åç»­çˆ¬å–
                            "source": "Tavily"
                        })
                        logger.info("âœ“ [Tavily] %s...", (res.get('title', '') or '')[:40])
        
        return all_results[:8]

    def scrape_missing_content(self, items: List[Dict[str, Any]]) -> None:
        """
        å¯¹ç¼ºå°‘æ­£æ–‡çš„æ¡ç›® (å¦‚æ¥è‡ª Tavily) è¿›è¡Œè¡¥å……çˆ¬å–
        ä½¿ç”¨ Jina Reader + Fallback
        """
        missing_items = [i for i in items if not i.get("text") or len(i.get("text")) < 200]
        if not missing_items:
            return
            
        logger.info("ğŸ“– [Step 2] è¡¥å……çˆ¬å– %s ä¸ªé¡µé¢ (Jina/Fallback)...", len(missing_items))
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        
        crawled_texts = []
        
        @retryable
        def _http_get(client: httpx.Client, url: str, headers: Optional[dict] = None):
            return client.get(url, headers=headers)

        with httpx.Client(timeout=60, proxy=self.proxy_url, follow_redirects=True) as client:
            for item in missing_items:
                url = item['url']
                logger.info("ğŸŒ çˆ¬å–: %s...", (item.get('title', '') or '')[:30])
                
                try:
                    # Jina
                    jina_resp = _http_get(client, f"https://r.jina.ai/{url}", headers=headers)
                    if jina_resp.status_code == 200 and len(jina_resp.text) > 500:
                        item['text'] = jina_resp.text
                        logger.info("âœ“ Jina æˆåŠŸ")
                        continue
                except:
                    pass
                
                try:
                    # Direct Fallback
                    raw_resp = _http_get(client, url, headers=headers)
                    if raw_resp.status_code == 200:
                        # æå…¶ç®€é™‹çš„æ–‡æœ¬æå–
                        from bs4 import BeautifulSoup
                        soup = BeautifulSoup(raw_resp.text, 'html.parser')
                        for s in soup(['script', 'style']): s.extract()
                        item['text'] = soup.get_text()[:10000]
                        logger.info("âœ“ ç›´è¿æˆåŠŸ")
                except Exception as e:
                    logger.error("âŒ å¤±è´¥: %s", e)

    def synthesize_notes(self, items: List[Dict[str, Any]], topic: str, strategic_intent: Optional[str] = None) -> str:
        """
        æ•´ç†æ‰€æœ‰ç´ æä¸ºç¬”è®° (å¸¦æ‰¹åˆ¤æ€§è¯„ä¼°è¿‡æ»¤å™¨)
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        1. å”¯æŠ€æœ¯è®ºï¼šåªæå–åº•å±‚æŠ€æœ¯/Prompt/æœ¬åœ°éƒ¨ç½²ç›¸å…³å†…å®¹ã€‚
        2. é„™è§†å¥—å£³ï¼šè¯†åˆ«å¹¶æ ‡è®°å›½å†…ä»˜è´¹å¥—å£³å·¥å…·ä¸ºâ€œé¿å‘é»‘åå•â€ã€‚
        """
        logger.info("ğŸ“ [Step 3] AI æ•´ç†ç¬”è®°...")
        
        # æ‹¼æ¥æ‰€æœ‰ç´ æ
        raw_text = ""
        for item in items:
            text = item.get("text", "")
            if len(text) > 100:
                raw_text += f"\n{'='*50}\nSource: {item['url']}\nTitle: {item.get('title')}\n{'='*50}\n{text[:8000]}\n"
        
        if not raw_text:
            return "# ç ”ç©¶å¤±è´¥ï¼šæœªè·å–åˆ°æœ‰æ•ˆå†…å®¹"

        strategic_block = ("\n\n" + "="*20 + "\n" + "ã€é€‰é¢˜ç­–åˆ’ä¹¦ / æˆ˜ç•¥æ„å›¾ï¼ˆæœ€é«˜æŒ‡ä»¤ï¼‰ã€‘\n" + (strategic_intent or "") + "\n" + "="*20 + "\n") if strategic_intent else ""

        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šå†…å®¹ç ”ç©¶å‘˜å’Œèµ„æ·±æŠ€æœ¯åšä¸»ã€‚è¯·æ ¹æ®ä»¥ä¸‹å¤šç¯‡æ¥æºæ–‡ç« ï¼Œä¸ºå…¬ä¼—å·æ–‡ç« ã€Š{topic}ã€‹æ•´ç†ç´ æã€‚{strategic_block}
        
        âš ï¸ **æ ¸å¿ƒä»·å€¼è§‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰**ï¼š
        1. **å”¯æŠ€æœ¯è®º**ï¼šæˆ‘ä»¬åªæ¨å´‡**åº•å±‚å¤§æ¨¡å‹ï¼ˆDeepSeek, ChatGPT, Claudeï¼‰** + **æç¤ºè¯å·¥ç¨‹**ã€‚
        2. **é„™è§†å¥—å£³**ï¼šæ‰€æœ‰å›½å†…å·ç§°â€œä¸€é”®é™é‡â€ã€â€œAIå†™ä½œç¥å™¨â€çš„ä»˜è´¹å·¥å…·ï¼ˆå¦‚**ç¬”çµã€å°å‘çŒ«ã€PaperYYã€å˜å˜**ç­‰ï¼‰ï¼Œä¸€å¾‹è§†ä¸º**â€œä½æŠ€æœ¯å«é‡çš„å¥—å£³äº§å“â€**ã€‚
        3. **è¯æ®æœé›†**ï¼šå¦‚æœç´ æä¸­å¹æ§è¿™äº›ä»˜è´¹å·¥å…·ï¼Œè¯·å¿½ç•¥å…¶è¥é”€è¯æœ¯ï¼Œé‡ç‚¹æŒ–æ˜å®ƒä»¬çš„ç¼ºç‚¹ï¼ˆå¦‚æ”¶è´¹è´µã€åº•å±‚æ¨¡å‹æœªçŸ¥ã€éšç§é£é™©ï¼‰ã€‚

        âš ï¸ **æˆ˜ç•¥æ„å›¾å¯¹é½ï¼ˆå¿…é¡»æ‰§è¡Œï¼‰**ï¼š
        - å¦‚æœä¸Šé¢çš„â€œé€‰é¢˜ç­–åˆ’ä¹¦â€å¼ºè°ƒäº†â€œé¿å‘/å¹³æ›¿/ç™½å«–/ææ•ˆ/çœé’±/çœæ—¶/æƒ…ç»ªå…±é¸£â€ç­‰å…³é”®è¯ï¼Œä½ çš„ç¬”è®°å¿…é¡»å›´ç»•å®ƒé€‰æä¸ç»„ç»‡ç»“æ„ã€‚
        - ä½ çš„è¾“å‡ºå¿…é¡»æ˜¾å¼è¦†ç›–ç­–åˆ’ä¹¦ä¸­çš„ï¼š
          1) ä¸€å¥è¯å–ç‚¹ï¼ˆè¯»è€…è·å¾—æ„Ÿï¼‰
          2) å¿ƒç†é”šç‚¹ï¼ˆè¯»è€…ä¸ºä»€ä¹ˆä¼šç‚¹å¼€/ä¼šç„¦è™‘ä»€ä¹ˆï¼‰
          3) æ ¸å¿ƒçœ‹ç‚¹ï¼ˆæ–‡ç« å¿…é¡»è¦†ç›–çš„è¦ç‚¹/ç»“æ„ï¼‰
        - å¦‚æœç´ æä¸ç­–åˆ’ä¹¦å†²çªï¼šä¼˜å…ˆä¿ç•™â€œå¯è¯æ®æ”¯æŒâ€çš„å†…å®¹ï¼Œå¹¶åœ¨ç¬”è®°ä¸­æ ‡æ³¨â€œä¸ç­–åˆ’ä¹¦å‡è®¾ä¸ä¸€è‡´â€ã€‚

        âš ï¸ **ä¿¡æ¯ä¸è¶³å…œåº•ï¼ˆå¿…é¡»æ‰§è¡Œï¼‰**ï¼š
        - å¦‚æœâ€œé€‰é¢˜ç­–åˆ’ä¹¦â€é‡Œç‚¹åäº†æŸä¸ªå…·ä½“å·¥å…·/é¡¹ç›®ï¼ˆä¾‹å¦‚ block/gooseï¼‰ï¼Œä½†åœ¨ç´ æä¸­æœä¸åˆ°è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·åœ¨ç¬”è®°ä¸­æ˜ç¡®æ ‡æ³¨ï¼š**â€œä¿¡æ¯ä¸è¶³â€**ï¼Œå¹¶è§£é‡Šç¼ºå¤±ç‚¹ï¼ˆå¦‚ï¼šç¼ºå®˜æ–¹æ–‡æ¡£/ç¼ºçœŸå®ä½“éªŒ/ç¼ºè¿‘æœŸæ›´æ–°ï¼‰ã€‚
        - åŒæ—¶ï¼Œä½ å¿…é¡»ä¸»åŠ¨å¯»æ‰¾ä¸€ä¸ªâ€œåŒç±»å‹çš„ GitHub é«˜æ˜Ÿé¡¹ç›® / å®˜æ–¹æ›¿ä»£æ–¹æ¡ˆâ€ä½œä¸ºå¤‡é€‰ï¼Œå¹¶å†™æ¸…æ¥šï¼š
          1) ä¸ºä»€ä¹ˆå®ƒæ˜¯åŒç±»å‹
          2) å®ƒçš„æ ¸å¿ƒèƒ½åŠ›
          3) å®ƒä¸ç­–åˆ’ä¹¦ç›®æ ‡çš„åŒ¹é…åº¦ï¼ˆå–ç‚¹/é”šç‚¹/æ ¸å¿ƒçœ‹ç‚¹ï¼‰
        - ç›®çš„ï¼šä¸å…è®¸å†™ä½œç«¯å‡ºç°â€œå¼€å¤©çª—â€ã€‚
        
        è¯·æå–ï¼š
        1. æ ¸å¿ƒè§‚ç‚¹ (æ ‡æ³¨æ¥æº)
        2. å…³é”®æ•°æ®/æ¡ˆä¾‹ (æ ‡æ³¨æ¥æº)
        3. **é¿å‘é»‘åå•**ï¼šå°†æ‰€æœ‰â€œä»˜è´¹å¥—å£³å·¥å…·â€å½’å…¥æ­¤ç±»ï¼Œå¹¶è¯´æ˜ç†ç”±ï¼ˆæ™ºå•†ç¨ï¼‰ã€‚
        4. **çœŸæ­£çš„é«˜é˜¶ç©æ³•**ï¼šå¯»æ‰¾å…³äº**Promptä¼˜åŒ–ã€å¤šæ¨¡å‹äº¤å‰éªŒè¯ã€æœ¬åœ°éƒ¨ç½²(Ollama)**ç­‰ç¡¬æ ¸å†…å®¹ã€‚
        
        è¾“å‡ºä¸ºæ¸…æ™°çš„ Markdown æ ¼å¼ã€‚
        """

        try:
            @retryable
            def _chat_create():
                return self.client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": prompt},
                        {"role": "user", "content": f"ç´ æå†…å®¹ï¼š\n{raw_text[:60000]}"} # æ§åˆ¶æ€»é•¿åº¦
                    ],
                    temperature=0.3,
                    max_tokens=4000,
                    stream=True
                )

            response = _chat_create()

            logger.info("%s", "="*20 + " ç¬”è®°ç”Ÿæˆä¸­ " + "="*20)

            collected = []
            for chunk in response:
                content = chunk.choices[0].delta.content
                if content:
                    sys.stdout.write(content)
                    sys.stdout.flush()
                    collected.append(content)
            sys.stdout.write("\n")
            sys.stdout.flush()

            return "".join(collected)

        except Exception as e:
            logger.error("âŒ æ•´ç†å¤±è´¥: %s", e)
            return f"æ•´ç†å¤±è´¥: {e}"


    def run(self, topic: str, queries: List[str], strategic_intent: Optional[str] = None) -> str:
        logger.info("%s", "="*60)
        logger.info("ğŸ”¬ ResearcherAgent v4.0 (Exa AI)")
        logger.info("ğŸ“Œ é€‰é¢˜: %s", topic)
        logger.info("%s", "="*60)

        # 1. Exa æœç´¢ (ä¼˜å…ˆ)
        results = self.search_exa(topic, queries)

        # 2. å¦‚æœ Exa ç»“æœå¤ªå°‘ï¼Œä½¿ç”¨ Tavily è¡¥å……
        if len(results) < 3:
            tavily_results = self.search_tavily_fallback(queries)
            results.extend(tavily_results)

        if not results:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å†…å®¹")
            return ""

        # 3. è¡¥å……çˆ¬å– (é’ˆå¯¹ Tavily æ¥æºæˆ– Exa æ²¡æŠ“åˆ°æ­£æ–‡çš„)
        self.scrape_missing_content(results)

        # 4. æ•´ç†ç¬”è®°
        notes = self.synthesize_notes(results, topic, strategic_intent=strategic_intent)

        # ä¿å­˜
        notes_file = get_research_notes_file()
        with open(notes_file, "w", encoding="utf-8") as f:
            intent_section = f"\n\n## ğŸ¯ æˆ˜ç•¥æ„å›¾ï¼ˆæ¥è‡ª FINAL_DECISION.mdï¼‰\n\n{strategic_intent.strip()}\n" if strategic_intent else ""
            f.write(f"# ğŸ”¬ è‡ªåŠ¨ç ”ç©¶ç¬”è®° (Exa AI)\n\n**é€‰é¢˜**: {topic}\n**æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n{intent_section}\n---\n\n{notes}")

        logger.info("ğŸ“ ç¬”è®°å·²ä¿å­˜: %s", notes_file)
        return notes

def main():
    agent = ResearcherAgent()
    agent.run("DeepSeek V3 éšè—åŠŸèƒ½", ["DeepSeek æ•™ç¨‹"])

if __name__ == "__main__":
    main()
