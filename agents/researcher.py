"""
===============================================================================
                    ğŸ”¬ ç ”ç©¶æ™ºèƒ½ä½“ (Researcher Agent) v4.0 (ç¡¬æ ¸ä»·å€¼ç‰ˆ)
===============================================================================
æ ¸å¿ƒç­–ç•¥ï¼š
1. æ™ºèƒ½èšåˆæœç´¢ï¼šExa AI (ä¼˜å…ˆ) + Tavily (å…œåº•)ï¼Œå…¨ç½‘æ·±åº¦æŒ–æ˜ã€‚
2. æ‰¹åˆ¤æ€§è¯„ä¼°è¿‡æ»¤å™¨ï¼šåœ¨ç¬”è®°æ•´ç†é˜¶æ®µï¼Œè‡ªåŠ¨è¯†åˆ«å¹¶æ ‡è®°â€œæ™ºå•†ç¨â€å·¥å…·ã€‚
3. åå¥—å£³æœºåˆ¶ï¼šå¼ºåˆ¶æå–åº•å±‚æŠ€æœ¯åŸç†ï¼Œæ‹’ç»è¥é”€è½¯æ–‡ã€‚
===============================================================================
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import httpx
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from openai import OpenAI
from tavily import TavilyClient
from config import (
    DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL, 
    TAVILY_API_KEY, EXA_API_KEY,
    PROXY_URL, REQUEST_TIMEOUT, get_research_notes_file
)


class ResearcherAgent:
    """è‡ªåŠ¨åŒ–ç ”ç©¶æ™ºèƒ½ä½“ï¼šExa AI æœç´¢ + å†…å®¹èšåˆ + ç¬”è®°æ•´ç†"""
    
    def __init__(self):
        # åˆå§‹åŒ– DeepSeek å®¢æˆ·ç«¯
        # å¼ºåˆ¶ä½¿ç”¨ç³»ç»Ÿä»£ç†ç¡®ä¿è¿æ¥ç¨³å®š
        proxy_url = PROXY_URL or "http://127.0.0.1:7898"
        self.client = OpenAI(
            api_key=DEEPSEEK_API_KEY,
            base_url=DEEPSEEK_BASE_URL,
            http_client=httpx.Client(proxy=proxy_url, timeout=REQUEST_TIMEOUT)
        )
        
        # åˆå§‹åŒ– Tavily (å¤‡ç”¨)
        self.tavily = TavilyClient(api_key=TAVILY_API_KEY)
        
        self.exa_api_key = EXA_API_KEY
        self.proxy_url = proxy_url
        
        print(f"   âœ… ResearcherAgent v2.0 åˆå§‹åŒ–å®Œæˆ (Exa + Tavily)")

    def search_exa(self, topic: str, queries: list[str]) -> list[dict]:
        """
        ä½¿ç”¨ Exa AI è¿›è¡Œé«˜çº§æœç´¢ (è‡ªåŠ¨åŒ…å«å†…å®¹)
        """
        if not self.exa_api_key:
            print("   âš ï¸ æœªé…ç½® EXA_API_KEYï¼Œè·³è¿‡ Exa æœç´¢")
            return []

        print(f"\nğŸ” [Step 1] Exa AI æ™ºèƒ½æœç´¢ (Topic: {topic})...")
        
        all_results = []
        headers = {
            "Authorization": f"Bearer {self.exa_api_key}",
            "Content-Type": "application/json"
        }
        
        # Exa API ç«¯ç‚¹
        url = "https://api.exa.ai/search"

        # å®šä¹‰æœç´¢æ‰¹æ¬¡
        # 1. ç¤¾äº¤åª’ä½“ä¸“é¡¹ (æŒ‡å®šåŸŸå)
        social_domains = [
            "mp.weixin.qq.com", "zhihu.com", "weibo.com", 
            "xiaohongshu.com", "v2ex.com", "juejin.cn"
        ]
        
        batches = [
            # Batch 1: é’ˆå¯¹ç¤¾äº¤åª’ä½“çš„ç²¾å‡†æœç´¢
            {
                "query": f"{topic} æ·±åº¦è§£æ é¿å‘æŒ‡å— æ•™ç¨‹",
                "numResults": 8,
                "includeDomains": social_domains,
                "useAutoprompt": True, # è®© Exa ä¼˜åŒ–æŸ¥è¯¢
                "contents": {"text": True} # ç›´æ¥è·å–æ­£æ–‡
            },
            # Batch 2: å…¨ç½‘é€šç”¨æœç´¢ (å¯»æ‰¾æœ€æ–°/é«˜è´¨é‡é•¿æ–‡)
            {
                "query": topic,
                "numResults": 5,
                "useAutoprompt": True,
                "contents": {"text": True}
            }
        ]
        
        with httpx.Client(timeout=60, proxy=self.proxy_url) as client:
            for i, payload in enumerate(batches):
                try:
                    print(f"   ğŸš€ Exa Batch {i+1} è¯·æ±‚ä¸­...")
                    resp = client.post(url, json=payload, headers=headers)
                    resp.raise_for_status()
                    data = resp.json()
                    
                    results = data.get("results", [])
                    for res in results:
                        all_results.append({
                            "url": res.get("url"),
                            "title": res.get("title"),
                            "text": res.get("text", ""), # Exa ç›´æ¥è¿”å›çš„æ­£æ–‡
                            "source": "Exa"
                        })
                        print(f"      âœ“ [Exa] {res.get('title', 'Unknown')[:40]}...")
                        
                except Exception as e:
                    print(f"      âŒ Exa Batch {i+1} å¤±è´¥: {e}")

        return all_results

    def search_tavily_fallback(self, queries: list[str]) -> list[dict]:
        """
        Tavily å¤‡ç”¨æœç´¢ (ä»…è·å– URLï¼Œæ— æ­£æ–‡)
        """
        print(f"\nğŸ”„ [Fallback] åˆ‡æ¢è‡³ Tavily å¹¶å‘æœç´¢...")
        
        all_results = []
        seen_urls = set()
        
        # æ„é€ æŸ¥è¯¢
        extended_queries = []
        for q in queries:
            extended_queries.append({"q": q, "type": "general"})
            extended_queries.append({"q": f"{q} site:mp.weixin.qq.com", "type": "wechat"})
            extended_queries.append({"q": f"{q} site:zhihu.com", "type": "zhihu"})
        
        def do_search(item):
            try:
                limit = 2 if item['type'] == "general" else 1
                resp = self.tavily.search(
                    query=item['q'], 
                    search_depth="advanced", 
                    max_results=limit,
                    days=30
                )
                return resp.get('results', [])
            except:
                return []

        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(do_search, item) for item in extended_queries]
            for future in as_completed(futures):
                for res in future.result():
                    if res['url'] not in seen_urls and "pdf" not in res['url']:
                        seen_urls.add(res['url'])
                        all_results.append({
                            "url": res['url'],
                            "title": res['title'],
                            "text": "", # Tavily ä¸å«å…¨æ–‡ï¼Œéœ€åç»­çˆ¬å–
                            "source": "Tavily"
                        })
                        print(f"      âœ“ [Tavily] {res['title'][:40]}...")
        
        return all_results[:8]

    def scrape_missing_content(self, items: list[dict]) -> str:
        """
        å¯¹ç¼ºå°‘æ­£æ–‡çš„æ¡ç›® (å¦‚æ¥è‡ª Tavily) è¿›è¡Œè¡¥å……çˆ¬å–
        ä½¿ç”¨ Jina Reader + Fallback
        """
        missing_items = [i for i in items if not i.get("text") or len(i.get("text")) < 200]
        if not missing_items:
            return ""
            
        print(f"\nğŸ“– [Step 2] è¡¥å……çˆ¬å– {len(missing_items)} ä¸ªé¡µé¢ (Jina/Fallback)...")
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        
        crawled_texts = []
        
        with httpx.Client(timeout=60, proxy=self.proxy_url, follow_redirects=True) as client:
            for item in missing_items:
                url = item['url']
                print(f"   ğŸŒ çˆ¬å–: {item.get('title', '')[:30]}...")
                
                try:
                    # Jina
                    jina_resp = client.get(f"https://r.jina.ai/{url}", headers=headers)
                    if jina_resp.status_code == 200 and len(jina_resp.text) > 500:
                        item['text'] = jina_resp.text
                        print(f"      âœ“ Jina æˆåŠŸ")
                        continue
                except:
                    pass
                
                try:
                    # Direct Fallback
                    raw_resp = client.get(url, headers=headers)
                    if raw_resp.status_code == 200:
                        # æå…¶ç®€é™‹çš„æ–‡æœ¬æå–
                        from bs4 import BeautifulSoup
                        soup = BeautifulSoup(raw_resp.text, 'html.parser')
                        for s in soup(['script', 'style']): s.extract()
                        item['text'] = soup.get_text()[:10000]
                        print(f"      âœ“ ç›´è¿æˆåŠŸ")
                except Exception as e:
                    print(f"      âŒ å¤±è´¥: {e}")

        return ""

    def synthesize_notes(self, items: list[dict], topic: str) -> str:
        """
        æ•´ç†æ‰€æœ‰ç´ æä¸ºç¬”è®° (å¸¦æ‰¹åˆ¤æ€§è¯„ä¼°è¿‡æ»¤å™¨)
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        1. å”¯æŠ€æœ¯è®ºï¼šåªæå–åº•å±‚æŠ€æœ¯/Prompt/æœ¬åœ°éƒ¨ç½²ç›¸å…³å†…å®¹ã€‚
        2. é„™è§†å¥—å£³ï¼šè¯†åˆ«å¹¶æ ‡è®°å›½å†…ä»˜è´¹å¥—å£³å·¥å…·ä¸ºâ€œé¿å‘é»‘åå•â€ã€‚
        """
        print(f"\nğŸ“ [Step 3] AI æ•´ç†ç¬”è®°...")
        
        # æ‹¼æ¥æ‰€æœ‰ç´ æ
        raw_text = ""
        for item in items:
            text = item.get("text", "")
            if len(text) > 100:
                raw_text += f"\n{'='*50}\nSource: {item['url']}\nTitle: {item.get('title')}\n{'='*50}\n{text[:8000]}\n"
        
        if not raw_text:
            return "# ç ”ç©¶å¤±è´¥ï¼šæœªè·å–åˆ°æœ‰æ•ˆå†…å®¹"

        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šå†…å®¹ç ”ç©¶å‘˜å’Œèµ„æ·±æŠ€æœ¯åšä¸»ã€‚è¯·æ ¹æ®ä»¥ä¸‹å¤šç¯‡æ¥æºæ–‡ç« ï¼Œä¸ºå…¬ä¼—å·æ–‡ç« ã€Š{topic}ã€‹æ•´ç†ç´ æã€‚
        
        âš ï¸ **æ ¸å¿ƒä»·å€¼è§‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰**ï¼š
        1. **å”¯æŠ€æœ¯è®º**ï¼šæˆ‘ä»¬åªæ¨å´‡**åº•å±‚å¤§æ¨¡å‹ï¼ˆDeepSeek, ChatGPT, Claudeï¼‰** + **æç¤ºè¯å·¥ç¨‹**ã€‚
        2. **é„™è§†å¥—å£³**ï¼šæ‰€æœ‰å›½å†…å·ç§°â€œä¸€é”®é™é‡â€ã€â€œAIå†™ä½œç¥å™¨â€çš„ä»˜è´¹å·¥å…·ï¼ˆå¦‚**ç¬”çµã€å°å‘çŒ«ã€PaperYYã€å˜å˜**ç­‰ï¼‰ï¼Œä¸€å¾‹è§†ä¸º**â€œä½æŠ€æœ¯å«é‡çš„å¥—å£³äº§å“â€**ã€‚
        3. **è¯æ®æœé›†**ï¼šå¦‚æœç´ æä¸­å¹æ§è¿™äº›ä»˜è´¹å·¥å…·ï¼Œè¯·å¿½ç•¥å…¶è¥é”€è¯æœ¯ï¼Œé‡ç‚¹æŒ–æ˜å®ƒä»¬çš„ç¼ºç‚¹ï¼ˆå¦‚æ”¶è´¹è´µã€åº•å±‚æ¨¡å‹æœªçŸ¥ã€éšç§é£é™©ï¼‰ã€‚
        
        è¯·æå–ï¼š
        1. æ ¸å¿ƒè§‚ç‚¹ (æ ‡æ³¨æ¥æº)
        2. å…³é”®æ•°æ®/æ¡ˆä¾‹ (æ ‡æ³¨æ¥æº)
        3. **é¿å‘é»‘åå•**ï¼šå°†æ‰€æœ‰â€œä»˜è´¹å¥—å£³å·¥å…·â€å½’å…¥æ­¤ç±»ï¼Œå¹¶è¯´æ˜ç†ç”±ï¼ˆæ™ºå•†ç¨ï¼‰ã€‚
        4. **çœŸæ­£çš„é«˜é˜¶ç©æ³•**ï¼šå¯»æ‰¾å…³äº**Promptä¼˜åŒ–ã€å¤šæ¨¡å‹äº¤å‰éªŒè¯ã€æœ¬åœ°éƒ¨ç½²(Ollama)**ç­‰ç¡¬æ ¸å†…å®¹ã€‚
        
        è¾“å‡ºä¸ºæ¸…æ™°çš„ Markdown æ ¼å¼ã€‚
        """

        try:
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": f"ç´ æå†…å®¹ï¼š\n{raw_text[:60000]}"} # æ§åˆ¶æ€»é•¿åº¦
                ],
                temperature=0.3,
                max_tokens=4000,
                stream=True
            )
            
            print("\n" + "="*20 + " ç¬”è®°ç”Ÿæˆä¸­ " + "="*20 + "\n")
            collected = []
            for chunk in response:
                content = chunk.choices[0].delta.content
                if content:
                    print(content, end="", flush=True)
                    collected.append(content)
            print("\n")
            return "".join(collected)
            
        except Exception as e:
            print(f"   âŒ æ•´ç†å¤±è´¥: {e}")
            return f"æ•´ç†å¤±è´¥: {e}"

    def run(self, topic: str, queries: list[str]) -> str:
        print("\n" + "="*60)
        print(f"ğŸ”¬ ResearcherAgent v2.0 (Exa AI)")
        print(f"ğŸ“Œ é€‰é¢˜: {topic}")
        print("="*60)
        
        # 1. Exa æœç´¢ (ä¼˜å…ˆ)
        results = self.search_exa(topic, queries)
        
        # 2. å¦‚æœ Exa ç»“æœå¤ªå°‘ï¼Œä½¿ç”¨ Tavily è¡¥å……
        if len(results) < 3:
            tavily_results = self.search_tavily_fallback(queries)
            results.extend(tavily_results)
        
        if not results:
            print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å†…å®¹")
            return ""
            
        # 3. è¡¥å……çˆ¬å– (é’ˆå¯¹ Tavily æ¥æºæˆ– Exa æ²¡æŠ“åˆ°æ­£æ–‡çš„)
        self.scrape_missing_content(results)
        
        # 4. æ•´ç†ç¬”è®°
        notes = self.synthesize_notes(results, topic)
        
        # ä¿å­˜
        notes_file = get_research_notes_file()
        with open(notes_file, "w", encoding="utf-8") as f:
            f.write(f"# ğŸ”¬ è‡ªåŠ¨ç ”ç©¶ç¬”è®° (Exa AI)\n\n**é€‰é¢˜**: {topic}\n**æ—¶é—´**: {__import__('datetime').datetime.now()}\n\n---\n\n{notes}")
            
        print(f"\nğŸ“ ç¬”è®°å·²ä¿å­˜: {notes_file}")
        return notes

def main():
    agent = ResearcherAgent()
    agent.run("DeepSeek V3 éšè—åŠŸèƒ½", ["DeepSeek æ•™ç¨‹"])

if __name__ == "__main__":
    main()
