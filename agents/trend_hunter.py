"""
ğŸš€ å…¨ç½‘é€‰é¢˜é›·è¾¾ (Trend Hunter Agent) v2.1 - å¼ºåŒ–ç‰ˆ
æ–°å¢ï¼šå…¨ç½‘ AI çƒ­ç‚¹æœç´¢ï¼Œé™ä½ GitHub æƒé‡ï¼Œæå‡æ–°é—»/è¯„æµ‹æƒé‡
"""
import sys, os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import httpx, time

# æœç´¢é‡è¯•æ¬¡æ•°
MAX_RETRIES = 3
from datetime import datetime
from bs4 import BeautifulSoup
from duckduckgo_search import DDGS
from openai import OpenAI
from config import DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL, PROXY_URL, REQUEST_TIMEOUT, get_topic_report_file, get_today_dir

def get_github_trending():
    print("ğŸ” [1/7] æ‰«æ GitHub Trending...")
    url = "https://github.com/trending/python?since=daily"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    try:
        with httpx.Client(proxy=PROXY_URL, timeout=15) as client:
            resp = client.get(url, headers=headers)
            resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')
        repos = soup.select('article.Box-row') or soup.select('.Box-row')
        results = []
        for repo in repos[:10]:
            name_tag = repo.select_one('h2 a') or repo.select_one('h1 a')
            if not name_tag: continue
            name = name_tag.get_text(strip=True).replace('\n', '').replace(' ', '')
            desc_tag = repo.select_one('p.col-9') or repo.select_one('p')
            desc = desc_tag.get_text(strip=True) if desc_tag else ""
            results.append(f"{name}: {desc}")
        return results if results else ["GitHub æš‚æ— æ•°æ®"]
    except Exception as e:
        return [f"GitHub æŠ“å–å¤±è´¥: {e}"]

def get_readhub_news():
    print("ğŸ” [2/7] æ‰«æ ReadHub ç§‘æŠ€æ–°é—»...")
    url = "https://api.readhub.cn/topic?lastCursor=&pageSize=15"
    try:
        with httpx.Client(proxy=PROXY_URL, timeout=10) as client:
            resp = client.get(url)
            data = resp.json()
        items = data.get('data', [])
        return [item.get('title', '') for item in items] or ["ReadHub æš‚æ— æ•°æ®"]
    except Exception as e:
        return [f"ReadHub æŠ“å–å¤±è´¥: {e}"]

def search_with_retry(query, site_domain=None, max_results=8):
    """å¸¦é‡è¯•æœºåˆ¶çš„ DuckDuckGo æœç´¢"""
    for attempt in range(MAX_RETRIES):
        try:
            with DDGS(proxy=PROXY_URL) as ddgs:
                search_query = f"site:{site_domain} {query}" if site_domain else query
                results = [r.get('title', '') for r in ddgs.text(search_query, region='cn-zh', timelimit='w', max_results=max_results) if r.get('title')]
                if results:
                    return results
        except Exception as e:
            time.sleep(1)
    return []

def get_global_ai_news():
    """å…¨ç½‘ AI çƒ­ç‚¹æœç´¢ - æ ¸å¿ƒæ–°å¢åŠŸèƒ½"""
    print("ğŸ” [3/7] æ‰«æå…¨ç½‘ AI çƒ­ç‚¹æ–°é—»...")
    # å¢åŠ æ›´å¤šæ–°é—»å¯¼å‘çš„æœç´¢è¯ï¼Œå‡å°‘ä»£ç åº“å¹²æ‰°
    queries = [
        "AI äººå·¥æ™ºèƒ½ æœ€æ–°æ–°é—» 2025",
        "å¤§æ¨¡å‹ å‘å¸ƒ æ›´æ–° 2025",
        "GPT Claude Gemini DeepSeek è¯„æµ‹ 2025",
        "AI å·¥å…· çˆ†æ¬¾ æ¨è 2025",
        "äººå·¥æ™ºèƒ½ è¡Œä¸šåŠ¨æ€ 2025",
        "LLM å¤§è¯­è¨€æ¨¡å‹ æœ€æ–°è¿›å±•"
    ]
    all_results = []
    for q in queries:
        results = search_with_retry(q, max_results=6)
        all_results.extend(results)
    unique_results = list(set(all_results))
    print(f"   âœ“ è·å–åˆ° {len(unique_results)} æ¡å…¨ç½‘çƒ­ç‚¹")
    return unique_results if unique_results else ["å…¨ç½‘æœç´¢æš‚æ— ç»“æœ"]

def search_platform(site_domain, site_name, query="AI å·¥å…·"):
    print(f"ğŸ” æ‰«æ {site_name}...")
    results = search_with_retry(query, site_domain, max_results=8)
    return results if results else [f"{site_name} æš‚æ— æ•°æ®"]

def scan_all_sources():
    all_titles = []
    
    # 1. GitHub (é™æƒï¼Œä»…ä½œå‚è€ƒï¼Œæ”¾æœ€åæ”¶é›†)
    github_data = get_github_trending()
    
    # 2. ReadHub ç§‘æŠ€æ–°é—»
    readhub_data = get_readhub_news()
    all_titles.extend(readhub_data)  # æ–°é—»ä¼˜å…ˆåŠ å…¥
    
    # 3. å…¨ç½‘ AI çƒ­ç‚¹ (æ ¸å¿ƒï¼Œé«˜æƒé‡ï¼Œä¼˜å…ˆåŠ å…¥)
    global_news = get_global_ai_news()
    all_titles.extend(global_news)  # å…¨ç½‘çƒ­ç‚¹ä¼˜å…ˆåŠ å…¥
    
    # 4-6. ç¤¾äº¤å¹³å°
    print("ğŸ” [4/7] æ‰«æå°çº¢ä¹¦...")
    xiaohongshu_data = search_platform("xiaohongshu.com", "å°çº¢ä¹¦", "AIå·¥å…· æ•™ç¨‹")
    all_titles.extend(xiaohongshu_data)
    
    print("ğŸ” [5/7] æ‰«æå¾®åš...")
    weibo_data = search_platform("weibo.com", "å¾®åš", "AI äººå·¥æ™ºèƒ½")
    all_titles.extend(weibo_data)
    
    print("ğŸ” [6/7] æ‰«æå°‘æ•°æ´¾...")
    sspai_data = search_platform("sspai.com", "å°‘æ•°æ´¾", "AI æ•ˆç‡ å·¥å…·")
    all_titles.extend(sspai_data)
    
    # GitHub æœ€ååŠ å…¥ï¼Œé™ä½å…¶åœ¨çƒ­è¯æå–ä¸­çš„æƒé‡
    all_titles.extend(github_data)
    
    return {
        "github": github_data, 
        "readhub": readhub_data, 
        "global_news": global_news,
        "xiaohongshu": xiaohongshu_data, 
        "weibo": weibo_data, 
        "sspai": sspai_data, 
        "all_titles": all_titles
    }

KEYWORD_PROMPT = """ä½ æ˜¯çƒ­ç‚¹åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹æ··åˆäº†æ–°é—»ã€ç¤¾äº¤åª’ä½“å’Œä»£ç åº“çš„æ ‡é¢˜æ•°æ®ã€‚

ä»»åŠ¡ï¼šæå–å½“å‰ AI åœˆæœ€ç«çš„ 5 ä¸ªå…·ä½“è¯é¢˜æˆ–äº§å“åã€‚

âš ï¸ é‡è¦è§„åˆ™ï¼š
1. **æ–°é—»äº‹ä»¶ä¼˜å…ˆ**ï¼šå¦‚ "Gemini 2.0 å‘å¸ƒ", "DeepSeek V3 è¯„æµ‹", "OpenAI å‘å¸ƒä¼š"
2. **GitHub é¡¹ç›®é™æƒ**ï¼šé™¤éæ˜¯ç°è±¡çº§é¡¹ç›®ï¼ˆå¦‚å½“å¹´çš„ ChatGPTã€AutoGPTï¼‰ï¼Œå¦åˆ™ä¸è¦é€‰ GitHub ä»“åº“å
3. **å…³æ³¨å¤§æ¨¡å‹åŠ¨æ€**ï¼šGPTã€Claudeã€Geminiã€DeepSeekã€Kimi ç­‰æ¨¡å‹çš„æ›´æ–°ã€è¯„æµ‹ã€å¯¹æ¯”
4. å¿½ç•¥çº¯æŠ€æœ¯æœ¯è¯­ï¼ˆå¦‚ "v0.1.0 release", "fix bug"ï¼‰

è¾“å‡ºï¼šåªè¾“å‡ºå…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”ã€‚"""

def extract_hot_keywords(all_titles, http_client):
    print("\nğŸ§  [7/7] DeepSeek æ­£åœ¨åˆ†æå…¨ç½‘çƒ­è¯...\n")
    client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL, http_client=http_client)
    titles_text = "\n".join(all_titles[:80])
    try:
        response = client.chat.completions.create(
            model="deepseek-chat", messages=[{"role": "system", "content": KEYWORD_PROMPT},
            {"role": "user", "content": f"æ ‡é¢˜ï¼š\n{titles_text}"}], temperature=0.3)
        keywords = [kw.strip() for kw in response.choices[0].message.content.strip().split(',') if kw.strip()]
        print(f"ğŸ“Š ä»Šæ—¥çƒ­è¯ï¼š{keywords}\n")
        return keywords[:5]
    except Exception as e:
        print(f"çƒ­è¯æå–å¤±è´¥: {e}")
        return ["DeepSeek", "AI Agent", "æ•ˆç‡å·¥å…·"]

def search_wechat_by_keywords(keywords):
    print("ğŸ” æœç´¢å¾®ä¿¡å…¬ä¼—å·ç«å“...\n")
    all_results = []
    for kw in keywords:
        print(f"  â”œâ”€ å…³é”®è¯: {kw}")
        try:
            with DDGS(proxy=PROXY_URL) as ddgs:
                results = [f"    â€¢ {r.get('title', '')}" for r in ddgs.text(f"site:mp.weixin.qq.com {kw}", region='cn-zh', timelimit='w', max_results=3)]
                all_results.append(f"ã€{kw}ã€‘\n" + ("\n".join(results) if results else "    â€¢ æš‚æ— "))
        except Exception as e:
            all_results.append(f"ã€{kw}ã€‘\n    â€¢ æœç´¢å¤±è´¥: {e}")
        time.sleep(0.5)
    return "\n\n".join(all_results)

EDITOR_PROMPT = """ä½ å«"ç‹å¾€AI"ï¼Œä¸“æ³¨ AI å·¥ä½œæµçš„ç¡¬æ ¸åšä¸»ã€‚
è¯·æ ¹æ®ä»¥ä¸‹ã€å…¨ç½‘æƒ…æŠ¥ã€‘ç­›é€‰ 3 ä¸ªæœ€å€¼å¾—å†™çš„é€‰é¢˜ã€‚

âš ï¸ é€‰é¢˜ä¼˜å…ˆçº§ï¼ˆé‡è¦ï¼‰ï¼š
1. **æ–°é—»äº‹ä»¶ > è¯„æµ‹å¯¹æ¯” > å·¥å…·æ•™ç¨‹ > ä»£ç åº“ä»‹ç»**
2. ä¼˜å…ˆé€‰æ‹©ï¼šæ¨¡å‹å‘å¸ƒï¼ˆå¦‚ Gemini/DeepSeek/Claude æ›´æ–°ï¼‰ã€è¯„æµ‹æ’åã€äº‰è®®è¯é¢˜ã€çˆ†æ¬¾å·¥å…·
3. GitHub é¡¹ç›®ï¼šé™¤éå®ƒæå…·é¢ è¦†æ€§ï¼ˆå¦‚å½“å¹´çš„ AutoGPTï¼‰ï¼Œå¦åˆ™é™æƒå¤„ç†
4. å¿…é¡»ä»"å…¨ç½‘çƒ­ç‚¹"ä¸­é€‰è‡³å°‘ 1 ä¸ªè¯é¢˜

è¾“å‡ºæ ¼å¼ï¼š
### é€‰é¢˜ 1ï¼š[æ ‡é¢˜]
* **çƒ­åº¦æ¥æº**ï¼š[æ¥æºï¼Œå¦‚æ–°é—»/å¾®åš/è¯„æµ‹è§†é¢‘]
* **æ¨èç†ç”±**ï¼š[ä¸ºä»€ä¹ˆç°åœ¨å†™ä¼šç«]
* **æ ¸å¿ƒçœ‹ç‚¹**ï¼š[å…·ä½“å†™ä»€ä¹ˆ]
---
## ä»Šæ—¥ä¸»æ¨
å‘Šè¯‰æˆ‘ä¸å†™ä¼šåæ‚”çš„é‚£ä¸ªã€‚"""

def final_decision(scan_data, hot_keywords, wechat_data, http_client):
    print("\n" + "="*50 + "\nğŸ“ DeepSeek ä¸»ç¼–å®¡æ ¸ä¸­...\n" + "="*50 + "\n")
    client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL, http_client=http_client)
    # æŠ¥å‘Šé¡ºåºï¼šå…¨ç½‘çƒ­ç‚¹ä¼˜å…ˆï¼ŒGitHub é™æƒæ”¾æœ€å
    full_report = f"""=== ğŸ”¥ å…¨ç½‘ AI çƒ­ç‚¹ (é‡ç‚¹å…³æ³¨) ===
{chr(10).join(scan_data['global_news'])}

=== ğŸ“° ReadHub ç§‘æŠ€æ–°é—» ===
{chr(10).join(scan_data['readhub'])}

=== ğŸ“± å¾®åš/å°çº¢ä¹¦/å°‘æ•°æ´¾ ===
{chr(10).join(scan_data['weibo'] + scan_data['xiaohongshu'] + scan_data['sspai'])}

=== ğŸ’» GitHub Trending (ä»…ä¾›å‚è€ƒ) ===
{chr(10).join(scan_data['github'])}

=== ğŸ·ï¸ æå–çƒ­è¯ ===
{', '.join(hot_keywords)}

=== ğŸ“ å…¬ä¼—å·ç«å“ ===
{wechat_data}"""
    try:
        response = client.chat.completions.create(
            model="deepseek-reasoner", messages=[{"role": "system", "content": EDITOR_PROMPT},
            {"role": "user", "content": full_report}], stream=True)
        print("\n" + "="*20 + " é€‰é¢˜æŠ¥å‘Š " + "="*20 + "\n")
        collected = []
        for chunk in response:
            if chunk.choices[0].delta.content:
                c = chunk.choices[0].delta.content
                print(c, end="", flush=True)
                collected.append(c)
        print("\n\n" + "="*50 + "\n")
        return full_report, "".join(collected)
    except Exception as e:
        print(f"å†³ç­–å¤±è´¥: {e}")
        return full_report, f"å¤±è´¥: {e}"

def save_report(raw_data, hot_keywords, analysis):
    filename = get_topic_report_file()
    content = f"# ğŸš€ é€‰é¢˜é›·è¾¾æŠ¥å‘Š\n\n**æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n**ç›®å½•**: {get_today_dir()}\n\n## çƒ­è¯\n> {', '.join(hot_keywords)}\n\n## æƒ…æŠ¥\n```\n{raw_data}\n```\n\n## åˆ†æ\n{analysis}"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(content)
    print(f"ğŸ“ æŠ¥å‘Šå·²ä¿å­˜: {filename}")

def main():
    print("\n" + "="*60 + "\nğŸš€ å…¨ç½‘é€‰é¢˜é›·è¾¾ v2.1 (å¼ºåŒ–ç‰ˆ) - ç‹å¾€AI\n" + "="*60 + "\n")
    print("ğŸ“¡ Step 1/4: å¹¿åŸŸæ‰«æ (å«å…¨ç½‘ AI çƒ­ç‚¹)...\n")
    scan_data = scan_all_sources()
    print("\nğŸ“¡ Step 2/4: çƒ­è¯è’¸é¦...")
    with httpx.Client(proxy=PROXY_URL, timeout=REQUEST_TIMEOUT) as http_client:
        hot_keywords = extract_hot_keywords(scan_data['all_titles'], http_client)
        print("\nğŸ“¡ Step 3/4: ç«å“éªŒè¯...")
        wechat_data = search_wechat_by_keywords(hot_keywords)
        print("\nğŸ“¡ Step 4/4: æœ€ç»ˆå†³ç­–...")
        raw_data, analysis = final_decision(scan_data, hot_keywords, wechat_data, http_client)
    save_report(raw_data, hot_keywords, analysis)
    print("\nâœ… é€‰é¢˜é›·è¾¾å®Œæˆï¼")

if __name__ == "__main__":
    main()
