"""
ğŸš€ å…¨ç½‘é€‰é¢˜é›·è¾¾ (Trend Hunter Agent) v7.0 - ç»ˆæä»·å€¼æŒ–æ˜ç‰ˆ
æ ¸å¿ƒå‡çº§ï¼š
1. çŸ©é˜µåŒ– WATCHLISTï¼šè¦†ç›–æ¨¡å‹ã€ç¼–ç¨‹ã€æ•ˆç‡å·¥å…·ä¸‰ç±»é¡¶æµï¼Œå¤–åŠ é€šç”¨æ•™ç¨‹ç±»ç›®ã€‚
2. å¿ƒç†å­¦æœç´¢ç­–ç•¥ï¼šå¼•å…¥"å³æ—¶æ»¡è¶³"(Bè·¯)å’Œ"æŸå¤±åŒæ¶"(Cè·¯)æœç´¢æ¨¡å‹ã€‚
3. ä»·å€¼æ’åºç®—æ³•ï¼šåŸºäº"è·å¾—æ„Ÿ"è¿›è¡ŒåŠ æƒï¼Œå‰”é™¤å®å¤§å™äº‹ã€‚
"""
import sys, os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import time
import json
import httpx
import random
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from openai import OpenAI
from config import (
    DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL, PROXY_URL, REQUEST_TIMEOUT,
    TAVILY_API_KEY, get_topic_report_file, get_today_dir,
    get_stage_dir, get_research_notes_file
)

# ================= é…ç½®åŒº =================

# é•¿æœŸå…³æ³¨çŸ©é˜µ (æµé‡åŸºæœ¬ç›˜)
WATCHLIST = [
    # é¡¶æµæ¨¡å‹
    "DeepSeek", "Kimi", "é€šä¹‰åƒé—®", "GPT-4o", "Gemini", "Grok",
    # ç¼–ç¨‹ç¥å™¨
    "Cursor", "Windsurf", "Claude Code", "GitHub Copilot",
    # æ•ˆç‡åº”ç”¨
    "å¤¸å…‹AI", "è±†åŒ…", "ç§˜å¡”æœç´¢", "è…¾è®¯å…ƒå®",
    # é€šç”¨ç±»ç›®
    "AIæ•™ç¨‹", "AIå‰¯ä¸š", "æ•ˆç‡ç¥å™¨"
]

# è¿è¥é˜¶æ®µé…ç½®
OPERATIONAL_PHASE = "VALUE_HACKER" # ä»·å€¼é»‘å®¢

PHASE_CONFIG = {
    "VALUE_HACKER": {
        "name": "ä»·å€¼é»‘å®¢æ¨¡å¼",
        "weights": {"news": 0.5, "social": 2.5, "github": 1.0}, # æåº¦é‡ç¤¾äº¤å’Œç—›ç‚¹
        "strategy": "åˆ©ç”¨å¿ƒç†å­¦é”šç‚¹(æ”¶ç›Š/æŸå¤±)ï¼ŒæŒ–æ˜èƒ½ç»™ç”¨æˆ·å¸¦æ¥'è·å¾—æ„Ÿ'çš„é€‰é¢˜ã€‚",
        "prompt_suffix": "âš ï¸ ç»å¯¹åŸåˆ™ï¼šåƒä¸€ä¸ª'ç”Ÿæ´»é»‘å®¢'ä¸€æ ·æ€è€ƒã€‚å‰”é™¤æ‰€æœ‰'æ–°é—»æŠ¥é“'ï¼Œåªä¿ç•™'è§£å†³æ–¹æ¡ˆ'ã€‚å¦‚æœæ˜¯å·¥å…·ï¼Œå¿…é¡»æ˜¯æ™®é€šäººæ‰‹æœº/ç”µè„‘èƒ½è£…çš„ï¼›å¦‚æœæ˜¯æ•™ç¨‹ï¼Œå¿…é¡»æ˜¯å°ç™½èƒ½çœ‹æ‡‚çš„ã€‚"
    }
}

CURRENT_CONFIG = PHASE_CONFIG[OPERATIONAL_PHASE]

# ================= Tavily æœç´¢å·¥å…· =================

class WebSearchTool:
    def __init__(self):
        self.api_key = TAVILY_API_KEY
        self.enabled = bool(self.api_key and len(self.api_key) > 10)
        if self.enabled:
            print("   âœ… Tavily Search API å·²å¯ç”¨")
    
    def search(self, query, max_results=5, include_answer=False):
        if not self.enabled: return []
        print(f"   ğŸ” Tavily: {query}")
        url = "https://api.tavily.com/search"
        payload = {
            "api_key": self.api_key, "query": query, "search_depth": "basic",
            "max_results": max_results, "include_answer": include_answer
        }
        try:
            # Tavily éœ€è¦ä»£ç† (å¦‚æœé…ç½®äº† PROXY_URL)
            # ä½¿ç”¨ trust_env=False é˜²æ­¢è¯»å–ç³»ç»Ÿç¯å¢ƒå˜é‡å¯¼è‡´æ··ä¹±ï¼Œæ˜¾å¼æŒ‡å®š proxy
            proxies = PROXY_URL if PROXY_URL else None
            with httpx.Client(timeout=30, proxy=proxies) as client:
                resp = client.post(url, json=payload)
                resp.raise_for_status()
                data = resp.json()
                results = []
                if data.get('answer'):
                    results.append({"title": "AI Summary", "body": data['answer'], "url": ""})
                for r in data.get('results', []):
                    results.append({
                        "title": r.get('title', ''),
                        "body": r.get('content', ''),
                        "url": r.get('url', '')
                    })
                return results
        except Exception as e:
            print(f"      âŒ æœç´¢å¤±è´¥: {e}")
            return []

# ================= è¾…åŠ©å‡½æ•° =================

def get_github_trending():
    print("   ğŸ” GitHub Trending (Weekly)...")
    url = "https://github.com/trending?since=weekly" # å…¨è¯­è¨€ Weeklyï¼ŒèŒƒå›´æ›´å¹¿
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        with httpx.Client(proxy=PROXY_URL, timeout=15) as client:
            resp = client.get(url, headers=headers)
        soup = BeautifulSoup(resp.text, 'html.parser')
        repos = soup.select('article.Box-row')
        results = []
        for repo in repos[:5]:
            name_tag = repo.select_one('h1 a') or repo.select_one('h2 a')
            if not name_tag: continue
            name = name_tag.get_text(strip=True).replace('\n', '').replace(' ', '')
            desc = repo.select_one('p').get_text(strip=True) if repo.select_one('p') else ""
            # è¿‡æ»¤æ‰éAI/å·¥å…·ç±»çš„ä»“åº“(ç®€å•å…³é”®è¯è¿‡æ»¤)
            results.append(f"- {name}: {desc}")
        return results
    except Exception as e:
        return [f"- GitHub æŠ“å–å¤±è´¥: {e}"]

# ================= æ ¸å¿ƒé€»è¾‘ =================

PLAN_PROMPT = """
ä½ æ˜¯"ç‹å¾€AI"çš„é¦–å¸­å†…å®¹ç­–ç•¥å®˜ã€‚
è¯·åŸºäºã€å…¨ç½‘æƒ…æŠ¥ã€‘å’Œã€å¿ƒç†å­¦ç­–ç•¥ã€‘ï¼ŒæŒ–æ˜ 3 ä¸ªæœ€å…·"çˆ†æ¬¾æ½œè´¨"çš„é€‰é¢˜æ–¹å‘ã€‚

å¿ƒç†å­¦ç­–ç•¥ï¼š
1. **Aè·¯ (é”šç‚¹æ•ˆåº”)**: å€ŸåŠ¿é¡¶æµ (DeepSeek/Kimi)ï¼Œå…³æ³¨å…¶"éšè—åŠŸèƒ½"æˆ–"æœ€æ–°ç©æ³•"ã€‚
2. **Bè·¯ (å³æ—¶æ»¡è¶³)**: å¯»æ‰¾"æ•ˆç‡ç¥å™¨"ã€"Life Hack"ï¼Œä¸»æ‰“"3åˆ†é’Ÿä¸Šæ‰‹"ã€"ä¸‹ç­æ—©èµ°1å°æ—¶"ã€‚
3. **Cè·¯ (æŸå¤±åŒæ¶)**: å¯»æ‰¾"é¿å‘æŒ‡å—"ã€"æ™ºå•†ç¨"ã€"å¹³æ›¿"ã€"ç¿»è½¦ç°åœº"ï¼Œå¼•å‘ç”¨æˆ·å±æœºæ„Ÿã€‚

è¾“å…¥æ•°æ®ï¼š
- é•¿æœŸå…³æ³¨å“ç±»åŠ¨æ€
- æœ¬å‘¨çƒ­é—¨å·¥å…·/æ•™ç¨‹
- ç”¨æˆ·åæ§½ä¸ç—›ç‚¹

å†³ç­–æ ‡å‡†ï¼š
- âœ… **ä¿ç•™**ï¼šDeepSeek è”ç½‘æœç´¢æ€ä¹ˆç”¨æ‰å‡†ã€Cursor å…è´¹é¢åº¦æ²¡äº†æ€ä¹ˆåŠã€å¤¸å…‹æ‰«æç‹å¯¹æ¯”ã€‚
- âŒ **å‰”é™¤**ï¼šOpenAI èèµ„æ¶ˆæ¯ã€Google å‘å¸ƒæ–°è®ºæ–‡ã€æŸæŸè¡Œä¸šå¤§æ¨¡å‹ç™½çš®ä¹¦ã€‚

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼ JSONï¼‰ï¼š
[
    {
        "event": "é€‰é¢˜æ ¸å¿ƒè¯ (å¦‚: DeepSeek)",
        "angle": "åˆ‡å…¥è§’åº¦ (å¦‚: éšè—ç©æ³• / é¿å‘æŒ‡å—)",
        "news_query": "åŠŸèƒ½æ€§æœç´¢è¯ (å¦‚: DeepSeek V3 file upload)",
        "social_query": "æƒ…ç»ªæ€§æœç´¢è¯ (å¦‚: DeepSeek æŠ¥é”™ / DeepSeek ä¸å¥½ç”¨)"
    },
    ...
]
"""

def step1_broad_scan_and_plan(client, search_tool):
    """Step 1: å¹¿åŸŸä»·å€¼æ‰«æ (å¿ƒç†å­¦ä¸‰è·¯ç­–ç•¥)"""
    print(f"\nğŸ“¡ [Step 1] å¹¿åŸŸä»·å€¼æ‰«æ (ç­–ç•¥: {CURRENT_CONFIG['name']})...")
    
    pre_scan_results = []
    
    # === Aè·¯: é¡¶æµé”šç‚¹ (Watchlist) ===
    # éšæœºé€‰ 3 ä¸ªé¡¶æµï¼Œæœ"ç©æ³•"
    targets = random.sample(WATCHLIST, 3)
    print(f"   ğŸ¯ [Aè·¯-é”šç‚¹] æ‰«æé¡¶æµ: {targets}")
    for t in targets:
        res = search_tool.search(f"{t} éšè—åŠŸèƒ½ ç©æ³• æ•™ç¨‹ 2025", max_results=2)
        pre_scan_results.extend(res)
        
    # === Bè·¯: å³æ—¶æ»¡è¶³ (Life Hack) ===
    # æœ"ç¥å™¨"ã€"é»‘ç§‘æŠ€"
    print(f"   âš¡ [Bè·¯-æ”¶ç›Š] æ‰«ææ•ˆç‡ç¥å™¨...")
    queries = ["æœ¬å‘¨ AI æ•ˆç‡ç¥å™¨ æ¨è", "AI è‡ªåŠ¨åŒ–åŠå…¬ æ•™ç¨‹", "Notion AI æ›¿ä»£å“"]
    for q in queries:
        res = search_tool.search(q, max_results=2)
        pre_scan_results.extend(res)
        
    # === Cè·¯: æŸå¤±åŒæ¶ (Pain Points) ===
    # æœ"é¿å‘"ã€"æ™ºå•†ç¨"
    print(f"   ğŸ›¡ï¸ [Cè·¯-æŸå¤±] æ‰«æé¿å‘/åæ§½...")
    queries = ["AIå·¥å…· æ™ºå•†ç¨ é¿å‘", "AIçœ¼é•œ ç¿»è½¦", "AI å†™ä½œ æŸ¥é‡"]
    for q in queries:
        res = search_tool.search(q, max_results=2)
        pre_scan_results.extend(res)
    
    pre_scan_text = "\n".join([f"- {r['title']}: {r['body'][:80]}" for r in pre_scan_results])
    
    # 2. æ™ºèƒ½ç­›é€‰ä¸è§„åˆ’
    print(f"   ğŸ“ æƒ…æŠ¥èšåˆå®Œæ¯•ï¼ŒDeepSeek æ­£åœ¨åº”ç”¨å¿ƒç†å­¦ç­–ç•¥é€‰é¢˜...")
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": PLAN_PROMPT},
                {"role": "user", "content": f"ã€æ··åˆæƒ…æŠ¥æ± ã€‘\n{pre_scan_text}"}
            ],
            temperature=0.7,
            response_format={ "type": "json_object" }
        )
        content = response.choices[0].message.content
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        
        search_plan = json.loads(content)
        if isinstance(search_plan, dict) and "events" in search_plan:
            search_plan = search_plan["events"]
            
        print(f"   ğŸ§  é€‰é¢˜æ–¹å‘å·²é”å®š: {[i['event'] + '-' + i['angle'] for i in search_plan]}\n")
        return search_plan
    except Exception as e:
        print(f"   âŒ è§„åˆ’å¤±è´¥: {e}")
        return [{"event": "DeepSeek", "angle": "é¿å‘", "news_query": "DeepSeek V3", "social_query": "DeepSeek å¹»è§‰"}]

def step2_deep_scan(search_plan, search_tool):
    """Step 2: æ·±åº¦éªŒè¯ (é‡ç¤¾äº¤/ç—›ç‚¹)"""
    print("ğŸ“¡ [Step 2] å¯åŠ¨æ·±åº¦ä»·å€¼éªŒè¯...\n")
    all_results = []
    
    w_news = CURRENT_CONFIG['weights']['news']
    w_social = CURRENT_CONFIG['weights']['social']
    
    for item in search_plan:
        event = item.get("event", "æœªçŸ¥")
        angle = item.get("angle", "é€šç”¨")
        news_q = item.get("news_query", "")
        social_q = item.get("social_query", "")
        
        print(f"   ğŸ” æ­£åœ¨æ·±æŒ–: ã€{event}ã€‘ ({angle}æ–¹å‘)")
        event_data = [f"=== é€‰é¢˜: {event} ({angle}) ==="]
        
        # 1. ç¤¾äº¤/ç—›ç‚¹æœç´¢ (æ ¸å¿ƒ)
        if social_q:
            print(f"      ğŸ’¬ ç¤¾äº¤èˆ†æƒ… (æƒé‡ {w_social}): {social_q}")
            # å¢åŠ çŸ¥ä¹ã€Bç«™(site:bilibili.com)
            full_social_q = f"{social_q} site:mp.weixin.qq.com OR site:xiaohongshu.com OR site:zhihu.com OR site:bilibili.com"
            res = search_tool.search(full_social_q, max_results=4)
            if res:
                event_data.append(f"--- ç”¨æˆ·çœŸå®åé¦ˆ ({social_q}) ---")
                event_data.extend([f"- {r['title']}: {r['body'][:80]}..." for r in res])
                
        # 2. å®˜æ–¹éªŒè¯ (è¾…åŠ©)
        if news_q:
            print(f"      ğŸ”¥ å®˜æ–¹éªŒè¯ (æƒé‡ {w_news}): {news_q}")
            res = search_tool.search(news_q, max_results=2)
            if res:
                event_data.append(f"--- å®˜æ–¹ä¿¡æ¯ ({news_q}) ---")
                event_data.extend([f"- {r['title']}" for r in res])
        
        all_results.append("\n".join(event_data))
        print("")
        time.sleep(1)

    # GitHub è¡¥å…… (Weekly)
    print(f"   ğŸ’» GitHub Weekly Trending...")
    github_res = get_github_trending()
    all_results.append("=== GitHub Weekly Trending ===\n" + "\n".join(github_res))
    
    return "\n\n".join(all_results)

def step3_final_decision(scan_data, client):
    """Step 3: å†³ç­–"""
    print("\n" + "="*50 + "\nğŸ“ DeepSeek ä¸»ç¼–å®¡æ ¸ä¸­...\n" + "="*50)
    
    prompt = f"""
    {EDITOR_PROMPT}
    
    å½“å‰ç­–ç•¥ï¼šã€{CURRENT_CONFIG['name']}ã€‘
    {CURRENT_CONFIG['prompt_suffix']}
    """
    
    try:
        response = client.chat.completions.create(
            model="deepseek-reasoner",
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": f"ã€æ·±åº¦éªŒè¯æƒ…æŠ¥ã€‘\n{scan_data}"}
            ],
            stream=True
        )
        
        print("\n" + "="*20 + " é€‰é¢˜æŠ¥å‘Š " + "="*20 + "\n")
        collected = []
        for chunk in response:
            if chunk.choices[0].delta.content:
                c = chunk.choices[0].delta.content
                print(c, end="", flush=True)
                collected.append(c)
        return "".join(collected)
    except Exception as e:
        print(f"âŒ å†³ç­–å¤±è´¥: {e}")
        return f"å¤±è´¥: {e}"

EDITOR_PROMPT = """
ä½ å«"ç‹å¾€AI"ï¼Œä¸“æ³¨ AI å·¥ä½œæµçš„ç¡¬æ ¸åšä¸»ã€‚
è¯·ç­›é€‰ 3 ä¸ªã€è·å¾—æ„Ÿæœ€é«˜ã€‘çš„é€‰é¢˜ã€‚

**è·å¾—æ„Ÿå…¬å¼** = (å¸®ç”¨æˆ·è§£å†³çš„ç—›ç‚¹ * èŠ‚çœçš„æ—¶é—´/é‡‘é’±) - é˜…è¯»é—¨æ§›

å†³ç­–é€»è¾‘ï¼š
1. **åªåšäººè¯**ï¼šæ‹’ç»æ‰€æœ‰æŠ€æœ¯é»‘è¯ï¼ŒæŠŠ"ä¸Šä¸‹æ–‡ç¼“å­˜"ç¿»è¯‘æˆ"è®©AIè®°ä½ä½ ä¸Šå‘¨è¯´äº†å•¥"ã€‚
2. **åªåšç—›ç‚¹**ï¼šä¼˜å…ˆé€‰"é¿å‘"ã€"å¹³æ›¿"ã€"ç™½å«–"ã€"ææ•ˆ"ç±»é€‰é¢˜ã€‚
3. **å…³è”çƒ­ç‚¹**ï¼šå¦‚æœæ¶‰åŠ WATCHLIST ä¸­çš„äº§å“ï¼ŒåŠ åˆ†ã€‚

è¾“å‡ºæ ¼å¼ï¼š
### é€‰é¢˜ 1ï¼š[æ ‡é¢˜] (éœ€æå…·å¸å¼•åŠ›ï¼Œå¦‚ï¼šDeepSeek å±…ç„¶è¿˜èƒ½è¿™ä¹ˆç©ï¼Ÿ)
* **è·å¾—æ„Ÿ**ï¼š[ç”¨æˆ·çœ‹å®Œèƒ½å¾—åˆ°ä»€ä¹ˆï¼Ÿçœé’±ï¼Ÿçœæ—¶ï¼Ÿ]
* **å¿ƒç†é”šç‚¹**ï¼š[åˆ©ç”¨äº†ä»€ä¹ˆå¿ƒç†ï¼Ÿè´ªä¾¿å®œï¼Ÿæ€•è½åï¼Ÿ]
* **æ ¸å¿ƒçœ‹ç‚¹**ï¼š[æ–‡ç« å¤§çº²ï¼ŒåŒ…å«å…·ä½“çš„å·¥å…·/æŠ€å·§]
---
## ä»Šæ—¥ä¸»æ¨
å‘Šè¯‰æˆ‘ä¸å†™ä¼šåæ‚”çš„é‚£ä¸ª (è·å¾—æ„Ÿæœ€å¼ºçš„)ã€‚
"""

def auto_init_workflow():
    """è‡ªåŠ¨åˆå§‹åŒ–åç»­å·¥ä½œæµæ–‡ä»¶å¤¹å’Œæ–‡ä»¶"""
    print("\nâš™ï¸ æ­£åœ¨åˆå§‹åŒ–åç»­å·¥ä½œæµ...")
    
    # 1. é¢„åˆ›å»ºæ‰€æœ‰é˜¶æ®µæ–‡ä»¶å¤¹
    from config import get_stage_dir, get_research_notes_file # å±€éƒ¨å¯¼å…¥é¿å…å¾ªç¯å¼•ç”¨é£é™©
    stages = ["research", "drafts", "publish", "assets"]
    for stage in stages:
        path = get_stage_dir(stage)
        print(f"   ğŸ“‚ ç›®å½•å°±ç»ª: {path}")
        
    # 2. åˆ›å»ºç©ºç™½ç ”ç©¶ç¬”è®°
    notes_file = get_research_notes_file()
    if not os.path.exists(notes_file):
        with open(notes_file, "w", encoding="utf-8") as f:
            f.write("# ç ”ç©¶ç¬”è®°\n\nè¯·å°† NotebookLM ç”Ÿæˆçš„ Briefing Doc ç²˜è´´åœ¨è¿™é‡Œ...\n")
        print(f"   ğŸ“„ ç¬”è®°æ–‡ä»¶å·²åˆ›å»º: {notes_file}")
        
    # 3. è¾“å‡º NotebookLM æç¤ºè¯
    print("\n" + "="*50)
    print("ğŸ’¡ NotebookLM æç¤ºè¯ (è¯·å¤åˆ¶åˆ° NotebookLM)")
    print("="*50)
    print("""
è¯·åŸºäºæˆ‘æä¾›çš„èµ„æ–™ï¼Œç”Ÿæˆä¸€ä»½ã€æ·±åº¦å†…å®¹ç®€æŠ¥ + è§†è§‰è„šæœ¬ã€‘ã€‚
ç›®æ ‡è¯»è€…æ˜¯ï¼šå¸Œæœ›æå‡å·¥ä½œæ•ˆç‡çš„èŒåœºäºº / æƒ³è¦å­¦ä¹ AIçš„å°ç™½ã€‚

### PART 1: æ·±åº¦ç®€æŠ¥ (ç”¨äºå†™æ–‡ç« )
1. **æ ¸å¿ƒè§‚ç‚¹ (The Hook)**ï¼šä¸€å¥è¯æ€»ç»“è¿™ä¸ªå·¥å…·/æŠ€å·§æœ€å¤§çš„ä»·å€¼ï¼ˆçœäº†å¤šå°‘é’±ï¼Ÿå¿«äº†å¤šå°‘å€ï¼Ÿï¼‰ã€‚
2. **ç—›ç‚¹æ‰“å‡» (Pain Points)**ï¼šç”¨æˆ·åœ¨æ²¡æœ‰è¿™ä¸ªå·¥å…·å‰ï¼Œé‡åˆ°äº†ä»€ä¹ˆå…·ä½“éº»çƒ¦ï¼Ÿ
3. **å®æ“æ­¥éª¤ (How-to)**ï¼šæå–å‡ºå…·ä½“çš„æ­¥éª¤ã€æŒ‡ä»¤æˆ–å‚æ•°é…ç½®ï¼ˆä¸è¦æ³›æ³›è€Œè°ˆï¼‰ã€‚
4. **é¿å‘æŒ‡å— (Watch Out)**ï¼šæœ‰ä»€ä¹ˆé™åˆ¶ã€æ”¶è´¹é™·é˜±æˆ–å¸¸è§æŠ¥é”™ï¼Ÿ
5. **ç²¾å½©é‡‘å¥ (Quotes)**ï¼šåŸæ–‡ä¸­é‚£äº›ç›´å‡»äººå¿ƒçš„å¥å­ã€‚

### PART 2: è§†è§‰è„šæœ¬ (ç”¨äºé…å›¾)
è¯·æ ¹æ®å†…å®¹ï¼Œå»ºè®® 3-5 å¼ å…³é”®é…å›¾æ–¹æ¡ˆï¼š
* **[æˆªå›¾]**ï¼šéœ€è¦æˆªå–è½¯ä»¶çš„å“ªä¸ªå…·ä½“ç•Œé¢ï¼Ÿ(ä¾‹å¦‚ï¼šDeepSeek API Key çš„åˆ›å»ºé¡µé¢)
* **[å¯¹æ¯”å›¾]**ï¼šéœ€è¦å¯¹æ¯”å“ªäº›æ ¸å¿ƒæ•°æ®ï¼Ÿ(ä¾‹å¦‚ï¼šV3 vs GPT-4 çš„ä»·æ ¼/é€Ÿåº¦å¯¹æ¯”è¡¨)
* **[ç¤ºä¾‹å›¾]**ï¼šéœ€è¦å±•ç¤ºä»€ä¹ˆæ ·çš„ç”Ÿæˆæ•ˆæœï¼Ÿ(ä¾‹å¦‚ï¼šç”Ÿæˆçš„ä»£ç ç‰‡æ®µæˆªå›¾)
* **[æ¢—å›¾/è¡¨æƒ…]**ï¼šæœ‰ä»€ä¹ˆæ§½ç‚¹æˆ–åå·®é€‚åˆåšè¡¨æƒ…åŒ…ï¼Ÿ

è¯·ç”¨ Markdown æ ¼å¼è¾“å‡ºï¼Œé€»è¾‘æ¸…æ™°ï¼Œå¹²è´§æ»¡æ»¡ã€‚
""")
    print("="*50)

def save_report(raw_data, analysis):
    filename = get_topic_report_file()
    content = f"# ğŸš€ é€‰é¢˜é›·è¾¾æŠ¥å‘Š v7.0 ({CURRENT_CONFIG['name']})\n\n**æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n**ç­–ç•¥**: {CURRENT_CONFIG['strategy']}\n\n## æ·±åº¦éªŒè¯æƒ…æŠ¥\n{raw_data}\n\n## é€‰é¢˜åˆ†æ\n{analysis}"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(content)
    print(f"\n\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜: {filename}")
    
    # ä¿å­˜åè‡ªåŠ¨åˆå§‹åŒ–å·¥ä½œæµ
    auto_init_workflow()

def main():
    print("\n" + "="*60 + "\nğŸš€ å…¨ç½‘é€‰é¢˜é›·è¾¾ v7.0 (ä»·å€¼æŒ–æ˜ç‰ˆ) - ç‹å¾€AI\n" + "="*60 + "\n")
    
    search_tool = WebSearchTool()
    
    # DeepSeek å»ºè®®ç›´è¿ï¼Œä¸èµ°ä»£ç† (é™¤é api.deepseek.com è¢«å¢™)
    # è¿™é‡Œæˆ‘ä»¬å°† proxy è®¾ä¸º Noneï¼Œç¡®ä¿å®ƒä¸èµ° PROXY_URL
    with httpx.Client(proxy=None, timeout=REQUEST_TIMEOUT) as http_client:
        client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL, http_client=http_client)
        
        # 1. å¹¿åŸŸæ‰«æ (Watchlist + Trend + Pain)
        search_plan = step1_broad_scan_and_plan(client, search_tool)
        
        # 2. æ·±åº¦éªŒè¯
        raw_data = step2_deep_scan(search_plan, search_tool)
        
        # 3. å†³ç­–
        analysis = step3_final_decision(raw_data, client)
        
        # 4. ä¿å­˜
        save_report(raw_data, analysis)
    
    print("\nâœ… é€‰é¢˜é›·è¾¾å®Œæˆï¼")

if __name__ == "__main__":
    main()
