"""
ğŸš€ å…¨ç½‘é€‰é¢˜é›·è¾¾ (Trend Hunter Agent) v4.0 (Hardcore Edition)
æ ¸å¿ƒç­–ç•¥ï¼š
1. ä¸‰çº§å®¹é”™æœºåˆ¶ï¼šJina Primary -> Jina Backup (RSS) -> Tavily Searchï¼Œç¡®ä¿æ•°æ®æºç¨³å®šã€‚
2. éšæœºåŒ–æ‰«æï¼šBè·¯(æ•ˆç‡)ä¸Cè·¯(é¿å‘)é‡‡ç”¨éšæœºæŠ½å–ç­–ç•¥ï¼Œé¿å…é‡å¤ã€‚
3. ä¸¥æ ¼å»é‡ï¼šåŸºäºå†å²è®°å½•çš„è‡ªåŠ¨å»é‡ä¸æ–°è¯æ‰¶æŒæœºåˆ¶ã€‚
"""
import sys, os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import time
import json
import re
import httpx
import random
from json_repair import repair_json
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from typing import List, Dict, Optional, Any
from bs4 import BeautifulSoup
from openai import OpenAI
from config import (
    DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL, PROXY_URL, REQUEST_TIMEOUT,
    TAVILY_API_KEY, get_topic_report_file, get_today_dir,
    get_stage_dir, get_research_notes_file, get_history_file, get_logger, retryable
)
from settings_data import (
    WATCHLIST, TREND_SOURCES, OPERATIONAL_PHASE, PHASE_CONFIG,
    EFFICIENCY_KEYWORDS, PAIN_KEYWORDS, RADAR_QUERIES,
    MAX_CONCURRENT_FETCHES, FETCH_TIMEOUT_SECONDS
)


logger = get_logger(__name__)


def log_print(*args, **kwargs):
    end = kwargs.get("end", "\n")
    flush = kwargs.get("flush", False)
    msg = " ".join(str(a) for a in args)

    if end == "" or flush:
        sys.stdout.write(msg + end)
        if flush:
            sys.stdout.flush()
        return

    if "âŒ" in msg:
        logger.error(msg)
    elif "âš ï¸" in msg or "ğŸ›¡ï¸" in msg:
        logger.warning(msg)
    else:
        logger.info(msg)

# ================= å†å²è®°å½•ç®¡ç† =================

def load_history():
    """åŠ è½½æœ€è¿‘ 7 å¤©çš„å†å²é€‰é¢˜"""
    history_file = get_history_file()
    if not os.path.exists(history_file):
        return []
    
    try:
        with open(history_file, "r", encoding="utf-8") as f:
            history = json.load(f)
            
        # è¿‡æ»¤å‡ºæœ€è¿‘ 7 å¤©çš„
        recent_history = []
        today = datetime.now()
        for item in history:
            date_str = item.get("date")
            try:
                item_date = datetime.strptime(date_str, "%Y-%m-%d")
                if (today - item_date).days <= 7:
                    recent_history.append(item)
            except:
                continue
        return recent_history
    except Exception:
        return []

def save_topic_to_history(topic, angle):
    """ä¿å­˜é€‰ä¸­é€‰é¢˜åˆ°å†å²è®°å½•"""
    history_file = get_history_file()
    history = []
    if os.path.exists(history_file):
        try:
            with open(history_file, "r", encoding="utf-8") as f:
                history = json.load(f)
        except:
            pass
            
    new_entry = {
        "date": datetime.now().strftime("%Y-%m-%d"),
        "topic": topic,
        "angle": angle
    }
    history.append(new_entry)
    
    # åªä¿ç•™æœ€è¿‘ 30 æ¡
    if len(history) > 30:
        history = history[-30:]
        
    with open(history_file, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)
    log_print(f"   ğŸ’¾ å†å²è®°å½•å·²æ›´æ–°: {topic}")

# ================= é…ç½®åŒºï¼ˆä» settings_data.py å¯¼å…¥ï¼‰ =================

CURRENT_CONFIG = PHASE_CONFIG[OPERATIONAL_PHASE]

# ================= Tavily æœç´¢å·¥å…· =================

class WebSearchTool:
    def __init__(self):
        self.api_key = TAVILY_API_KEY
        self.enabled = bool(self.api_key and len(self.api_key) > 10)
        if self.enabled:
            log_print("   âœ… Tavily Search API å·²å¯ç”¨")
    
    def search(self, query, max_results=5, include_answer=False, topic=None, days=3):
        """Tavily æœç´¢ï¼Œå¼ºåˆ¶åªè¿”å›æœ€è¿‘ N å¤©çš„æ–°é—»"""
        if not self.enabled: return []
        log_print(f"   ğŸ” Tavily (æœ€è¿‘{days}å¤©): {query}")
        url = "https://api.tavily.com/search"
        payload = {
            "api_key": self.api_key,
            "query": query,
            "search_depth": "advanced",  # ä½¿ç”¨ advanced ä»¥æ”¯æŒæ—¶é—´è¿‡æ»¤
            "max_results": max_results,
            "include_answer": include_answer,
            "days": days                  # åªçœ‹æœ€è¿‘ N å¤©çš„çƒ­ç‚¹
        }
        if topic:
            payload["topic"] = topic
            
        try:
            # Tavily éœ€è¦ä»£ç† (å¦‚æœé…ç½®äº† PROXY_URL)
            # ä½¿ç”¨ trust_env=False é˜²æ­¢è¯»å–ç³»ç»Ÿç¯å¢ƒå˜é‡å¯¼è‡´æ··ä¹±ï¼Œæ˜¾å¼æŒ‡å®š proxy
            proxies = PROXY_URL if PROXY_URL else None
            with httpx.Client(timeout=30, proxy=proxies) as client:
                @retryable
                def _post():
                    return client.post(url, json=payload)

                resp = _post()
                resp.raise_for_status()
                data = resp.json()
                results = []
                if data.get('answer'):
                    results.append({"title": "AI Summary", "body": data['answer'], "url": ""})
                for r in data.get('results', []):
                    results.append({
                        "title": r.get('title', ''),
                        "body": r.get('content', ''),
                        "url": r.get('url', '')
                    })
                return results
        except Exception as e:
            log_print(f"      âŒ æœç´¢å¤±è´¥: {e}")
            return []

# ================= è¾…åŠ©å‡½æ•° =================

def get_github_trending():
    log_print("   ğŸ” GitHub Trending (Weekly)...")
    url = "https://github.com/trending?since=weekly" # å…¨è¯­è¨€ Weeklyï¼ŒèŒƒå›´æ›´å¹¿
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        with httpx.Client(proxy=PROXY_URL, timeout=15) as client:
            @retryable
            def _get():
                return client.get(url, headers=headers)

            resp = _get()
        soup = BeautifulSoup(resp.text, 'html.parser')
        repos = soup.select('article.Box-row')
        results = []
        for repo in repos[:5]:
            name_tag = repo.select_one('h1 a') or repo.select_one('h2 a')
            if not name_tag: continue
            name = name_tag.get_text(strip=True).replace('\n', '').replace(' ', '')
            desc = repo.select_one('p').get_text(strip=True) if repo.select_one('p') else ""
            # è¿‡æ»¤æ‰éAI/å·¥å…·ç±»çš„ä»“åº“(ç®€å•å…³é”®è¯è¿‡æ»¤)
            results.append(f"- {name}: {desc}")
        return results
    except Exception as e:
        return [f"- GitHub æŠ“å–å¤±è´¥: {e}"]

# ================= çƒ­æ¦œåŠ¨æ€æŠ“å– =================

def _fetch_single_source(
    source: Dict[str, str],
    search_tool: Optional["WebSearchTool"]
) -> Optional[str]:
    """
    æŠ“å–å•ä¸ªçƒ­æ¦œæºï¼ˆä¾›å¹¶å‘è°ƒç”¨ï¼‰ã€‚
    éš”ç¦»å¼‚å¸¸ï¼Œä¿è¯å•æºå¤±è´¥ä¸å½±å“æ•´ä½“ã€‚
    """
    try:
        return _fetch_with_fallback(
            source["primary"],
            source["backup"],
            source["name"],
            search_tool
        )
    except Exception as e:
        log_print(f"      âš ï¸ [{source['name']}] æŠ“å–å¼‚å¸¸: {e}")
        return None


def fetch_dynamic_trends(
    client: OpenAI,
    search_tool: Optional["WebSearchTool"] = None
) -> List[str]:
    """
    ä»çƒ­æ¦œç½‘ç«™å¹¶å‘æŠ“å–å®æ—¶å…³é”®è¯ï¼ˆä¸‰çº§å®¹é”™æœºåˆ¶ï¼‰
    1. Jina Primary -> 2. Jina Backup (RSS) -> 3. Tavily Search
    
    ä½¿ç”¨ ThreadPoolExecutor å®ç°å¤šæºå¹¶å‘ï¼Œæ˜¾è‘—æå‡é‡‡é›†æ•ˆç‡ã€‚
    å•ä¸ªæºçš„è¶…æ—¶/å¤±è´¥ä¸ä¼šé˜»å¡å…¶ä»–æºã€‚
    """
    log_print("   ğŸŒ [çƒ­æ¦œæŠ“å–] ä»å…¨ç½‘çƒ­æ¦œè·å–å®æ—¶è¶‹åŠ¿ (å¹¶å‘æ¨¡å¼)...")
    
    # æ•°æ®æºé…ç½®å·²ç§»è‡³ settings_data.py
    sources = TREND_SOURCES
    
    # ===== Phase 1: å¹¶å‘æŠ“å–æ‰€æœ‰æº =====
    source_contents: Dict[str, Optional[str]] = {}
    
    with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_FETCHES) as executor:
        future_to_source = {
            executor.submit(_fetch_single_source, src, search_tool): src
            for src in sources
        }
        
        for future in as_completed(future_to_source):
            src = future_to_source[future]
            try:
                content = future.result(timeout=FETCH_TIMEOUT_SECONDS)
                source_contents[src["name"]] = content
            except Exception as e:
                log_print(f"      âš ï¸ [{src['name']}] å¹¶å‘ä»»åŠ¡å¼‚å¸¸: {e}")
                source_contents[src["name"]] = None
    
    log_print(f"   ğŸ“Š æŠ“å–å®Œæˆ: {sum(1 for v in source_contents.values() if v)}/{len(sources)} ä¸ªæºæˆåŠŸ")
    
    # ===== Phase 2: ä¸²è¡Œæå–å…³é”®è¯ï¼ˆLLM è°ƒç”¨ä¸å®œè¿‡åº¦å¹¶å‘ï¼‰ =====
    all_keywords: List[str] = []
    
    for src in sources:
        content = source_contents.get(src["name"])
        if content:
            keywords = _extract_keywords_from_single_source(
                client,
                content,
                src["name"],
                src["tag"]
            )
            all_keywords.extend(keywords)
    
    if not all_keywords:
        log_print("      âš ï¸ æ‰€æœ‰çƒ­æ¦œæºæå–å…³é”®è¯å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨")
        return []
    
    # å»é‡å¹¶é™åˆ¶æ•°é‡
    unique_keywords = list(dict.fromkeys(all_keywords))[:10]
    log_print(f"   ğŸ”¥ [çƒ­æ¦œæ±‡æ€»] å®æ—¶å…³é”®è¯: {unique_keywords}")
    return unique_keywords


def _fetch_with_fallback(
    primary_url: str,
    backup_url: str,
    source_name: str,
    search_tool: Optional["WebSearchTool"] = None
) -> Optional[str]:
    """
    ä¸‰çº§è·å–ç­–ç•¥ï¼šJina Primary -> Jina Backup -> Tavily Search
    """
    jina_base = "https://r.jina.ai/"
    
    # 1. å°è¯• Jina Primary
    content = _fetch_via_jina(jina_base + primary_url, source_name, "primary")
    if content and len(content) >= 500:
        return content
    
    # 2. å°è¯• Jina Backup (RSS)
    if backup_url:
        log_print(f"      ğŸ”„ [{source_name}] Primary å¤±è´¥ï¼Œå°è¯• Backup (RSS)...")
        content = _fetch_via_jina(jina_base + backup_url, source_name, "backup")
        if content and len(content) >= 500:
            return content

    # 3. å°è¯• Tavily ç»ˆææ•‘æ´
    if search_tool and search_tool.enabled:
        log_print(f"      ğŸ›¡ï¸ [{source_name}] å¯ç”¨ Tavily ç»ˆææ•‘æ´...")
        # æ„é€ æœç´¢è¯
        query = f"{source_name} çƒ­é—¨ AI ç§‘æŠ€å†…å®¹ {datetime.now().strftime('%Y-%m-%d')}"
        results = search_tool.search(query, max_results=3, days=3)
        if results:
            # æ‹¼æ¥ Tavily çš„æœç´¢ç»“æœä½œä¸ºä¼ªé€ çš„"ç½‘é¡µå†…å®¹"
            combined_text = "\n".join([f"Title: {r['title']}\nSnippet: {r['body']}" for r in results])
            log_print(f"      âœ… [{source_name}] Tavily æ•‘æ´æˆåŠŸ: æŠ“å– {len(results)} æ¡ç»“æœ")
            return combined_text
            
    log_print(f"      âŒ [{source_name}] æ‰€æœ‰é€šé“å‡å¤±è´¥")
    return None


def _fetch_via_jina(url: str, source_name: str, url_type: str) -> Optional[str]:
    """
    é€šè¿‡ Jina Reader API è·å–ç½‘é¡µå†…å®¹
    """
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "x-no-cache": "true"  # å¼ºåˆ¶ Jina Reader æŠ“å–æœ€æ–°é¡µé¢ï¼Œä¸è¿”å›ç¼“å­˜
        }
        with httpx.Client(proxy=PROXY_URL, timeout=30) as client:
            @retryable
            def _get():
                return client.get(url, headers=headers)

            resp = _get()
            
            if resp.status_code != 200:
                log_print(f"      âš ï¸ [{source_name}] {url_type} çŠ¶æ€ç : {resp.status_code}")
                return None
            
            content = resp.text
            if len(content) < 500:
                log_print(f"      âš ï¸ [{source_name}] {url_type} å†…å®¹è¿‡çŸ­: {len(content)} å­—ç¬¦")
                return None
            
            log_print(f"      âœ… [{source_name}] {url_type} æˆåŠŸ: {len(content)} å­—ç¬¦")
            return content[:8000]  # é™åˆ¶é•¿åº¦ï¼Œé¿å… token è¿‡å¤š
            
    except httpx.TimeoutException:
        log_print(f"      âš ï¸ [{source_name}] {url_type} è¶…æ—¶")
        return None
    except Exception as e:
        log_print(f"      âš ï¸ [{source_name}] {url_type} å¼‚å¸¸: {e}")
        return None


def _extract_keywords_from_single_source(
    client: OpenAI,
    content: str,
    name: str,
    tag: str
) -> List[str]:
    """
    ä½¿ç”¨ LLM ä»å•ä¸ªçƒ­æ¦œæºä¸­æå–å…³é”®è¯ï¼ˆå¸¦ä¸¥æ ¼é™å™ªè¿‡æ»¤ï¼‰
    """
    if not content:
        return []
    
    # é™åˆ¶å†…å®¹é•¿åº¦ï¼ˆä¿ç•™è¾ƒå¤šå†…å®¹ä»¥è¦†ç›–çƒ­æ¦œå‰50åï¼‰
    content_truncated = content[:8000]
    
    prompt = f"""
è¿™æ˜¯ã€{name}ã€‘ä»Šå¤©çš„çƒ­æ¦œæˆ–æœç´¢æ‘˜è¦ã€‚
è¯·ä»ä¸­æå– 2-3 ä¸ªæœ€ç¬¦åˆ"{tag}"é¢†åŸŸçš„å…·ä½“æŠ€æœ¯åè¯æˆ–äº§å“åç§°ã€‚

âš ï¸ å…³é”®è¿‡æ»¤è§„åˆ™ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š
1. ğŸ”´ **ç»å¯¹æ’é™¤åº•å±‚æŠ€æœ¯**ï¼šä¸¥ç¦æå– åç«¯æ¡†æ¶(Spring Boot/Django)ã€æ•°æ®åº“(Redis/SQL)ã€è¿ç»´(K8s/Docker)ã€åº•å±‚é©±åŠ¨(CUDA/NATS)ã€ç¼–ç¨‹è¯­è¨€ç‰ˆæœ¬(Java 21/Vite 8)ã€‚**æˆ‘ä»¬åªè¦ç»™å°ç™½ç”¨çš„å·¥å…·ï¼**
2. ğŸŸ¢ **åªä¿ç•™åº”ç”¨å±‚**ï¼š
   - AI åº”ç”¨/å¤§æ¨¡å‹ (DeepSeek, Kimi, Claude 4.5, Sora)
   - æ•ˆç‡å·¥å…· (Notion, Cursor, Obsidian, Arcæµè§ˆå™¨)
   - è½åœ°ç©æ³• (AIåšPPT, æ™ºèƒ½ä½“å¼€å‘, æœ¬åœ°éƒ¨ç½²)
   - è¡Œä¸šçƒ­ç‚¹ (AIçœ¼é•œ, å…·èº«æ™ºèƒ½)
3. æ’é™¤å¨±ä¹æ˜æ˜Ÿå’Œç¤¾ä¼šæ–°é—»ã€‚
4. å¦‚æœé¡µé¢æ˜¯ RSS XML æ ¼å¼ï¼Œè¯·å¿½ç•¥ XML æ ‡ç­¾ï¼Œåªæå– Title ä¸­çš„æŠ€æœ¯åè¯ã€‚
5. è¿”å›æ ¼å¼ï¼šåªè¿”å›åè¯ï¼Œç”¨è‹±æ–‡é€—å·åˆ†éš”ã€‚å¦‚æœä¸ç¡®å®šæˆ–æ— ç›¸å…³å†…å®¹ï¼Œè¿”å› "NONE"ã€‚
6. ä¼˜å…ˆæå–**çŸ¥åç§‘æŠ€å…¬å¸**ï¼ˆå¦‚æ·±åº¦æ±‚ç´¢ï¼Œæ™ºè°±, å­—èŠ‚, è…¾è®¯ã€é˜¿é‡Œã€OpenAIï¼ŒGoogle ï¼ŒClaude ï¼ŒBing ï¼Œæœˆä¹‹æš—é¢ï¼Œè®¯é£ï¼Œç™¾åº¦ï¼Œå¾®è½¯ï¼Œè‹¹æœï¼Œå°çº¢ä¹¦ï¼‰å‘å¸ƒçš„**æ–°äº§å“åç§°**ï¼ˆå¦‚ AutoGLM, Soraï¼‰ï¼Œé™ä½å¯¹ä¸çŸ¥åå°å·¥å…·çš„æå–æƒé‡ã€‚

ç¤ºä¾‹ï¼š
âŒ é”™è¯¯ï¼šSpring Boot, MySQL, React Hooks
âœ… æ­£ç¡®ï¼šDeepSeek, Cursor, ç§˜å¡”æœç´¢
"""
    
    try:
        @retryable
        def _chat_create():
            return client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ•é”çš„æŠ€æœ¯è¶‹åŠ¿æ•æ‰‹ï¼Œæ“…é•¿ä»æ‚ä¹±çš„ç½‘é¡µå†…å®¹ä¸­æå–æœ‰ä»·å€¼çš„æŠ€æœ¯å…³é”®è¯ï¼Œå¹¶è¿‡æ»¤æ‰æ— å…³çš„å¨±ä¹å…«å¦ã€‚"},
                    {"role": "user", "content": f"ã€{name} çƒ­æ¦œå†…å®¹ã€‘\n{content_truncated}\n\n{prompt}"}
                ],
                temperature=0.2
            )

        response = _chat_create()
        result = response.choices[0].message.content.strip()
        
        # å¤„ç† NONE æƒ…å†µ
        if result.upper() == "NONE" or "NONE" in result.upper():
            log_print(f"      â­ï¸ [{name}] æ— ç›¸å…³æŠ€æœ¯å†…å®¹ï¼Œè·³è¿‡")
            return []
        
        # æ¸…æ´—å¹¶è¿”å›
        keywords = [k.strip() for k in result.split(',') if k.strip() and len(k.strip()) < 30]
        keywords = keywords[:3]  # æ¯ä¸ªæºæœ€å¤š3ä¸ª
        
        if keywords:
            log_print(f"      ğŸ“Œ [{name}] æå–: {keywords}")
        return keywords
        
    except Exception as e:
        log_print(f"      âš ï¸ [{name}] å…³é”®è¯æå–å¤±è´¥: {e}")
        return []


def extract_hot_entities(client: OpenAI, search_results: List[Dict[str, Any]]) -> List[str]:
    """ä»æœç´¢ç»“æœä¸­æå– 2-3 ä¸ªçƒ­é—¨æŠ€æœ¯åè¯"""
    if not search_results: return []
    
    text = "\n".join([f"- {r['title']}" for r in search_results[:10]]) # é™åˆ¶è¾“å…¥é•¿åº¦
    prompt = """
    è¯·ä»ä¸Šè¿°æ–°é—»æ ‡é¢˜ä¸­ï¼Œæå– 2-3 ä¸ªå½“å‰æœ€ç«çš„ AI æŠ€æœ¯æˆ–äº§å“åç§°ã€‚
    è¦æ±‚ï¼š
    1. åªè¿”å›å…·ä½“åè¯ï¼Œå¦‚ "DeepSeek V3", "MCP", "Sora 2.0"ã€‚
    2. ä¸è¦è¿”å›é€šç”¨è¯ï¼ˆå¦‚ "AI", "LLM", "Technology"ï¼‰ã€‚
    3. è¾“å‡ºæ ¼å¼ï¼šç”¨è‹±æ–‡é€—å·åˆ†éš”ï¼Œä¸è¦å…¶ä»–åºŸè¯ã€‚
    """
    try:
        @retryable
        def _chat_create():
            return client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ•é”çš„æŠ€æœ¯è¶‹åŠ¿æ•æ‰‹ã€‚"},
                    {"role": "user", "content": f"ã€æ–°é—»æ ‡é¢˜åˆ—è¡¨ã€‘\n{text}\n\n{prompt}"}
                ],
                temperature=0.1
            )

        response = _chat_create()
        content = response.choices[0].message.content.strip()
        # ç®€å•æ¸…ç†
        entities = [e.strip() for e in content.split(',') if e.strip() and len(e.strip()) < 20]
        return entities[:3]
    except Exception as e:
        log_print(f"      âš ï¸ çƒ­ç‚¹æå–å¤±è´¥: {e}")
        return []

# ================= æ ¸å¿ƒé€»è¾‘ =================

def _robust_json_parse(content: str) -> Any:
    """
    v4.1: é²æ£’ JSON è§£æå™¨
    æ— è®º LLM è¾“å‡ºå¸¦ä¸å¸¦ Markdown ä»£ç å—ï¼Œæˆ–è€… JSON ç¼ºäº†é€—å·å¼•å·ï¼Œéƒ½èƒ½æ­£ç¡®è§£æ
    """
    if not content:
        return []
    
    # 1. å°è¯•ç›´æ¥è§£æï¼ˆæœ€ä¼˜æƒ…å†µï¼šå¹²å‡€çš„ JSONï¼‰
    try:
        return json.loads(content)
    except json.JSONDecodeError:
        pass
    
    # 2. ç”¨æ­£åˆ™æå–ç¬¬ä¸€ä¸ª JSON å¯¹è±¡/æ•°ç»„
    json_pattern = r'(\{.*\}|\[.*\])'
    match = re.search(json_pattern, content, re.DOTALL)
    
    if match:
        raw_json = match.group(1)
        try:
            # 3. ä½¿ç”¨ json_repair ä¿®å¤å¹¶è§£æ
            repaired = repair_json(raw_json, return_objects=True)
            log_print(f"      ğŸ”§ JSON å·²è‡ªåŠ¨ä¿®å¤")
            return repaired
        except Exception as e:
            log_print(f"      âš ï¸ JSON ä¿®å¤å¤±è´¥: {e}")
    
    # 4. ç»ˆæå›é€€ï¼šæ•´ä½“ä¿®å¤
    try:
        repaired = repair_json(content, return_objects=True)
        return repaired
    except Exception as e:
        log_print(f"      âŒ JSON è§£æå½»åº•å¤±è´¥: {e}")
        return []


def get_plan_prompt(history_text: str = "", directed_topic: Optional[str] = None) -> str:
    """åŠ¨æ€ç”Ÿæˆè§„åˆ’æç¤ºè¯ï¼Œæ³¨å…¥å½“å‰æ—¥æœŸã€å†å²è®°å½•å’Œç”¨æˆ·æ„å›¾"""
    today = datetime.now().strftime('%Y-%m-%d')
    
    intent_instruction = ""
    if directed_topic:
        intent_instruction = f"""
    ğŸ‘¤ **ç”¨æˆ·æ ¸å¿ƒæŒ‡ä»¤**ï¼š
    ç”¨æˆ·æŒ‡å®šäº†ä¸»é¢˜ã€{directed_topic}ã€‘ã€‚
    1. ä½ ç”Ÿæˆçš„ 3 ä¸ªé€‰é¢˜ä¸­ï¼Œ**å¿…é¡»åŒ…å«**è‡³å°‘ 1 ä¸ªä¸ã€{directed_topic}ã€‘æ·±åº¦ç›¸å…³çš„é€‰é¢˜ï¼ˆä½œä¸º A æ–¹æ¡ˆï¼‰ã€‚
    2. åŒæ—¶ï¼Œè¯·ä»æƒ…æŠ¥æ± ä¸­æŒ–æ˜å¦å¤– 1-2 ä¸ª**é«˜æ½œè´¨**çš„éšæœºçƒ­ç‚¹æˆ–å…³è”è¯é¢˜ï¼ˆä½œä¸º Plan B/Cï¼‰ï¼Œä¸ç”¨æˆ·æŒ‡å®šä¸»é¢˜è¿›è¡Œ"ä»·å€¼PK"ã€‚
    3. å¦‚æœå‘ç°ã€{directed_topic}ã€‘ç›®å‰æ¯«æ— æ–°æ„ï¼ˆæ— æ–°é—»ã€æ— ç—›ç‚¹ï¼‰ï¼Œä½ å¯ä»¥"æŠ—æ—¨"ï¼Œå…¨æ¨å…¶ä»–æ›´æœ‰ä»·å€¼çš„çƒ­ç‚¹ï¼Œä½†å¿…é¡»åœ¨åˆ†æä¸­è¯´æ˜ç†ç”±ã€‚
    """
    
    return f"""
    ğŸ“… ä»Šå¤©æ˜¯ {today}ã€‚ä½ å¿…é¡»åªå…³æ³¨æœ€è¿‘ 3-7 å¤©å†…å‘ç”Ÿçš„ AI åœˆæœ€æ–°å¤§äº‹ä»¶ã€‚
    â— ç»å¯¹ç¦æ­¢æŠ¥é“ 2024 å¹´æˆ–æ›´æ—©çš„æ—§é—»ï¼ˆå¦‚ DeepSeek R1ã€GPT-4 å‘å¸ƒç­‰å†å²äº‹ä»¶ï¼‰ã€‚
    {intent_instruction}

    ã€å†å²å‘æ–‡è®°å½• (æœ€è¿‘7å¤©)ã€‘
{history_text}
âš ï¸ æŸ¥é‡æŒ‡ä»¤ï¼šå¦‚æœä¸Šè¿°å†å²è®°å½•ä¸­å·²å­˜åœ¨ç›¸ä¼¼é€‰é¢˜ï¼Œè¯·å¿…é¡»è°ƒæ•´åˆ‡å…¥è§’åº¦ï¼ˆä¾‹å¦‚ï¼šä»"æ–°é—»æŠ¥é“"è½¬å‘"æ·±åº¦å®æµ‹"æˆ–"é¿å‘æŒ‡å—"ï¼‰ã€‚å¦‚æœæ— æ³•å·®å¼‚åŒ–ï¼Œè¯·ç›´æ¥ä¸¢å¼ƒè¯¥é€‰é¢˜ã€‚

ä½ æ˜¯â€œç‹å¾€AIâ€çš„é¦–å¸­å†…å®¹ç­–ç•¥å®˜ã€‚
è¯·åŸºäºã€å…¨ç½‘æƒ…æŠ¥ã€‘å’Œã€å¿ƒç†å­¦ç­–ç•¥ã€‘ï¼ŒæŒ–æ˜ 3 ä¸ªæœ€å…·â€œçˆ†æ¬¾æ½œè´¨â€çš„é€‰é¢˜æ–¹å‘ã€‚

## ä»·å€¼å…¬å¼
**é€‰é¢˜ä»·å€¼** = (ä¿¡æ¯å·® Ã— è®¤çŸ¥å†²å‡») + (ç—›ç‚¹å¼ºåº¦ Ã— è§£å†³æ•ˆç‡) - é˜…è¯»é—¨æ§›

## å¿ƒç†å­¦ä¸‰è·¯ç­–ç•¥ï¼ˆå¿…é¡»è¦†ç›–è‡³å°‘2è·¯ï¼Œä¿è¯å¤šæ ·æ€§ï¼‰
1. **Aè·¯ - é”šç‚¹æ•ˆåº” (å€ŸåŠ¿é¡¶æµ)**ï¼šå€ŸåŠ© DeepSeek/Cursor/Gemini ç­‰é¡¶æµäº§å“çš„çŸ¥ååº¦ï¼Œå…³æ³¨å…¶"éšè—åŠŸèƒ½"æˆ–"æœ€æ–°ç©æ³•"ã€‚ç”¨æˆ·çœ‹åˆ°ç†Ÿæ‚‰çš„åå­—æ›´å®¹æ˜“ç‚¹å‡»ã€‚
2. **Bè·¯ - å³æ—¶æ»¡è¶³ (æ•ˆèƒ½ç¥å™¨)**ï¼šå¯»æ‰¾çœŸæ­£çš„"æ•ˆç‡ç¥å™¨"ï¼Œä¸»æ‰“"3åˆ†é’Ÿä¸Šæ‰‹"ã€"ä¸‹ç­æ—©èµ°1å°æ—¶"ã€‚è®©ç”¨æˆ·è§‰å¾—"çœ‹å®Œå°±èƒ½ç”¨"ã€‚
3. **Cè·¯ - æŸå¤±åŒæ¶ (é¿å‘/è®¤çŸ¥)**ï¼š
   - é¿å‘ç±»ï¼šå¯»æ‰¾"æ™ºå•†ç¨"ã€"ç¿»è½¦ç°åœº"ã€"å¹³æ›¿"ï¼Œè§¦å‘ç”¨æˆ·å®³æ€•è¸©å‘çš„å¿ƒç†ã€‚
   - è®¤çŸ¥ç±»ï¼šè§£è¯»æ–°è¶‹åŠ¿ã€æ–°ç¡¬ä»¶ï¼ˆå¦‚ AI è€³æœºã€æ‰‹æœºæ™ºèƒ½ä½“ï¼‰ï¼Œè®©ç”¨æˆ·å®³æ€•"è½åäºæ—¶ä»£"ã€‚

è¾“å…¥æ•°æ®ï¼š
- é•¿æœŸå…³æ³¨å“ç±»åŠ¨æ€
- æœ¬å‘¨çƒ­é—¨å·¥å…·/æ•™ç¨‹
- ç”¨æˆ·åæ§½ä¸ç—›ç‚¹
- å¤§å‚æ–°å‘å¸ƒåŠ¨æ€

å†³ç­–æ ‡å‡†ï¼š
- âœ… **ä¿ç•™**ï¼šDeepSeek éšè—ç©æ³•ï¼ˆé”šç‚¹ï¼‰ã€å…è´¹ç”»æ¶æ„å›¾ï¼ˆå³æ—¶æ»¡è¶³ï¼‰ã€Cursor æ”¶è´¹é¿å‘ï¼ˆæŸå¤±åŒæ¶ï¼‰ã€Google AI è€³æœºä½“éªŒï¼ˆè®¤çŸ¥å‡çº§ï¼‰ã€‚
- âŒ **å‰”é™¤**ï¼šçº¯æ¯ç‡¥çš„èèµ„æ–°é—»ã€è¿‡äºå­¦æœ¯çš„è®ºæ–‡è§£è¯»ã€æ¯«æ— æ–°æ„çš„"æ­£ç¡®çš„åºŸè¯"ã€å†·é—¨æ— åå°å·¥å…·ã€‚

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼ JSONï¼‰ï¼š
[
    {{
        "event": "é€‰é¢˜æ ¸å¿ƒè¯ (å¦‚: DeepSeek)",
        "angle": "åˆ‡å…¥è§’åº¦ (å¦‚: éšè—ç©æ³• / é¿å‘æŒ‡å— / æ·±åº¦è¯„æµ‹)",
        "news_query": "åŠŸèƒ½æ€§æœç´¢è¯ (å¦‚: DeepSeek V3 file upload)",
        "social_query": "æƒ…ç»ªæ€§æœç´¢è¯ (å¦‚: DeepSeek æŠ¥é”™ / DeepSeek ä¸å¥½ç”¨)"
    }},
    ...
]
"""

# ä¿ç•™å†å²å…¼å®¹æ€§
PLAN_PROMPT = get_plan_prompt()

def step1_broad_scan_and_plan(
    client: OpenAI,
    search_tool: "WebSearchTool",
    directed_topic: Optional[str] = None
) -> List[Dict[str, str]]:
    """
    Step 1: å¹¿åŸŸä»·å€¼æ‰«æ (å¿ƒç†å­¦ä¸‰è·¯ç­–ç•¥ + å…¨ç½‘é›·è¾¾)
    æ··åˆæ¨¡å¼ï¼šå¦‚æœä¼ å…¥ directed_topicï¼Œå°†å…¶ä½œä¸º A è·¯æ ¸å¿ƒï¼ŒåŒæ—¶ä¿ç•™ B/C è·¯éšæœºæ¢ç´¢
    """
    log_print(f"\nğŸ“¡ [Step 1] å¹¿åŸŸä»·å€¼æ‰«æ (ç­–ç•¥: {CURRENT_CONFIG['name']})...")
    if directed_topic:
        log_print(f"   ğŸ¯ [æ··åˆæ¨¡å¼] æ ¸å¿ƒä¸»é¢˜: ã€Œ{directed_topic}ã€ + å…¨ç½‘éšæœºæ‰«æ")
    
    pre_scan_results: List[Dict[str, Any]] = []
    
    # === Phase 0: å…¨ç½‘é›·è¾¾ (Global Radar) ===
    # ç ´é™¤ä¿¡æ¯èŒ§æˆ¿ï¼Œä¸»åŠ¨å—…æ¢ä¸åœ¨ WATCHLIST é‡Œçš„æ–°é»‘é©¬
    log_print(f"   ğŸŒ‘ [Phase 0] å…¨ç½‘é›·è¾¾æ‰«æ (å‘ç°æ–°ç‰©ç§)...")
    for q in RADAR_QUERIES:
        res = search_tool.search(q, max_results=2, topic="news", days=1) # åªçœ‹24å°æ—¶å†…
        pre_scan_results.extend(res)

    # === Phase 0.5: çƒ­ç‚¹æå– ===
    hot_entities = extract_hot_entities(client, pre_scan_results)
    if hot_entities:
        log_print(f"   ğŸ”¥ [é›·è¾¾é”å®š] çªå‘çƒ­ç‚¹: {hot_entities}")

    # === Phase 0.6: çƒ­æ¦œåŠ¨æ€è¶‹åŠ¿ ===
    fresh_keywords = []
    try:
        fresh_keywords = fetch_dynamic_trends(client, search_tool)
    except Exception as e:
        log_print(f"      âš ï¸ çƒ­æ¦œæŠ“å–å¼‚å¸¸ï¼Œè·³è¿‡: {e}")
    
    # === Aè·¯: é¡¶æµé”šç‚¹ (Watchlist + Hotspots + Fresh) ===
    if directed_topic:
        # å®šå‘æ¨¡å¼ï¼šæ ¸å¿ƒæ˜¯ directed_topicï¼Œä½†ä¹Ÿæ¥çº³çªå‘çƒ­ç‚¹
        targets = [directed_topic]
        # é€‚å½“åŠ å…¥çƒ­ç‚¹ï¼ˆå¦‚æœæœ‰é‡å¤§çªå‘ï¼‰ï¼Œä½†ä¹Ÿå¯èƒ½è¢« LLM è¿‡æ»¤
        for h in hot_entities:
            if h.lower() not in directed_topic.lower():
                targets.append(h)
        targets = targets[:4] # ä¿æŒèšç„¦
    else:
        # éšæœºæ¨¡å¼
        targets = random.sample(WATCHLIST, 3)
        # å°†çƒ­æ¦œå…³é”®è¯åŠ å…¥ targets (æœ€é«˜ä¼˜å…ˆçº§)
        for fk in fresh_keywords:
            if not any(fk.lower() in t.lower() for t in targets):
                targets.insert(0, fk)
        # å°†çƒ­ç‚¹åŠ å…¥ targets (ä¼˜å…ˆä¾¦å¯Ÿ)
        for h in hot_entities:
            if not any(h.lower() in t.lower() for t in targets):
                targets.insert(0, h)
        targets = targets[:6]

    log_print(f"   ğŸ¯ [Aè·¯-é”šç‚¹] æ‰«æç›®æ ‡: {targets}")
    for t in targets:
        # æ¿€æ´»åƒµå°¸å…³é”®è¯ï¼šåŒæ—¶æœ"éšè—åŠŸèƒ½"å’Œ"æœ€æ–°æ›´æ–°"
        queries = [
            f"{t} éšè—åŠŸèƒ½ ç©æ³• æ•™ç¨‹ 2025",
            f"{t} new features latest update" # è‹±æ–‡æœæ›´æ–°å¾€å¾€æ›´å‡†
        ]
        for q in queries:
            res = search_tool.search(q, max_results=1, topic="news", days=3)
            pre_scan_results.extend(res)
        
    # === Bè·¯: éšæœºæ”¶ç›Šåœºæ™¯ (Life Hack) ===
    log_print(f"   âš¡ [Bè·¯-æ”¶ç›Š] æ‰«ææ•ˆç‡ç¥å™¨...")
    selected_efficiency = random.sample(EFFICIENCY_KEYWORDS, 3)
    if directed_topic:
        # æ··åˆæ¨¡å¼ï¼šåŠ å…¥å®šå‘ä¸»é¢˜çš„æ•ˆç‡åœºæ™¯
        selected_efficiency.insert(0, f"{directed_topic} æ•ˆç‡ç¥å™¨")
        
    log_print(f"      ğŸ² éšæœºæŠ½å–: {selected_efficiency}")
    for kw in selected_efficiency:
        # Bè·¯: å¼ºåˆ¶è¿½åŠ é«˜è´¨é‡ä¿¡æºï¼Œè¿‡æ»¤ SEO åƒåœ¾
        q = f"{kw} æ¨è site:sspai.com OR site:36kr.com OR site:v2ex.com OR site:zhihu.com"
        res = search_tool.search(q, max_results=2, days=3)
        pre_scan_results.extend(res)
        
    # === Cè·¯: éšæœºé¿å‘åœºæ™¯ (Pain Points) ===
    log_print(f"   ğŸ›¡ï¸ [Cè·¯-æŸå¤±] æ‰«æé¿å‘/åæ§½...")
    selected_pain = random.sample(PAIN_KEYWORDS, 3)
    if directed_topic:
        # æ··åˆæ¨¡å¼ï¼šåŠ å…¥å®šå‘ä¸»é¢˜çš„é¿å‘åœºæ™¯
        selected_pain.insert(0, f"{directed_topic} é¿å‘ åæ§½")
        
    log_print(f"      ğŸ² éšæœºæŠ½å–: {selected_pain}")
    for kw in selected_pain:
        # Cè·¯: å¼ºåˆ¶è¿½åŠ ç¤¾åŒºä¿¡æº
        q = f"{kw} åæ§½ é¿å‘ site:v2ex.com OR site:reddit.com OR site:zhihu.com"
        res = search_tool.search(q, max_results=2, days=3)
        pre_scan_results.extend(res)
    
    pre_scan_text = "\n".join([f"- {r['title']}: {r['body'][:80]}" for r in pre_scan_results])
    
    # 2. æ™ºèƒ½ç­›é€‰ä¸è§„åˆ’
    log_print(f"   ğŸ“ æƒ…æŠ¥èšåˆå®Œæ¯•ï¼ŒDeepSeek æ­£åœ¨åº”ç”¨å¿ƒç†å­¦ç­–ç•¥é€‰é¢˜...")
    
    # åŠ è½½å†å²è®°å½•
    history = load_history()
    history_text = "\n".join([f"- {h['date']}: {h['topic']} ({h['angle']})" for h in history])
    if not history_text: history_text = "æ— ï¼ˆè¿™æ˜¯ç¬¬ä¸€ç¯‡ï¼‰"

    try:
        @retryable
        def _chat_create():
            return client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": get_plan_prompt(history_text, directed_topic)},
                    {"role": "user", "content": f"ã€æ··åˆæƒ…æŠ¥æ± ã€‘\n{pre_scan_text}"}
                ],
                temperature=0.7,
                response_format={ "type": "json_object" }
            )

        response = _chat_create()
        content = response.choices[0].message.content
        
        # v4.1: ä½¿ç”¨ json_repair å¢å¼ºé²æ£’æ€§
        search_plan = _robust_json_parse(content)
        if isinstance(search_plan, dict) and "events" in search_plan:
            search_plan = search_plan["events"]
            
        log_print(f"   ğŸ§  é€‰é¢˜æ–¹å‘å·²é”å®š: {[i['event'] + '-' + i['angle'] for i in search_plan]}\n")
        return search_plan
    except Exception as e:
        log_print(f"   âŒ è§„åˆ’å¤±è´¥: {e}")
        return [{"event": "DeepSeek", "angle": "é¿å‘", "news_query": "DeepSeek V3", "social_query": "DeepSeek å¹»è§‰"}]

def _clean_text(text: Optional[str], max_len: int = 100) -> str:
    """æ¸…æ´—æ–‡æœ¬ï¼šç§»é™¤å¤šä½™ç©ºç™½ã€HTMLæ ‡ç­¾ã€æˆªæ–­é•¿åº¦"""
    if not text:
        return ""
    # ç§»é™¤å¤šä½™ç©ºç™½å’Œæ¢è¡Œ
    import re
    text = re.sub(r'\s+', ' ', text).strip()
    # ç§»é™¤å¸¸è§ HTML æ ‡ç­¾æ®‹ç•™
    text = re.sub(r'<[^>]+>', '', text)
    # æˆªæ–­å¹¶æ·»åŠ çœç•¥å·
    if len(text) > max_len:
        text = text[:max_len] + "..."
    return text

def step2_deep_scan(
    search_plan: List[Dict[str, str]],
    search_tool: "WebSearchTool",
    directed_topic: Optional[str] = None
) -> str:
    """
    Step 2: æ·±åº¦éªŒè¯ (é‡ç¤¾äº¤/ç—›ç‚¹)
    è¾“å‡ºæ ¼å¼ï¼šæ¸…æ™°çš„ Markdown åˆ—è¡¨ï¼ŒåŒ…å«æ‘˜è¦å’Œæ¥æº URL
    """
    log_print("ğŸ“¡ [Step 2] å¯åŠ¨æ·±åº¦ä»·å€¼éªŒè¯...\n")
    all_results = []
    
    w_news = CURRENT_CONFIG['weights']['news']
    w_social = CURRENT_CONFIG['weights']['social']
    
    for item in search_plan:
        event = item.get("event", "æœªçŸ¥")
        angle = item.get("angle", "é€šç”¨")
        news_q = item.get("news_query", "")
        social_q = item.get("social_query", "")

        is_core = False
        if directed_topic and event:
            dt = str(directed_topic).lower()
            ev = str(event).lower()
            is_core = (dt in ev) or (ev in dt)

        # é˜²å¹²æ‰°ï¼šå®šå‘æ¨¡å¼ä¸‹ï¼ŒæŠŠæ›´å¤šæ£€ç´¢é¢åº¦ç•™ç»™æ ¸å¿ƒä¸»é¢˜ï¼›éæ ¸å¿ƒä¸»é¢˜é™é…é¢
        social_max_results = 4
        news_max_results = 2
        if directed_topic:
            social_max_results = 4 if is_core else 2
            news_max_results = 2 if is_core else 1
        
        log_print(f"   ğŸ” æ­£åœ¨æ·±æŒ–: ã€{event}ã€‘ ({angle}æ–¹å‘)")
        event_data = [f"### ğŸ¯ é€‰é¢˜: {event} ({angle})"]
        
        # 1. ç¤¾äº¤/ç—›ç‚¹æœç´¢ (æ ¸å¿ƒ)
        if social_q:
            log_print(f"      ğŸ’¬ ç¤¾äº¤èˆ†æƒ… (æƒé‡ {w_social}): {social_q}")
            full_social_q = f"{social_q} site:mp.weixin.qq.com OR site:xiaohongshu.com OR site:zhihu.com OR site:bilibili.com"
            res = search_tool.search(full_social_q, max_results=social_max_results)
            if res:
                event_data.append(f"\n**ğŸ’¬ ç”¨æˆ·åé¦ˆ** ({social_q})")
                for r in res:
                    title = _clean_text(r.get('title', 'æ— æ ‡é¢˜'), 50)
                    body = _clean_text(r.get('body', ''), 100)
                    url = r.get('url', '')
                    if url:
                        event_data.append(f"- **{title}**: {body} [[æ¥æº]({url})]")
                    else:
                        event_data.append(f"- **{title}**: {body}")
                
        # 2. å®˜æ–¹éªŒè¯ (è¾…åŠ©)
        if news_q:
            log_print(f"      ğŸ”¥ å®˜æ–¹éªŒè¯ (æƒé‡ {w_news}): {news_q}")
            res = search_tool.search(news_q, max_results=news_max_results)
            if res:
                event_data.append(f"\n**ğŸ“° å®˜æ–¹ä¿¡æ¯** ({news_q})")
                for r in res:
                    title = _clean_text(r.get('title', 'æ— æ ‡é¢˜'), 60)
                    url = r.get('url', '')
                    if url:
                        event_data.append(f"- {title} [[æ¥æº]({url})]")
                    else:
                        event_data.append(f"- {title}")
        
        all_results.append("\n".join(event_data))
        log_print("")
        time.sleep(1)

    # GitHub è¡¥å…… (Weekly)
    log_print(f"   ğŸ’» GitHub Weekly Trending...")
    github_res = get_github_trending()
    all_results.append("### ğŸ’» GitHub Weekly Trending\n" + "\n".join(github_res))
    
    return "\n\n---\n\n".join(all_results)

def step3_final_decision(
    scan_data: str,
    client: OpenAI,
    history_text: str = "æ— ï¼ˆè¿™æ˜¯ç¬¬ä¸€ç¯‡ï¼‰",
    directed_topic: Optional[str] = None
) -> str:
    """Step 3: å†³ç­–ï¼ˆå¸¦å»é‡å’Œæ–°è¯æ‰¶æŒ + ç”¨æˆ·æ„å›¾åŠ æƒï¼‰"""
    log_print("\n" + "="*50 + "\nğŸ“ DeepSeek ä¸»ç¼–å®¡æ ¸ä¸­...\n" + "="*50)
    
    # æ„é€ ç”¨æˆ·æ„å›¾æç¤º
    user_intent_prompt = ""
    if directed_topic:
        user_intent_prompt = f"""
    ğŸ‘¤ **ç”¨æˆ·æ„å›¾ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰**ï¼š
    ç”¨æˆ·æ˜ç¡®å¸Œæœ›å†™å…³äºã€{directed_topic}ã€‘çš„å†…å®¹ã€‚
    **å†³ç­–åŸåˆ™**ï¼š
    1. é»˜è®¤ä¼˜å…ˆï¼šåœ¨åŒç­‰ä»·å€¼ä¸‹ï¼Œä¼˜å…ˆé€‰æ‹©ä¸ã€{directed_topic}ã€‘ç›¸å…³çš„é€‰é¢˜ã€‚
    2. å…è®¸æŠ—æ—¨ï¼šåªæœ‰å½“æ‰«æåˆ°çš„å…¶ä»–çƒ­ç‚¹ï¼ˆå¦‚çªå‘é‡å¤§æŠ€æœ¯æ›´æ–°ï¼‰å…·æœ‰**æé«˜çš„çˆ†æ¬¾æ½œè´¨**æ—¶ï¼Œä½ æ‰å»ºè®®æ”¾å¼ƒç”¨æˆ·æŒ‡å®šä¸»é¢˜ã€‚
    3. æ··åˆç­–ç•¥ï¼šå¦‚æœå¯èƒ½ï¼Œå°è¯•å°†ã€{directed_topic}ã€‘ä¸å…¶ä»–çƒ­ç‚¹ç»“åˆï¼ˆä¾‹å¦‚ "ç”¨ {directed_topic} è§£å†³è¿™ä¸ªæ–°çƒ­ç‚¹é—®é¢˜"ï¼‰ã€‚
    """

    prompt = f"""
    {EDITOR_PROMPT}
    {user_intent_prompt}
    
    âŒ **ä¸¥æ ¼å»é‡**ï¼šä»¥ä¸‹æ˜¯æœ€è¿‘å·²å†™è¿‡çš„é€‰é¢˜ï¼š
    {history_text}
    
    **ç»å¯¹ç¦æ­¢**å†æ¬¡é€‰æ‹©ä¸ä¸Šè¿°æå…¶ç›¸ä¼¼çš„é€‰é¢˜ï¼å¿…é¡»æ¢ä¸ªå·¥å…·æˆ–æ¢ä¸ªè§’åº¦ï¼
    
    âœ¨ **æ‰¶æŒæ–°è¯**ï¼šè¯·ä¼˜å…ˆå…³æ³¨æƒ…æŠ¥ä¸­æåˆ°çš„ã€ç”Ÿåƒ»æŠ€æœ¯åè¯ã€‘ï¼ˆå¦‚ AutoGLM, Dayflow ç­‰ï¼‰ï¼Œå¦‚æœå®ƒä»¬æœ‰ä»·å€¼ï¼Œä¼˜å…ˆå…¥é€‰ã€‚
    
    å½“å‰ç­–ç•¥ï¼šã€{CURRENT_CONFIG['name']}ã€‘
    {CURRENT_CONFIG['prompt_suffix']}
    """
    
    try:
        # å•æ¬¡æ‰«æç”¨ chat æ¨¡å‹ï¼ˆå¿«ã€ä¾¿å®œï¼‰ï¼Œç»¼åˆå†³ç­–æ‰ç”¨ reasoner
        @retryable
        def _chat_create():
            return client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": f"ã€æ·±åº¦éªŒè¯æƒ…æŠ¥ã€‘\n{scan_data}"}
                ],
                stream=True
            )

        response = _chat_create()

        log_print("\n" + "="*20 + " é€‰é¢˜æŠ¥å‘Š " + "="*20 + "\n")
        collected = []
        for chunk in response:
            if chunk.choices[0].delta.content:
                c = chunk.choices[0].delta.content
                log_print(c, end="", flush=True)
                collected.append(c)
        return "".join(collected)
    except Exception as e:
        log_print(f"âŒ å†³ç­–å¤±è´¥: {e}")
        return f"å¤±è´¥: {e}"

EDITOR_PROMPT = """
ä½ å«"ç‹å¾€AI"ï¼Œä¸“æ³¨ AI å·¥ä½œæµçš„ç¡¬æ ¸åšä¸»ã€‚
è¯·ç­›é€‰ 3 ä¸ªã€ä»·å€¼æœ€é«˜ã€‘çš„é€‰é¢˜ï¼Œ**å¿…é¡»è¦†ç›–è‡³å°‘ 2 ç§å¿ƒç†ç­–ç•¥**ä»¥ä¿è¯å¤šæ ·æ€§ã€‚

## ä»·å€¼å…¬å¼
**é€‰é¢˜ä»·å€¼** = (ä¿¡æ¯å·® Ã— è®¤çŸ¥å†²å‡») + (ç—›ç‚¹å¼ºåº¦ Ã— è§£å†³æ•ˆç‡) - é˜…è¯»é—¨æ§›

## å¿ƒç†å­¦ç­–ç•¥ï¼ˆ3 ä¸ªé€‰é¢˜å¿…é¡»è¦†ç›–è‡³å°‘ 2 è·¯ï¼‰
1. **é”šç‚¹æ•ˆåº” (å€ŸåŠ¿é¡¶æµ)**ï¼šå€ŸåŠ© DeepSeek/Cursor/Gemini ç­‰é¡¶æµäº§å“çš„çŸ¥ååº¦ï¼Œç”¨æˆ·æ›´å®¹æ˜“ç‚¹å‡»ã€‚
2. **å³æ—¶æ»¡è¶³ (æ•ˆèƒ½ç¥å™¨)**ï¼šè®©ç”¨æˆ·è§‰å¾—"çœ‹å®Œå°±èƒ½ç”¨"ï¼Œè·å¾—æ­£åé¦ˆã€‚å¦‚"3åˆ†é’Ÿå­¦ä¼š"ã€"å…è´¹ç™½å«–"ã€‚
3. **æŸå¤±åŒæ¶ (é¿å‘/è®¤çŸ¥)**ï¼šè§¦å‘ç”¨æˆ·"å®³æ€•è¸©å‘"æˆ–"å®³æ€•è½å"çš„å¿ƒç†ã€‚å¦‚"ç¿»è½¦ç°åœº"ã€"æ–°è¶‹åŠ¿è§£è¯»"ã€‚

ğŸ›¡ï¸ **è´¨é‡è¿‡æ»¤çº¢çº¿**ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š
1. **æ‹’ç»ä½è´¨å†…å®¹**ï¼šå‰”é™¤æ¯«æ— æ–°æ„çš„"æ­£ç¡®çš„åºŸè¯"å’Œå†·é—¨æ— åå°å·¥å…·ã€‚
2. **å¤§å‚æ–°åŠ¨ä½œä¼˜å…ˆ**ï¼šGoogleã€OpenAIã€DeepSeekã€Anthropic ç­‰å¤§å‚çš„æ–°å‘å¸ƒã€æ–°åŠŸèƒ½ä¼˜å…ˆçº§æœ€é«˜ã€‚
3. **å‰æ²¿è¶‹åŠ¿ä¼˜å…ˆ**ï¼šæ–°çš„ Agent ç©æ³•ã€æ–°çš„å¼€æºé»‘é©¬é¡¹ç›®ã€æ–°çš„ç¡¬ä»¶ä½“éªŒï¼ˆå¦‚ AI è€³æœº/æ‰‹æœºï¼‰å€¼å¾—å…³æ³¨ã€‚

å†³ç­–é€»è¾‘ï¼š
1. **æ—¢è¦å®æ“ä¹Ÿè¦è®¤çŸ¥**ï¼šä¸è¦åªç›¯ç€"çœæ—¶é—´"çš„å°å·¥å…·ã€‚å¦‚æœæœ‰ä¸€ä¸ªæ–°çš„æŠ€æœ¯è¶‹åŠ¿ï¼Œå³ä½¿æš‚æ—¶ä¸èƒ½ä¸‹è½½ï¼Œåªè¦èƒ½å¸¦æ¥"è®¤çŸ¥éœ‡æ’¼"ï¼Œä¹Ÿæ˜¯å¥½é€‰é¢˜ã€‚
2. **æ‹’ç»è¿‡åº¦è¥é”€**ï¼šå‰”é™¤é‚£äº›åªæœ‰è¥é”€å™±å¤´æ²¡æœ‰å®è´¨å†…å®¹çš„å·¥å…·ã€‚
3. **å…³è”çƒ­ç‚¹**ï¼šå¦‚æœæ¶‰åŠ WATCHLIST ä¸­çš„äº§å“ï¼ŒåŠ åˆ†ã€‚

è¾“å‡ºæ ¼å¼ï¼š
### é€‰é¢˜ 1ï¼š[æ ‡é¢˜] (éœ€æå…·å¸å¼•åŠ›)
* **å¿ƒç†é”šç‚¹**ï¼š[é”šç‚¹æ•ˆåº” / å³æ—¶æ»¡è¶³ / æŸå¤±åŒæ¶]
* **æ ¸å¿ƒä»·å€¼**ï¼š[ç”¨æˆ·çœ‹å®Œèƒ½å¾—åˆ°ä»€ä¹ˆï¼Ÿæ–°çŸ¥ï¼ŸæŠ€èƒ½ï¼Ÿé¿å‘ï¼Ÿ]
* **çƒ­åº¦è¯„çº§**ï¼š[â­â­â­â­â­]
* **æ¨èç†ç”±**ï¼š[ä¸ºä»€ä¹ˆè¿™ä¸ªé€‰é¢˜ç°åœ¨å€¼å¾—å†™ï¼Ÿ]
---
## ä»Šæ—¥ä¸»æ¨
å‘Šè¯‰æˆ‘ä¸å†™ä¼šåæ‚”çš„é‚£ä¸ª (ä»·å€¼æœ€é«˜çš„)ï¼Œå¹¶è¯´æ˜å®ƒå‘½ä¸­äº†å“ªä¸ªå¿ƒç†é”šç‚¹ã€‚
"""

def auto_init_workflow() -> None:
    """è‡ªåŠ¨åˆå§‹åŒ–åç»­å·¥ä½œæµæ–‡ä»¶å¤¹å’Œæ–‡ä»¶"""
    log_print("\nâš™ï¸ æ­£åœ¨åˆå§‹åŒ–åç»­å·¥ä½œæµ...")
    
    # 1. é¢„åˆ›å»ºæ‰€æœ‰é˜¶æ®µæ–‡ä»¶å¤¹
    from config import get_stage_dir, get_research_notes_file
    stages = ["research", "drafts", "publish", "assets"]
    for stage in stages:
        path = get_stage_dir(stage)
        log_print(f"   ğŸ“‚ ç›®å½•å°±ç»ª: {path}")
        
    # 2. åˆ›å»ºç©ºç™½ç ”ç©¶ç¬”è®°
    notes_file = get_research_notes_file()
    if not os.path.exists(notes_file):
        with open(notes_file, "w", encoding="utf-8") as f:
            f.write("# ç ”ç©¶ç¬”è®°\n\nè¯´æ˜ï¼šæ­¤æ–‡ä»¶é€šå¸¸ç”± `python run.py research` è‡ªåŠ¨ç”Ÿæˆã€‚\nå¦‚éœ€äººå·¥è¡¥å……ï¼Œè¯·åœ¨æ­¤å¤„è¿½åŠ ä½ çš„å…³é”®å‘ç°ä¸å¼•ç”¨é“¾æ¥ã€‚\n")
        log_print(f"   ğŸ“„ ç¬”è®°æ–‡ä»¶å·²åˆ›å»º: {notes_file}")
    
    # 3. æç¤ºä¸‹ä¸€æ­¥
    log_print("\nğŸ’¡ ä¸‹ä¸€æ­¥ï¼š")
    log_print("   - å¯ç»§ç»­è¿è¡Œ hunt è·å–æ›´å¤šé€‰é¢˜")
    log_print("   - æˆ–è¿è¡Œ `python run.py final` ç»¼åˆæ‰€æœ‰æŠ¥å‘Šï¼Œè·å¾— 3 ä¸ªæç¤ºè¯")

def save_report(raw_data: str, analysis: str, directed_topic: Optional[str] = None) -> None:
    filename = get_topic_report_file()
    mode_info = f"å®šå‘æœç´¢: {directed_topic}" if directed_topic else CURRENT_CONFIG['name']
    content = f"# ğŸš€ é€‰é¢˜é›·è¾¾æŠ¥å‘Š v4.0 ({mode_info})\n\n**æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n**ç­–ç•¥**: {CURRENT_CONFIG['strategy']}\n\n## æ·±åº¦éªŒè¯æƒ…æŠ¥\n\n{raw_data}\n\n---\n\n## é€‰é¢˜åˆ†æ\n\n{analysis}"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(content)
    log_print(f"\n\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜: {filename}")
    
    # ä¿å­˜åè‡ªåŠ¨åˆå§‹åŒ–å·¥ä½œæµ
    auto_init_workflow()

def main(topic=None):
    """
    é€‰é¢˜é›·è¾¾ä¸»å…¥å£
    å‚æ•°:
        topic: å¯é€‰ï¼ŒæŒ‡å®šæœç´¢ä¸»é¢˜ã€‚è‹¥æä¾›ï¼Œå°†å¯ç”¨â€œæ··åˆä¼˜å…ˆçº§â€ï¼šä¸»é¢˜ä¼˜å…ˆï¼Œä½†ä»ä¿ç•™å…¨ç½‘éšæœºæ¢ç´¢ä»¥æ•æ‰çªå‘çƒ­ç‚¹
    """
    mode_text = f"å®šå‘æœç´¢: {topic}" if topic else "å…¨ç½‘é›·è¾¾"
    log_print("\n" + "="*60 + f"\nğŸš€ é€‰é¢˜é›·è¾¾ v4.0 ({mode_text}) - ç‹å¾€AI\n" + "="*60 + "\n")
    
    search_tool = WebSearchTool()
    
    with httpx.Client(proxy=PROXY_URL, timeout=REQUEST_TIMEOUT) as http_client:
        client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL, http_client=http_client)
        
        # åŠ è½½å†å²è®°å½•ç”¨äºå»é‡
        history = load_history()
        history_text = "\n".join([f"- {h['date']}: {h['topic']} ({h['angle']})" for h in history])
        if not history_text: history_text = "æ— ï¼ˆè¿™æ˜¯ç¬¬ä¸€ç¯‡ï¼‰"
        
        # 1. å¹¿åŸŸæ‰«æ / å®šå‘æœç´¢
        search_plan = step1_broad_scan_and_plan(client, search_tool, directed_topic=topic)
        
        # 2. æ·±åº¦éªŒè¯
        raw_data = step2_deep_scan(search_plan, search_tool, directed_topic=topic)
        
        # 3. å†³ç­–ï¼ˆä¼ å…¥å†å²è®°å½•ç”¨äºå»é‡ï¼‰
        analysis = step3_final_decision(raw_data, client, history_text, directed_topic=topic)
        
        # 4. ä¿å­˜
        save_report(raw_data, analysis, directed_topic=topic)
    
    log_print("\nâœ… é€‰é¢˜é›·è¾¾å®Œæˆï¼")

def _extract_topic_frequencies(reports_content: str) -> Dict[str, int]:
    """
    v4.1: ä»å¤šä»½æŠ¥å‘Šä¸­æå–å…³é”®è¯å‡ºç°é¢‘ç‡
    é«˜é¢‘å‡ºç°çš„å…³é”®è¯è¯´æ˜çƒ­åº¦æŒç»­ï¼Œåº”ä¼˜å…ˆè€ƒè™‘
    """
    from collections import Counter
    
    # å®šä¹‰é«˜ä»·å€¼å…³é”®è¯æ¨¡å¼ï¼ˆå¤§å‚äº§å“ã€çƒ­é—¨æ¦‚å¿µï¼‰
    high_value_keywords = [
        # å¤§å‚äº§å“
        "DeepSeek", "Cursor", "Gemini", "Claude", "GPT", "Kimi", "Copilot",
        "Windsurf", "Bolt", "Lovable", "ç§˜å¡”", "è±†åŒ…", "é€šä¹‰", "æ™ºè°±", "AutoGLM",
        # çƒ­é—¨æ¦‚å¿µ
        "Agent", "æ™ºèƒ½ä½“", "MCP", "RAG", "å®æ—¶ç¿»è¯‘", "AI è€³æœº", "æ‰‹æœºåŠ©æ‰‹",
        "æ¶æ„å›¾", "æµç¨‹å›¾", "æ€ç»´å¯¼å›¾", "æ–‡æ¡£åˆ†æ", "ä»£ç ç”Ÿæˆ",
        # æ•ˆç‡åœºæ™¯
        "å…è´¹", "å¹³æ›¿", "ç™½å«–", "é¿å‘", "ç¿»è½¦"
    ]
    
    freq = Counter()
    content_lower = reports_content.lower()
    
    for kw in high_value_keywords:
        count = content_lower.count(kw.lower())
        if count > 0:
            freq[kw] = count
    
    return dict(freq.most_common(10))


def _generate_topic_insights(freq: Dict[str, int], reports_count: int) -> str:
    """
    v4.1: æ ¹æ®é¢‘ç‡ç»Ÿè®¡ç”Ÿæˆé€‰é¢˜æ´å¯Ÿï¼Œæ³¨å…¥åˆ° Prompt ä¸­è¾…åŠ©å†³ç­–
    """
    if not freq:
        return "æš‚æ— é«˜é¢‘å…³é”®è¯ç»Ÿè®¡ã€‚"
    
    insights = []
    insights.append(f"ğŸ“Š **å…³é”®è¯çƒ­åº¦ç»Ÿè®¡** (æ¥è‡ª {reports_count} ä»½æŠ¥å‘Š)ï¼š")
    
    for kw, count in freq.items():
        if count >= 3:
            insights.append(f"   ğŸ”¥ğŸ”¥ğŸ”¥ **{kw}**: å‡ºç° {count} æ¬¡ (æé«˜çƒ­åº¦ï¼Œå¼ºçƒˆæ¨è)")
        elif count >= 2:
            insights.append(f"   ğŸ”¥ğŸ”¥ **{kw}**: å‡ºç° {count} æ¬¡ (é«˜çƒ­åº¦)")
        else:
            insights.append(f"   ğŸ”¥ **{kw}**: å‡ºç° {count} æ¬¡")
    
    return "\n".join(insights)


def final_summary():
    """ç»¼åˆå½“å¤©æ‰€æœ‰æŠ¥å‘Šï¼Œç»™å‡ºæœ€ç»ˆé€‰é¢˜æ¨èå’Œä¸‰ä¸ªæç¤ºè¯"""
    import glob
    from config import get_today_dir
    
    log_print("\n" + "="*60)
    log_print("ğŸ¯ ç»¼åˆé€‰é¢˜å†³ç­– v4.1 - æ•´åˆä»Šæ—¥æ‰€æœ‰æŠ¥å‘Š")
    log_print("="*60 + "\n")
    
    # 1. è¯»å–å½“å¤©æ‰€æœ‰æŠ¥å‘Š
    topics_dir = os.path.join(get_today_dir(), "1_topics")
    reports = glob.glob(os.path.join(topics_dir, "report_*.md"))
    
    if not reports:
        log_print("âŒ ä»Šæ—¥æš‚æ— æŠ¥å‘Šï¼Œè¯·å…ˆè¿è¡Œ `python run.py hunt`")
        return
    
    log_print(f"ğŸ“Š æ‰¾åˆ° {len(reports)} ä»½æŠ¥å‘Šï¼š")
    all_content = []
    for r in sorted(reports):
        log_print(f"   ğŸ“„ {os.path.basename(r)}")
        with open(r, "r", encoding="utf-8") as f:
            all_content.append(f"=== {os.path.basename(r)} ===\n{f.read()}")
    
    combined = "\n\n".join(all_content)
    
    # === v4.1: é¢„å¤„ç† - å…³é”®è¯é¢‘ç‡åˆ†æ ===
    log_print("\nğŸ” [v4.1] æ­£åœ¨åˆ†æå…³é”®è¯çƒ­åº¦...")
    topic_freq = _extract_topic_frequencies(combined)
    topic_insights = _generate_topic_insights(topic_freq, len(reports))
    log_print(topic_insights)
    
    # 2. DeepSeek ç»¼åˆåˆ†æ
    log_print("\nğŸ§  DeepSeek æ­£åœ¨ç»¼åˆåˆ†æ...")
    
    FINAL_PROMPT = f"""
ä½ æ˜¯"ç‹å¾€AI"ï¼Œä¸€ä¸ªæ“…é•¿ä»å¤šä»½æƒ…æŠ¥ä¸­æç‚¼æ ¸å¿ƒé€‰é¢˜çš„å…¬ä¼—å·ä¸»ç¼–ã€‚

ä½ çš„ä»»åŠ¡ï¼šç»¼åˆåˆ†æä»Šå¤©çš„æ‰€æœ‰é€‰é¢˜æŠ¥å‘Šï¼Œé€‰å‡ºã€1ä¸ªæœ€ç»ˆé€‰é¢˜ã€‘ï¼Œå¹¶è¾“å‡º3ä¸ªç»“æ„åŒ–æç¤ºè¯ã€‚

## ğŸ”¥ ç³»ç»Ÿé¢„å¤„ç†ï¼šå…³é”®è¯çƒ­åº¦åˆ†æ
{topic_insights}

âš ï¸ **é‡è¦æŒ‡ä»¤**ï¼šä¸Šè¿°é«˜é¢‘å…³é”®è¯ä»£è¡¨ä»Šæ—¥æŒç»­çƒ­ç‚¹ï¼Œè¯·åœ¨é€‰é¢˜æ—¶**ä¼˜å…ˆè€ƒè™‘**è¿™äº›æ–¹å‘ï¼

## ä»·å€¼å…¬å¼ (å¿ƒç†å­¦é©±åŠ¨)
**é€‰é¢˜ä»·å€¼** = (ä¿¡æ¯å·® Ã— è®¤çŸ¥å†²å‡») + (ç—›ç‚¹å¼ºåº¦ Ã— è§£å†³æ•ˆç‡) - é˜…è¯»é—¨æ§›

å¿ƒç†å­¦ç­–ç•¥ï¼ˆä¸‰é€‰ä¸€ï¼Œä½†å¯æ··æ­ï¼‰ï¼š
1. **é”šç‚¹æ•ˆåº” (å€ŸåŠ¿)**ï¼šå€ŸåŠ©é¡¶æµäº§å“çš„çŸ¥ååº¦ï¼Œç”¨æˆ·æ›´å®¹æ˜“ç‚¹å‡»ï¼ˆå¦‚ "DeepSeek éšè—ç©æ³•"ï¼‰
2. **å³æ—¶æ»¡è¶³ (æ•ˆèƒ½)**ï¼šè®©ç”¨æˆ·è§‰å¾—"çœ‹å®Œå°±èƒ½ç”¨"ï¼Œè·å¾—å³æ—¶æ­£åé¦ˆï¼ˆå¦‚ "3åˆ†é’Ÿå­¦ä¼š"ï¼‰
3. **æŸå¤±åŒæ¶ (é¿å‘)**ï¼šè®©ç”¨æˆ·å®³æ€•é”™è¿‡æˆ–è¸©å‘ï¼Œè§¦å‘ç´§è¿«æ„Ÿï¼ˆå¦‚ "åˆ«å†è¢«å‘äº†"ï¼‰

## é€‰é¢˜æ ‡å‡† (æŒ‰ä¼˜å…ˆçº§)
1. **é«˜é¢‘çƒ­ç‚¹ä¼˜å…ˆ**ï¼šå¤šæ¬¡å‡ºç°åœ¨æŠ¥å‘Šä¸­çš„å…³é”®è¯è¯´æ˜çƒ­åº¦æŒç»­ï¼Œä¼˜å…ˆé€‰æ‹©
2. **å¤§å‚åŠ¨ä½œä¼˜å…ˆ**ï¼šGoogle/OpenAI/DeepSeek ç­‰å¤§å‚çš„æ–°å‘å¸ƒã€æ–°åŠŸèƒ½ä¼˜å…ˆçº§æœ€é«˜
3. **ä»·å€¼å¤šå…ƒ**ï¼š
   - **è®¤çŸ¥ç±»**ï¼šè§£è¯»æ–°è¶‹åŠ¿ã€æ–°ç¡¬ä»¶ï¼ˆå¦‚ AI è€³æœºã€æ‰‹æœºæ™ºèƒ½ä½“ï¼‰ï¼Œæ»¡è¶³æ±‚çŸ¥æ¬²
   - **å®æ“ç±»**ï¼šçœŸæ­£çš„æ•ˆç‡ç¥å™¨ï¼ˆå¦‚ å…è´¹ç”»å›¾ï¼‰ï¼Œæ»¡è¶³å³æ—¶æ»¡è¶³å¿ƒç†
   - **é¿å‘ç±»**ï¼šç¿»è½¦ç°åœºã€æ™ºå•†ç¨æ­ç§˜ï¼Œæ»¡è¶³æŸå¤±åŒæ¶å¿ƒç†
4. **æ‹’ç»å¹³åº¸**ï¼šå‰”é™¤é‚£äº›"çœ‹èµ·æ¥æœ‰ç”¨ä½†å®é™…æ²¡å•¥ç”¨"çš„å·¥å…·

## è¾“å‡ºæ ¼å¼

### ğŸ† ä»Šæ—¥æœ€ç»ˆé€‰é¢˜
**æ ‡é¢˜**ï¼š[çˆ†æ¬¾æ ‡é¢˜ï¼Œ15-25å­—ï¼Œè¿ç”¨å¿ƒç†å­¦æŠ€å·§]
**å¿ƒç†é”šç‚¹**ï¼š[é”šç‚¹æ•ˆåº” / å³æ—¶æ»¡è¶³ / æŸå¤±åŒæ¶ï¼Œé€‰ä¸€ä¸ªä¸»æ‰“]
**ä¸€å¥è¯å–ç‚¹**ï¼š[ç”¨æˆ·çœ‹å®Œèƒ½å¾—åˆ°ä»€ä¹ˆï¼Ÿè®¤çŸ¥å‡çº§ï¼Ÿè§£å†³ç—›ç‚¹ï¼Ÿé¿å¼€é™·é˜±ï¼Ÿ]
**å…³é”®è¯**ï¼š[3-5ä¸ªæœç´¢å…³é”®è¯ï¼Œç”¨äºåç»­ç´ ææœé›†]

### ğŸ“¡ æç¤ºè¯ 1ï¼šFast Research (ç”¨äºè‡ªåŠ¨ç ”ç©¶ / research é˜¶æ®µ)
```
[è¯·ç”¨ä¸­æ–‡ï¼Œå‘Šè¯‰ Researcher éœ€è¦æœç´¢å“ªäº›å…·ä½“å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š
- å®˜æ–¹æ–‡æ¡£/å‘å¸ƒä¼š/Demo
- è¡Œä¸šä¸“å®¶çš„æ·±åº¦è§£è¯»/è¯„æµ‹
- ç”¨æˆ·çš„çœŸå®ä½“éªŒ/åæ§½
- ç«å“å¯¹æ¯”
æ ¼å¼è¦æ±‚ï¼šåˆ†æ¡åˆ—å‡ºï¼Œæ¯æ¡ä¸€ä¸ªæ˜ç¡®çš„æœç´¢ä»»åŠ¡]
```

### ğŸ¨ æç¤ºè¯ 2ï¼šè§†è§‰è„šæœ¬ (ç”¨äºé…å›¾æ–¹æ¡ˆ)
```
[è¯·ç”¨ä¸­æ–‡ï¼Œå»ºè®®éœ€è¦å‡†å¤‡çš„é…å›¾ï¼ŒåŒ…æ‹¬ï¼š
- å…³é”®æˆªå›¾ (å¦‚ï¼šæ–°åŠŸèƒ½ç•Œé¢ã€Demoæ¼”ç¤º)
- å¯¹æ¯”å›¾ (æ–°æ—§å¯¹æ¯”ã€ç«å“å¯¹æ¯”)
- æ¦‚å¿µå›¾ (å¦‚æœæ˜¯æŠ½è±¡æ¦‚å¿µï¼Œå¦‚ä½•å¯è§†åŒ–)
- å°é¢å›¾é£æ ¼å»ºè®® (é«˜å¤§ä¸Šã€ç§‘æŠ€æ„Ÿæˆ–æç®€é£)]
```

### ğŸ¨ è§†è§‰é…å›¾æŒ‡å— (Visual Guide)
**è¯´æ˜**ï¼šè¯·ä¸ºäººå·¥é…å›¾æä¾›è¯¦ç»†çš„ç”»é¢å»ºè®®ï¼Œå¸®åŠ©åšä¸»å¿«é€Ÿäº§å‡ºé«˜è´¨é‡ç´ æã€‚
[è¯·ç”¨ä¸­æ–‡åˆ—å‡ºä¸å°‘äº 3 å¼ å…³é”®é…å›¾çš„å»ºè®®ï¼š

å°é¢å›¾ï¼š[ç”»é¢æè¿°ï¼Œå¦‚ï¼šç§‘æŠ€æ„Ÿæµå…‰èƒŒæ™¯ï¼Œçªå‡ºæ ¸å¿ƒå…³é”®è¯]

å†…é¡µå›¾1ï¼š[æè¿°]

å†…é¡µå›¾2ï¼š[æè¿°]

å†…é¡µå›¾3ï¼š[æè¿°] ]
"""

    with httpx.Client(proxy=PROXY_URL, timeout=REQUEST_TIMEOUT) as http_client:
        client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL, http_client=http_client)
        
        try:
            @retryable
            def _chat_create():
                return client.chat.completions.create(
                    model="deepseek-reasoner",
                    messages=[
                        {"role": "system", "content": FINAL_PROMPT},
                        {"role": "user", "content": f"ä»¥ä¸‹æ˜¯ä»Šæ—¥çš„æ‰€æœ‰é€‰é¢˜æŠ¥å‘Šï¼Œè¯·ç»¼åˆåˆ†æåç»™å‡ºæœ€ç»ˆæ¨èï¼š\n\n{combined}"}
                    ],
                    stream=True
                )

            response = _chat_create()
            
            log_print("\n" + "="*60)
            log_print("ğŸ† æœ€ç»ˆé€‰é¢˜æ¨è")
            log_print("="*60 + "\n")
            
            collected = []
            for chunk in response:
                if chunk.choices[0].delta.content:
                    c = chunk.choices[0].delta.content
                    log_print(c, end="", flush=True)
                    collected.append(c)
            
            # ä¿å­˜ç»¼åˆæŠ¥å‘Š
            final_report = os.path.join(topics_dir, "FINAL_DECISION.md")
            content_str = ''.join(collected)
            with open(final_report, "w", encoding="utf-8") as f:
                f.write(f"# ğŸ† ä»Šæ—¥æœ€ç»ˆé€‰é¢˜å†³ç­–\n\n**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n**ç»¼åˆæŠ¥å‘Šæ•°**: {len(reports)}\n\n{content_str}")
            
            log_print(f"\n\nğŸ“ ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: {final_report}")

            # === è‡ªåŠ¨æ›´æ–°å†å²è®°å½• (Memory Update) ===
            try:
                import re
                # ä¼˜åŒ–æ­£åˆ™ï¼šå…¼å®¹ä¸­è‹±æ–‡å†’å·ã€å¿½ç•¥å‰åç©ºæ ¼ã€å¤šè¡ŒåŒ¹é…
                # æ¨¡å¼1: **æ ‡é¢˜**: xxx
                title_pattern1 = r'\*\*æ ‡é¢˜\*\*\s*[:ï¼š]\s*(.+)'
                # æ¨¡å¼2: ### é€‰é¢˜ 1ï¼šxxx
                title_pattern2 = r'###\s*é€‰é¢˜\s*\d+\s*[:ï¼š]\s*(.+)'
                
                final_topic = None
                
                # å°è¯•åŒ¹é…
                match1 = re.search(title_pattern1, content_str)
                if match1:
                    final_topic = match1.group(1).strip()
                else:
                    match2 = re.search(title_pattern2, content_str)
                    if match2:
                        final_topic = match2.group(1).strip()
                
                if final_topic:
                    save_topic_to_history(final_topic, "ç»¼åˆå†³ç­–")
                else:
                    # Fallback: å°è¯•æå–ç¬¬ä¸€è¡Œæœ‰æ•ˆæ–‡æœ¬
                    lines = [l.strip() for l in content_str.split('\n') if l.strip() and not l.startswith('#')]
                    if lines:
                        fallback_title = lines[0][:50]  # å–å‰50å­—ç¬¦
                        save_topic_to_history(fallback_title, "ç»¼åˆå†³ç­–")
                        log_print(f"âš ï¸ ä½¿ç”¨ Fallback æ ‡é¢˜: {fallback_title}")
                    else:
                        log_print("âš ï¸ è­¦å‘Š: æ— æ³•ä»æŠ¥å‘Šä¸­æå–æœ€ç»ˆé€‰é¢˜æ ‡é¢˜ï¼Œå†å²è®°å½•æœªæ›´æ–°ã€‚")
                        log_print(f"   è°ƒè¯•ä¿¡æ¯: å†…å®¹å‰200å­— -> {content_str[:200].replace(chr(10), ' ')}")
            
            except Exception as e:
                 log_print(f"âš ï¸ å†å²è®°å½•æ›´æ–°å¤±è´¥: {e}")
            
        except Exception as e:
            log_print(f"âŒ ç»¼åˆåˆ†æå¤±è´¥: {e}")

    log_print("\nâœ… ç»¼åˆé€‰é¢˜å®Œæˆï¼")

if __name__ == "__main__":
    main()
