"""
ğŸš€ å…¨ç½‘é€‰é¢˜é›·è¾¾ (Trend Hunter Agent) v7.0 - ç»ˆæä»·å€¼æŒ–æ˜ç‰ˆ
æ ¸å¿ƒå‡çº§ï¼š
1. çŸ©é˜µåŒ– WATCHLISTï¼šè¦†ç›–æ¨¡å‹ã€ç¼–ç¨‹ã€æ•ˆç‡å·¥å…·ä¸‰ç±»é¡¶æµï¼Œå¤–åŠ é€šç”¨æ•™ç¨‹ç±»ç›®ã€‚
2. å¿ƒç†å­¦æœç´¢ç­–ç•¥ï¼šå¼•å…¥"å³æ—¶æ»¡è¶³"(Bè·¯)å’Œ"æŸå¤±åŒæ¶"(Cè·¯)æœç´¢æ¨¡å‹ã€‚
3. ä»·å€¼æ’åºç®—æ³•ï¼šåŸºäº"è·å¾—æ„Ÿ"è¿›è¡ŒåŠ æƒï¼Œå‰”é™¤å®å¤§å™äº‹ã€‚
"""
import sys, os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import time
import json
import httpx
import random
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from openai import OpenAI
from config import (
    DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL, PROXY_URL, REQUEST_TIMEOUT,
    TAVILY_API_KEY, get_topic_report_file, get_today_dir,
    get_stage_dir, get_research_notes_file
)

# ================= é…ç½®åŒº =================

# é•¿æœŸå…³æ³¨çŸ©é˜µ (æµé‡åŸºæœ¬ç›˜)
WATCHLIST = [
    # é¡¶æµæ¨¡å‹
    "DeepSeek", "Kimi", "é€šä¹‰åƒé—®", "GPT-4o", "Gemini", "Grok",
    # ç¼–ç¨‹ç¥å™¨
    "Cursor", "Windsurf", "Claude Code", "GitHub Copilot",
    # æ•ˆç‡åº”ç”¨
    "å¤¸å…‹AI", "è±†åŒ…", "ç§˜å¡”æœç´¢", "è…¾è®¯å…ƒå®",
    # é€šç”¨ç±»ç›®
    "AIæ•™ç¨‹", "AIå‰¯ä¸š", "æ•ˆç‡ç¥å™¨"
]

# è¿è¥é˜¶æ®µé…ç½®
OPERATIONAL_PHASE = "VALUE_HACKER" # ä»·å€¼é»‘å®¢

PHASE_CONFIG = {
    "VALUE_HACKER": {
        "name": "ä»·å€¼é»‘å®¢æ¨¡å¼",
        "weights": {"news": 0.5, "social": 2.5, "github": 1.0}, # æåº¦é‡ç¤¾äº¤å’Œç—›ç‚¹
        "strategy": "åˆ©ç”¨å¿ƒç†å­¦é”šç‚¹(æ”¶ç›Š/æŸå¤±)ï¼ŒæŒ–æ˜èƒ½ç»™ç”¨æˆ·å¸¦æ¥'è·å¾—æ„Ÿ'çš„é€‰é¢˜ã€‚",
        "prompt_suffix": "âš ï¸ ç»å¯¹åŸåˆ™ï¼šåƒä¸€ä¸ª'ç”Ÿæ´»é»‘å®¢'ä¸€æ ·æ€è€ƒã€‚å‰”é™¤æ‰€æœ‰'æ–°é—»æŠ¥é“'ï¼Œåªä¿ç•™'è§£å†³æ–¹æ¡ˆ'ã€‚å¦‚æœæ˜¯å·¥å…·ï¼Œå¿…é¡»æ˜¯æ™®é€šäººæ‰‹æœº/ç”µè„‘èƒ½è£…çš„ï¼›å¦‚æœæ˜¯æ•™ç¨‹ï¼Œå¿…é¡»æ˜¯å°ç™½èƒ½çœ‹æ‡‚çš„ã€‚"
    }
}

CURRENT_CONFIG = PHASE_CONFIG[OPERATIONAL_PHASE]

# ================= Tavily æœç´¢å·¥å…· =================

class WebSearchTool:
    def __init__(self):
        self.api_key = TAVILY_API_KEY
        self.enabled = bool(self.api_key and len(self.api_key) > 10)
        if self.enabled:
            print("   âœ… Tavily Search API å·²å¯ç”¨")
    
    def search(self, query, max_results=5, include_answer=False):
        if not self.enabled: return []
        print(f"   ğŸ” Tavily: {query}")
        url = "https://api.tavily.com/search"
        payload = {
            "api_key": self.api_key, "query": query, "search_depth": "basic",
            "max_results": max_results, "include_answer": include_answer
        }
        try:
            # Tavily éœ€è¦ä»£ç† (å¦‚æœé…ç½®äº† PROXY_URL)
            # ä½¿ç”¨ trust_env=False é˜²æ­¢è¯»å–ç³»ç»Ÿç¯å¢ƒå˜é‡å¯¼è‡´æ··ä¹±ï¼Œæ˜¾å¼æŒ‡å®š proxy
            proxies = PROXY_URL if PROXY_URL else None
            with httpx.Client(timeout=30, proxy=proxies) as client:
                resp = client.post(url, json=payload)
                resp.raise_for_status()
                data = resp.json()
                results = []
                if data.get('answer'):
                    results.append({"title": "AI Summary", "body": data['answer'], "url": ""})
                for r in data.get('results', []):
                    results.append({
                        "title": r.get('title', ''),
                        "body": r.get('content', ''),
                        "url": r.get('url', '')
                    })
                return results
        except Exception as e:
            print(f"      âŒ æœç´¢å¤±è´¥: {e}")
            return []

# ================= è¾…åŠ©å‡½æ•° =================

def get_github_trending():
    print("   ğŸ” GitHub Trending (Weekly)...")
    url = "https://github.com/trending?since=weekly" # å…¨è¯­è¨€ Weeklyï¼ŒèŒƒå›´æ›´å¹¿
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        with httpx.Client(proxy=PROXY_URL, timeout=15) as client:
            resp = client.get(url, headers=headers)
        soup = BeautifulSoup(resp.text, 'html.parser')
        repos = soup.select('article.Box-row')
        results = []
        for repo in repos[:5]:
            name_tag = repo.select_one('h1 a') or repo.select_one('h2 a')
            if not name_tag: continue
            name = name_tag.get_text(strip=True).replace('\n', '').replace(' ', '')
            desc = repo.select_one('p').get_text(strip=True) if repo.select_one('p') else ""
            # è¿‡æ»¤æ‰éAI/å·¥å…·ç±»çš„ä»“åº“(ç®€å•å…³é”®è¯è¿‡æ»¤)
            results.append(f"- {name}: {desc}")
        return results
    except Exception as e:
        return [f"- GitHub æŠ“å–å¤±è´¥: {e}"]

# ================= æ ¸å¿ƒé€»è¾‘ =================

PLAN_PROMPT = """
ä½ æ˜¯"ç‹å¾€AI"çš„é¦–å¸­å†…å®¹ç­–ç•¥å®˜ã€‚
è¯·åŸºäºã€å…¨ç½‘æƒ…æŠ¥ã€‘å’Œã€å¿ƒç†å­¦ç­–ç•¥ã€‘ï¼ŒæŒ–æ˜ 3 ä¸ªæœ€å…·"çˆ†æ¬¾æ½œè´¨"çš„é€‰é¢˜æ–¹å‘ã€‚

å¿ƒç†å­¦ç­–ç•¥ï¼š
1. **Aè·¯ (é”šç‚¹æ•ˆåº”)**: å€ŸåŠ¿é¡¶æµ (DeepSeek/Kimi)ï¼Œå…³æ³¨å…¶"éšè—åŠŸèƒ½"æˆ–"æœ€æ–°ç©æ³•"ã€‚
2. **Bè·¯ (å³æ—¶æ»¡è¶³)**: å¯»æ‰¾"æ•ˆç‡ç¥å™¨"ã€"Life Hack"ï¼Œä¸»æ‰“"3åˆ†é’Ÿä¸Šæ‰‹"ã€"ä¸‹ç­æ—©èµ°1å°æ—¶"ã€‚
3. **Cè·¯ (æŸå¤±åŒæ¶)**: å¯»æ‰¾"é¿å‘æŒ‡å—"ã€"æ™ºå•†ç¨"ã€"å¹³æ›¿"ã€"ç¿»è½¦ç°åœº"ï¼Œå¼•å‘ç”¨æˆ·å±æœºæ„Ÿã€‚

è¾“å…¥æ•°æ®ï¼š
- é•¿æœŸå…³æ³¨å“ç±»åŠ¨æ€
- æœ¬å‘¨çƒ­é—¨å·¥å…·/æ•™ç¨‹
- ç”¨æˆ·åæ§½ä¸ç—›ç‚¹

å†³ç­–æ ‡å‡†ï¼š
- âœ… **ä¿ç•™**ï¼šDeepSeek è”ç½‘æœç´¢æ€ä¹ˆç”¨æ‰å‡†ã€Cursor å…è´¹é¢åº¦æ²¡äº†æ€ä¹ˆåŠã€å¤¸å…‹æ‰«æç‹å¯¹æ¯”ã€‚
- âŒ **å‰”é™¤**ï¼šOpenAI èèµ„æ¶ˆæ¯ã€Google å‘å¸ƒæ–°è®ºæ–‡ã€æŸæŸè¡Œä¸šå¤§æ¨¡å‹ç™½çš®ä¹¦ã€‚

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼ JSONï¼‰ï¼š
[
    {
        "event": "é€‰é¢˜æ ¸å¿ƒè¯ (å¦‚: DeepSeek)",
        "angle": "åˆ‡å…¥è§’åº¦ (å¦‚: éšè—ç©æ³• / é¿å‘æŒ‡å—)",
        "news_query": "åŠŸèƒ½æ€§æœç´¢è¯ (å¦‚: DeepSeek V3 file upload)",
        "social_query": "æƒ…ç»ªæ€§æœç´¢è¯ (å¦‚: DeepSeek æŠ¥é”™ / DeepSeek ä¸å¥½ç”¨)"
    },
    ...
]
"""

def step1_broad_scan_and_plan(client, search_tool):
    """Step 1: å¹¿åŸŸä»·å€¼æ‰«æ (å¿ƒç†å­¦ä¸‰è·¯ç­–ç•¥)"""
    print(f"\nğŸ“¡ [Step 1] å¹¿åŸŸä»·å€¼æ‰«æ (ç­–ç•¥: {CURRENT_CONFIG['name']})...")
    
    pre_scan_results = []
    
    # === Aè·¯: é¡¶æµé”šç‚¹ (Watchlist) ===
    # éšæœºé€‰ 3 ä¸ªé¡¶æµï¼Œæœ"ç©æ³•"
    targets = random.sample(WATCHLIST, 3)
    print(f"   ğŸ¯ [Aè·¯-é”šç‚¹] æ‰«æé¡¶æµ: {targets}")
    for t in targets:
        res = search_tool.search(f"{t} éšè—åŠŸèƒ½ ç©æ³• æ•™ç¨‹ 2025", max_results=2)
        pre_scan_results.extend(res)
        
    # === Bè·¯: å³æ—¶æ»¡è¶³ (Life Hack) ===
    # æœ"ç¥å™¨"ã€"é»‘ç§‘æŠ€"
    print(f"   âš¡ [Bè·¯-æ”¶ç›Š] æ‰«ææ•ˆç‡ç¥å™¨...")
    queries = ["æœ¬å‘¨ AI æ•ˆç‡ç¥å™¨ æ¨è", "AI è‡ªåŠ¨åŒ–åŠå…¬ æ•™ç¨‹", "Notion AI æ›¿ä»£å“"]
    for q in queries:
        res = search_tool.search(q, max_results=2)
        pre_scan_results.extend(res)
        
    # === Cè·¯: æŸå¤±åŒæ¶ (Pain Points) ===
    # æœ"é¿å‘"ã€"æ™ºå•†ç¨"
    print(f"   ğŸ›¡ï¸ [Cè·¯-æŸå¤±] æ‰«æé¿å‘/åæ§½...")
    queries = ["AIå·¥å…· æ™ºå•†ç¨ é¿å‘", "AIçœ¼é•œ ç¿»è½¦", "AI å†™ä½œ æŸ¥é‡"]
    for q in queries:
        res = search_tool.search(q, max_results=2)
        pre_scan_results.extend(res)
    
    pre_scan_text = "\n".join([f"- {r['title']}: {r['body'][:80]}" for r in pre_scan_results])
    
    # 2. æ™ºèƒ½ç­›é€‰ä¸è§„åˆ’
    print(f"   ğŸ“ æƒ…æŠ¥èšåˆå®Œæ¯•ï¼ŒDeepSeek æ­£åœ¨åº”ç”¨å¿ƒç†å­¦ç­–ç•¥é€‰é¢˜...")
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": PLAN_PROMPT},
                {"role": "user", "content": f"ã€æ··åˆæƒ…æŠ¥æ± ã€‘\n{pre_scan_text}"}
            ],
            temperature=0.7,
            response_format={ "type": "json_object" }
        )
        content = response.choices[0].message.content
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        
        search_plan = json.loads(content)
        if isinstance(search_plan, dict) and "events" in search_plan:
            search_plan = search_plan["events"]
            
        print(f"   ğŸ§  é€‰é¢˜æ–¹å‘å·²é”å®š: {[i['event'] + '-' + i['angle'] for i in search_plan]}\n")
        return search_plan
    except Exception as e:
        print(f"   âŒ è§„åˆ’å¤±è´¥: {e}")
        return [{"event": "DeepSeek", "angle": "é¿å‘", "news_query": "DeepSeek V3", "social_query": "DeepSeek å¹»è§‰"}]

def step2_deep_scan(search_plan, search_tool):
    """Step 2: æ·±åº¦éªŒè¯ (é‡ç¤¾äº¤/ç—›ç‚¹)"""
    print("ğŸ“¡ [Step 2] å¯åŠ¨æ·±åº¦ä»·å€¼éªŒè¯...\n")
    all_results = []
    
    w_news = CURRENT_CONFIG['weights']['news']
    w_social = CURRENT_CONFIG['weights']['social']
    
    for item in search_plan:
        event = item.get("event", "æœªçŸ¥")
        angle = item.get("angle", "é€šç”¨")
        news_q = item.get("news_query", "")
        social_q = item.get("social_query", "")
        
        print(f"   ğŸ” æ­£åœ¨æ·±æŒ–: ã€{event}ã€‘ ({angle}æ–¹å‘)")
        event_data = [f"=== é€‰é¢˜: {event} ({angle}) ==="]
        
        # 1. ç¤¾äº¤/ç—›ç‚¹æœç´¢ (æ ¸å¿ƒ)
        if social_q:
            print(f"      ğŸ’¬ ç¤¾äº¤èˆ†æƒ… (æƒé‡ {w_social}): {social_q}")
            # å¢åŠ çŸ¥ä¹ã€Bç«™(site:bilibili.com)
            full_social_q = f"{social_q} site:mp.weixin.qq.com OR site:xiaohongshu.com OR site:zhihu.com OR site:bilibili.com"
            res = search_tool.search(full_social_q, max_results=4)
            if res:
                event_data.append(f"--- ç”¨æˆ·çœŸå®åé¦ˆ ({social_q}) ---")
                event_data.extend([f"- {r['title']}: {r['body'][:80]}..." for r in res])
                
        # 2. å®˜æ–¹éªŒè¯ (è¾…åŠ©)
        if news_q:
            print(f"      ğŸ”¥ å®˜æ–¹éªŒè¯ (æƒé‡ {w_news}): {news_q}")
            res = search_tool.search(news_q, max_results=2)
            if res:
                event_data.append(f"--- å®˜æ–¹ä¿¡æ¯ ({news_q}) ---")
                event_data.extend([f"- {r['title']}" for r in res])
        
        all_results.append("\n".join(event_data))
        print("")
        time.sleep(1)

    # GitHub è¡¥å…… (Weekly)
    print(f"   ğŸ’» GitHub Weekly Trending...")
    github_res = get_github_trending()
    all_results.append("=== GitHub Weekly Trending ===\n" + "\n".join(github_res))
    
    return "\n\n".join(all_results)

def step3_final_decision(scan_data, client):
    """Step 3: å†³ç­–"""
    print("\n" + "="*50 + "\nğŸ“ DeepSeek ä¸»ç¼–å®¡æ ¸ä¸­...\n" + "="*50)
    
    prompt = f"""
    {EDITOR_PROMPT}
    
    å½“å‰ç­–ç•¥ï¼šã€{CURRENT_CONFIG['name']}ã€‘
    {CURRENT_CONFIG['prompt_suffix']}
    """
    
    try:
        # å•æ¬¡æ‰«æç”¨ chat æ¨¡å‹ï¼ˆå¿«ã€ä¾¿å®œï¼‰ï¼Œç»¼åˆå†³ç­–æ‰ç”¨ reasoner
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": f"ã€æ·±åº¦éªŒè¯æƒ…æŠ¥ã€‘\n{scan_data}"}
            ],
            stream=True
        )
        
        print("\n" + "="*20 + " é€‰é¢˜æŠ¥å‘Š " + "="*20 + "\n")
        collected = []
        for chunk in response:
            if chunk.choices[0].delta.content:
                c = chunk.choices[0].delta.content
                print(c, end="", flush=True)
                collected.append(c)
        return "".join(collected)
    except Exception as e:
        print(f"âŒ å†³ç­–å¤±è´¥: {e}")
        return f"å¤±è´¥: {e}"

EDITOR_PROMPT = """
ä½ å«"ç‹å¾€AI"ï¼Œä¸“æ³¨ AI å·¥ä½œæµçš„ç¡¬æ ¸åšä¸»ã€‚
è¯·ç­›é€‰ 3 ä¸ªã€è·å¾—æ„Ÿæœ€é«˜ã€‘çš„é€‰é¢˜ã€‚

**è·å¾—æ„Ÿå…¬å¼** = (å¸®ç”¨æˆ·è§£å†³çš„ç—›ç‚¹ * èŠ‚çœçš„æ—¶é—´/é‡‘é’±) - é˜…è¯»é—¨æ§›

å†³ç­–é€»è¾‘ï¼š
1. **åªåšäººè¯**ï¼šæ‹’ç»æ‰€æœ‰æŠ€æœ¯é»‘è¯ï¼ŒæŠŠ"ä¸Šä¸‹æ–‡ç¼“å­˜"ç¿»è¯‘æˆ"è®©AIè®°ä½ä½ ä¸Šå‘¨è¯´äº†å•¥"ã€‚
2. **åªåšç—›ç‚¹**ï¼šä¼˜å…ˆé€‰"é¿å‘"ã€"å¹³æ›¿"ã€"ç™½å«–"ã€"ææ•ˆ"ç±»é€‰é¢˜ã€‚
3. **å…³è”çƒ­ç‚¹**ï¼šå¦‚æœæ¶‰åŠ WATCHLIST ä¸­çš„äº§å“ï¼ŒåŠ åˆ†ã€‚

è¾“å‡ºæ ¼å¼ï¼š
### é€‰é¢˜ 1ï¼š[æ ‡é¢˜] (éœ€æå…·å¸å¼•åŠ›ï¼Œå¦‚ï¼šDeepSeek å±…ç„¶è¿˜èƒ½è¿™ä¹ˆç©ï¼Ÿ)
* **è·å¾—æ„Ÿ**ï¼š[ç”¨æˆ·çœ‹å®Œèƒ½å¾—åˆ°ä»€ä¹ˆï¼Ÿçœé’±ï¼Ÿçœæ—¶ï¼Ÿ]
* **å¿ƒç†é”šç‚¹**ï¼š[åˆ©ç”¨äº†ä»€ä¹ˆå¿ƒç†ï¼Ÿè´ªä¾¿å®œï¼Ÿæ€•è½åï¼Ÿ]
* **æ ¸å¿ƒçœ‹ç‚¹**ï¼š[æ–‡ç« å¤§çº²ï¼ŒåŒ…å«å…·ä½“çš„å·¥å…·/æŠ€å·§]
---
## ä»Šæ—¥ä¸»æ¨
å‘Šè¯‰æˆ‘ä¸å†™ä¼šåæ‚”çš„é‚£ä¸ª (è·å¾—æ„Ÿæœ€å¼ºçš„)ã€‚
"""

def auto_init_workflow():
    """è‡ªåŠ¨åˆå§‹åŒ–åç»­å·¥ä½œæµæ–‡ä»¶å¤¹å’Œæ–‡ä»¶"""
    print("\nâš™ï¸ æ­£åœ¨åˆå§‹åŒ–åç»­å·¥ä½œæµ...")
    
    # 1. é¢„åˆ›å»ºæ‰€æœ‰é˜¶æ®µæ–‡ä»¶å¤¹
    from config import get_stage_dir, get_research_notes_file
    stages = ["research", "drafts", "publish", "assets"]
    for stage in stages:
        path = get_stage_dir(stage)
        print(f"   ğŸ“‚ ç›®å½•å°±ç»ª: {path}")
        
    # 2. åˆ›å»ºç©ºç™½ç ”ç©¶ç¬”è®°
    notes_file = get_research_notes_file()
    if not os.path.exists(notes_file):
        with open(notes_file, "w", encoding="utf-8") as f:
            f.write("# ç ”ç©¶ç¬”è®°\n\nè¯·å°† NotebookLM ç”Ÿæˆçš„ Briefing Doc ç²˜è´´åœ¨è¿™é‡Œ...\n")
        print(f"   ğŸ“„ ç¬”è®°æ–‡ä»¶å·²åˆ›å»º: {notes_file}")
    
    # 3. æç¤ºä¸‹ä¸€æ­¥
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥ï¼š")
    print("   - å¯ç»§ç»­è¿è¡Œ hunt è·å–æ›´å¤šé€‰é¢˜")
    print("   - æˆ–è¿è¡Œ `python run.py final` ç»¼åˆæ‰€æœ‰æŠ¥å‘Šï¼Œè·å¾— 3 ä¸ªæç¤ºè¯")

def save_report(raw_data, analysis):
    filename = get_topic_report_file()
    content = f"# ğŸš€ é€‰é¢˜é›·è¾¾æŠ¥å‘Š v7.0 ({CURRENT_CONFIG['name']})\n\n**æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n**ç­–ç•¥**: {CURRENT_CONFIG['strategy']}\n\n## æ·±åº¦éªŒè¯æƒ…æŠ¥\n{raw_data}\n\n## é€‰é¢˜åˆ†æ\n{analysis}"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(content)
    print(f"\n\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜: {filename}")
    
    # ä¿å­˜åè‡ªåŠ¨åˆå§‹åŒ–å·¥ä½œæµ
    auto_init_workflow()

def main():
    print("\n" + "="*60 + "\nğŸš€ å…¨ç½‘é€‰é¢˜é›·è¾¾ v7.0 (ä»·å€¼æŒ–æ˜ç‰ˆ) - ç‹å¾€AI\n" + "="*60 + "\n")
    
    search_tool = WebSearchTool()
    
    # DeepSeek å»ºè®®ç›´è¿ï¼Œä¸èµ°ä»£ç† (é™¤é api.deepseek.com è¢«å¢™)
    # è¿™é‡Œæˆ‘ä»¬å°† proxy è®¾ä¸º Noneï¼Œç¡®ä¿å®ƒä¸èµ° PROXY_URL
    with httpx.Client(proxy=None, timeout=REQUEST_TIMEOUT) as http_client:
        client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL, http_client=http_client)
        
        # 1. å¹¿åŸŸæ‰«æ (Watchlist + Trend + Pain)
        search_plan = step1_broad_scan_and_plan(client, search_tool)
        
        # 2. æ·±åº¦éªŒè¯
        raw_data = step2_deep_scan(search_plan, search_tool)
        
        # 3. å†³ç­–
        analysis = step3_final_decision(raw_data, client)
        
        # 4. ä¿å­˜
        save_report(raw_data, analysis)
    
    print("\nâœ… é€‰é¢˜é›·è¾¾å®Œæˆï¼")

def final_summary():
    """ç»¼åˆå½“å¤©æ‰€æœ‰æŠ¥å‘Šï¼Œç»™å‡ºæœ€ç»ˆé€‰é¢˜æ¨èå’Œä¸‰ä¸ªæç¤ºè¯"""
    import glob
    from config import get_today_dir
    
    print("\n" + "="*60)
    print("ğŸ¯ ç»¼åˆé€‰é¢˜å†³ç­– - æ•´åˆä»Šæ—¥æ‰€æœ‰æŠ¥å‘Š")
    print("="*60 + "\n")
    
    # 1. è¯»å–å½“å¤©æ‰€æœ‰æŠ¥å‘Š
    topics_dir = os.path.join(get_today_dir(), "1_topics")
    reports = glob.glob(os.path.join(topics_dir, "report_*.md"))
    
    if not reports:
        print("âŒ ä»Šæ—¥æš‚æ— æŠ¥å‘Šï¼Œè¯·å…ˆè¿è¡Œ `python run.py hunt`")
        return
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(reports)} ä»½æŠ¥å‘Šï¼š")
    all_content = []
    for r in sorted(reports):
        print(f"   ğŸ“„ {os.path.basename(r)}")
        with open(r, "r", encoding="utf-8") as f:
            all_content.append(f"=== {os.path.basename(r)} ===\n{f.read()}")
    
    combined = "\n\n".join(all_content)
    
    # 2. DeepSeek ç»¼åˆåˆ†æ
    print("\nğŸ§  DeepSeek æ­£åœ¨ç»¼åˆåˆ†æ...")
    
    FINAL_PROMPT = """
ä½ æ˜¯"ç‹å¾€AI"ï¼Œä¸€ä¸ªæ“…é•¿ä»å¤šä»½æƒ…æŠ¥ä¸­æç‚¼æ ¸å¿ƒé€‰é¢˜çš„å…¬ä¼—å·ä¸»ç¼–ã€‚

ä½ çš„ä»»åŠ¡ï¼šç»¼åˆåˆ†æä»Šå¤©çš„æ‰€æœ‰é€‰é¢˜æŠ¥å‘Šï¼Œé€‰å‡ºã€1ä¸ªæœ€ç»ˆé€‰é¢˜ã€‘ï¼Œå¹¶è¾“å‡º3ä¸ªç»“æ„åŒ–æç¤ºè¯ã€‚

## é€‰é¢˜æ ‡å‡† (æŒ‰ä¼˜å…ˆçº§)
1. **å‡ºç°é¢‘ç‡é«˜**ï¼šå¤šæ¬¡å‡ºç°çš„é€‰é¢˜è¯´æ˜çƒ­åº¦æŒç»­ï¼Œå€¼å¾—æ·±æŒ–
2. **è·å¾—æ„Ÿå¼º**ï¼šèƒ½è®©è¯»è€…"çœé’±ã€çœæ—¶ã€å­¦ä¼šæ–°æŠ€èƒ½"çš„é€‰é¢˜ä¼˜å…ˆ
3. **ç—›ç‚¹å°–é”**ï¼šè§£å†³çš„é—®é¢˜è¶Šå…·ä½“ã€è¶Šç—›ï¼Œè¶Šæœ‰çˆ†æ¬¾æ½œè´¨

## è¾“å‡ºæ ¼å¼

### ğŸ† ä»Šæ—¥æœ€ç»ˆé€‰é¢˜
**æ ‡é¢˜**ï¼š[çˆ†æ¬¾æ ‡é¢˜ï¼Œ15-25å­—]
**ä¸€å¥è¯å–ç‚¹**ï¼š[ç”¨æˆ·çœ‹å®Œèƒ½å¾—åˆ°ä»€ä¹ˆï¼Ÿ]
**å…³é”®è¯**ï¼š[3-5ä¸ªæœç´¢å…³é”®è¯ï¼Œç”¨äºåç»­ç´ ææœé›†]

### ğŸ“¡ æç¤ºè¯ 1ï¼šFast Research (ç”¨äº NotebookLM æœç´¢ç´ æ)
```
[è¯·ç”¨ä¸­æ–‡ï¼Œå‘Šè¯‰ NotebookLM éœ€è¦æœç´¢å“ªäº›å…·ä½“å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š
- å®˜æ–¹æ–‡æ¡£/æ•™ç¨‹
- ç”¨æˆ·çœŸå®è¯„ä»·/é¿å‘ç»éªŒ
- åŒç±»å·¥å…·å¯¹æ¯”
- æœ€æ–°æ›´æ–°/ç‰ˆæœ¬å˜åŒ–
æ ¼å¼è¦æ±‚ï¼šåˆ†æ¡åˆ—å‡ºï¼Œæ¯æ¡ä¸€ä¸ªæ˜ç¡®çš„æœç´¢ä»»åŠ¡]
```

### âœï¸ æç¤ºè¯ 2ï¼šè‰ç¨¿å¤§çº² (ç”¨äºç”Ÿæˆæ–‡ç« æ¡†æ¶)
**ä½¿ç”¨æ–¹æ³•**ï¼šå¤åˆ¶åˆ° NotebookLMï¼Œè®©å®ƒæ ¹æ®å·²å¯¼å…¥çš„ Sources æ¥å®Œå–„å¤§çº²
```
è¯·æ ¹æ®æ¥æºå†…å®¹æ¥å®Œå–„ä¸‹é¢è‰ç¨¿å¤§çº²ï¼Œè¾“å‡ºå®Œæ•´çš„æ–‡ç« åˆç¨¿ï¼š

[ç»™å‡ºä¸€ä¸ªå®Œæ•´çš„æ–‡ç« å¤§çº²ï¼ŒåŒ…æ‹¬ï¼š
- å¼€å¤´ Hook (å¦‚ä½•åœ¨3ç§’å†…æŠ“ä½è¯»è€…)
- ç—›ç‚¹æè¿° (è¯»è€…å…±é¸£)
- è§£å†³æ–¹æ¡ˆ (æ‰‹æŠŠæ‰‹æ­¥éª¤)
- è¿›é˜¶æŠ€å·§ (é¢å¤–ä»·å€¼)
- ç»“å°¾ Call to Action]
```

### ğŸ¨ æç¤ºè¯ 3ï¼šè§†è§‰è„šæœ¬ (ç”¨äºé…å›¾æ–¹æ¡ˆ)
**ä½¿ç”¨æ–¹æ³•**ï¼šå¤åˆ¶åˆ° NotebookLM Chatï¼Œç„¶åç‚¹å‡»å³ä¾§ Studio â†’ **Infographic** ç”Ÿæˆä¿¡æ¯å›¾
```
[è¯·ç”¨ä¸­æ–‡ï¼Œå»ºè®®éœ€è¦å‡†å¤‡çš„é…å›¾ï¼ŒåŒ…æ‹¬ï¼š
- å…³é”®æˆªå›¾ (å“ªä¸ªç•Œé¢ã€å“ªä¸ªæ­¥éª¤)
- å¯¹æ¯”å›¾ (ä»€ä¹ˆ vs ä»€ä¹ˆ)
- æµç¨‹å›¾ (å¦‚æœæœ‰å¤æ‚æµç¨‹)
- å°é¢å›¾é£æ ¼å»ºè®®
- ä¿¡æ¯å›¾è¦ç‚¹ (é€‚åˆç”¨ Infographic ç”Ÿæˆçš„æ•°æ®/å¯¹æ¯”)]
```
"""

    with httpx.Client(proxy=None, timeout=REQUEST_TIMEOUT) as http_client:
        client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL, http_client=http_client)
        
        try:
            response = client.chat.completions.create(
                model="deepseek-reasoner",
                messages=[
                    {"role": "system", "content": FINAL_PROMPT},
                    {"role": "user", "content": f"ä»¥ä¸‹æ˜¯ä»Šæ—¥çš„æ‰€æœ‰é€‰é¢˜æŠ¥å‘Šï¼Œè¯·ç»¼åˆåˆ†æåç»™å‡ºæœ€ç»ˆæ¨èï¼š\n\n{combined}"}
                ],
                stream=True
            )
            
            print("\n" + "="*60)
            print("ğŸ† æœ€ç»ˆé€‰é¢˜æ¨è")
            print("="*60 + "\n")
            
            collected = []
            for chunk in response:
                if chunk.choices[0].delta.content:
                    c = chunk.choices[0].delta.content
                    print(c, end="", flush=True)
                    collected.append(c)
            
            # ä¿å­˜ç»¼åˆæŠ¥å‘Š
            final_report = os.path.join(topics_dir, "FINAL_DECISION.md")
            with open(final_report, "w", encoding="utf-8") as f:
                f.write(f"# ğŸ† ä»Šæ—¥æœ€ç»ˆé€‰é¢˜å†³ç­–\n\n**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n**ç»¼åˆæŠ¥å‘Šæ•°**: {len(reports)}\n\n{''.join(collected)}")
            
            print(f"\n\nğŸ“ ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: {final_report}")
            
        except Exception as e:
            print(f"âŒ ç»¼åˆåˆ†æå¤±è´¥: {e}")

    print("\nâœ… ç»¼åˆé€‰é¢˜å®Œæˆï¼")

if __name__ == "__main__":
    main()
