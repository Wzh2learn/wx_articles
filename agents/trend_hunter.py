"""
ğŸš€ å…¨ç½‘é€‰é¢˜é›·è¾¾ (Trend Hunter Agent) v3.0 - Tavily è”ç½‘ç‰ˆ
æ ¸å¿ƒå‡çº§ï¼š
1. æ”¯æŒ Tavily API (ç¨³å®šã€ä¸“ä¸º AI ä¼˜åŒ–)
2. DeepSeek åŠ¨æ€ç”Ÿæˆæœç´¢è¯
3. ä¿ç•™ GitHub Trending ä½œä¸ºè¡¥å……
"""
import sys, os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import time
import httpx
from datetime import datetime
from bs4 import BeautifulSoup
from openai import OpenAI
from config import (
    DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL, PROXY_URL, REQUEST_TIMEOUT,
    TAVILY_API_KEY, get_topic_report_file, get_today_dir
)

# ================= Tavily æœç´¢å·¥å…· =================

class WebSearchTool:
    """Tavily AI Search - ä¸“ä¸º LLM ä¼˜åŒ–çš„æœç´¢ API"""
    
    def __init__(self):
        self.api_key = TAVILY_API_KEY
        self.enabled = bool(self.api_key and len(self.api_key) > 10)
        if self.enabled:
            print("   âœ… Tavily Search API å·²å¯ç”¨")
        else:
            print("   âš ï¸ æœªé…ç½® Tavily APIï¼Œæœç´¢åŠŸèƒ½å—é™")
    
    def search(self, query, max_results=5):
        """æ‰§è¡Œ Tavily æœç´¢"""
        if not self.enabled:
            return []
        
        print(f"   ğŸ” Tavily: {query}")
        url = "https://api.tavily.com/search"
        payload = {
            "api_key": self.api_key,
            "query": query,
            "search_depth": "basic",
            "max_results": max_results,
            "include_answer": True
        }
        try:
            with httpx.Client(timeout=30) as client:
                resp = client.post(url, json=payload)
                resp.raise_for_status()
                data = resp.json()
                
                results = []
                if data.get('answer'):
                    results.append({"title": "AI Summary", "body": data['answer'], "url": ""})
                
                for r in data.get('results', []):
                    results.append({
                        "title": r.get('title', ''),
                        "body": r.get('content', ''),
                        "url": r.get('url', '')
                    })
                return results
        except Exception as e:
            print(f"      âŒ æœç´¢å¤±è´¥: {e}")
            return []

def get_github_trending():
    """æŠ“å– GitHub Trending"""
    print("   ğŸ” GitHub Trending...")
    url = "https://github.com/trending/python?since=daily"
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        with httpx.Client(proxy=PROXY_URL, timeout=15) as client:
            resp = client.get(url, headers=headers)
        soup = BeautifulSoup(resp.text, 'html.parser')
        repos = soup.select('article.Box-row')
        results = []
        for repo in repos[:8]:
            name_tag = repo.select_one('h1 a') or repo.select_one('h2 a')
            if not name_tag: continue
            name = name_tag.get_text(strip=True).replace('\n', '').replace(' ', '')
            desc = repo.select_one('p').get_text(strip=True) if repo.select_one('p') else ""
            results.append(f"- {name}: {desc}")
        return results
    except Exception as e:
        return [f"- GitHub æŠ“å–å¤±è´¥: {e}"]

# ================= DeepSeek æ€è€ƒé“¾ =================

SEARCH_PLAN_PROMPT = """
ä½ æ˜¯"ç‹å¾€AI"çš„é€‰é¢˜åŠ©ç†ã€‚è¯·æ ¹æ®ä»Šå¤©æ—¥æœŸï¼Œç”Ÿæˆ 5 ä¸ªæœ€å€¼å¾—æœç´¢çš„ AI çƒ­ç‚¹å…³é”®è¯ã€‚

è¦æ±‚ï¼š
1. æ—¶æ•ˆæ€§ï¼šåŒ…å«å½“å‰å¹´æœˆï¼Œå¦‚ "DeepSeek V3 December 2025"
2. é’ˆå¯¹æ€§ï¼šå…³æ³¨å¤§æ¨¡å‹å‘å¸ƒã€è¯„æµ‹ã€GitHub çˆ†æ¬¾ã€AI å·¥å…·
3. å¤šæ ·æ€§ï¼šä¸­è‹±æ–‡æ··åˆ

è¾“å‡ºï¼šä»…è¾“å‡ºå…³é”®è¯ï¼Œé€—å·åˆ†éš”ã€‚
"""

EDITOR_PROMPT = """ä½ å«"ç‹å¾€AI"ï¼Œä¸“æ³¨ AI å·¥ä½œæµçš„ç¡¬æ ¸åšä¸»ã€‚
è¯·æ ¹æ®ä»¥ä¸‹ã€å…¨ç½‘æƒ…æŠ¥ã€‘ç­›é€‰ 3 ä¸ªæœ€å€¼å¾—å†™çš„é€‰é¢˜ã€‚

âš ï¸ é€‰é¢˜ä¼˜å…ˆçº§ï¼š
1. **é‡å¤§çªå‘**ï¼šå¦‚ DeepSeek V3.2 å‘å¸ƒã€GPT-5 ä¸Šçº¿
2. **äº‰è®®è¯é¢˜**ï¼šå¦‚ AI ç¨‹åºå‘˜å–ä»£äººç±»ã€å¼€æº vs é—­æº
3. **å®æˆ˜å¹²è´§**ï¼šå¦‚ Cursor æ·±åº¦è¯„æµ‹ã€Agent å·¥ä½œæµ

è¾“å‡ºæ ¼å¼ï¼š
### é€‰é¢˜ 1ï¼š[æ ‡é¢˜]
* **çƒ­åº¦æ¥æº**ï¼š[å…·ä½“çš„æœç´¢ç»“æœ]
* **æ¨èç†ç”±**ï¼š[ä¸ºä»€ä¹ˆç°åœ¨å†™ä¼šç«]
* **æ ¸å¿ƒçœ‹ç‚¹**ï¼š[æ–‡ç« å¤§çº²]
---
## ä»Šæ—¥ä¸»æ¨
å‘Šè¯‰æˆ‘ä¸å†™ä¼šåæ‚”çš„é‚£ä¸ªã€‚"""

def run_search_plan(client):
    """Step 1: DeepSeek è§„åˆ’æœç´¢è¯"""
    print("\nğŸ§  DeepSeek æ­£åœ¨æ€è€ƒä»Šæ—¥æœç´¢ç­–ç•¥...")
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": SEARCH_PLAN_PROMPT},
                {"role": "user", "content": f"ä»Šå¤©æ˜¯ {datetime.now().strftime('%Y-%m-%d')}"}
            ],
            temperature=0.7
        )
        keywords = [k.strip() for k in response.choices[0].message.content.strip().split(',') if k.strip()]
        print(f"ğŸ“ æœç´¢è®¡åˆ’: {keywords}\n")
        return keywords[:5]
    except Exception as e:
        print(f"âŒ æ€è€ƒå¤±è´¥: {e}")
        return ["DeepSeek V3 æœ€æ–°æ¶ˆæ¯", "AI Agent å·¥å…· 2025", "LLM è¯„æµ‹æ’è¡Œ"]

def execute_search(keywords, search_tool):
    """Step 2: æ‰§è¡Œ Tavily æœç´¢"""
    print("ğŸ“¡ å¼€å§‹å…¨ç½‘æ‰«æ...\n")
    all_results = []
    
    for kw in keywords:
        results = search_tool.search(kw, max_results=4)
        if results:
            all_results.append(f"\n=== æœç´¢: {kw} ===")
            for r in results:
                title = r.get('title', '')
                body = r.get('body', '')[:200] if r.get('body') else ''
                url = r.get('url', '')
                all_results.append(f"- [{title}]({url})\n  {body}...")
        time.sleep(0.5)
    
    # è¡¥å…… GitHub
    print("\nğŸ“¡ è¡¥å……æ‰«æ GitHub Trending...")
    github_res = get_github_trending()
    all_results.append("\n=== GitHub Trending ===")
    all_results.extend(github_res)
    
    return "\n".join(all_results)

def final_decision(search_results, client):
    """Step 3: DeepSeek ç”Ÿæˆé€‰é¢˜æŠ¥å‘Š"""
    print("\n" + "="*50 + "\nğŸ“ DeepSeek ä¸»ç¼–å®¡æ ¸ä¸­...\n" + "="*50)
    try:
        response = client.chat.completions.create(
            model="deepseek-reasoner",
            messages=[
                {"role": "system", "content": EDITOR_PROMPT},
                {"role": "user", "content": f"ã€ä»Šæ—¥å…¨ç½‘æƒ…æŠ¥ã€‘\n{search_results}"}
            ],
            stream=True
        )
        
        print("\n" + "="*20 + " é€‰é¢˜æŠ¥å‘Š " + "="*20 + "\n")
        collected = []
        for chunk in response:
            if chunk.choices[0].delta.content:
                c = chunk.choices[0].delta.content
                print(c, end="", flush=True)
                collected.append(c)
        return search_results, "".join(collected)
    except Exception as e:
        print(f"âŒ å†³ç­–å¤±è´¥: {e}")
        return search_results, f"å¤±è´¥: {e}"

def save_report(raw_data, analysis):
    filename = get_topic_report_file()
    content = f"# ğŸš€ é€‰é¢˜é›·è¾¾æŠ¥å‘Š v3.0\n\n**æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n## å…¨ç½‘æƒ…æŠ¥\n{raw_data}\n\n## é€‰é¢˜åˆ†æ\n{analysis}"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(content)
    print(f"\n\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜: {filename}")

def main():
    print("\n" + "="*60 + "\nğŸš€ å…¨ç½‘é€‰é¢˜é›·è¾¾ v3.0 (Tavily è”ç½‘ç‰ˆ) - ç‹å¾€AI\n" + "="*60 + "\n")
    
    # åˆå§‹åŒ–æœç´¢å·¥å…·
    search_tool = WebSearchTool()
    
    with httpx.Client(proxy=PROXY_URL, timeout=REQUEST_TIMEOUT) as http_client:
        client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL, http_client=http_client)
        
        # 1. æ€è€ƒï¼šè§„åˆ’æœç´¢è¯
        keywords = run_search_plan(client)
        
        # 2. æ‰§è¡Œï¼šTavily æœç´¢
        raw_data = execute_search(keywords, search_tool)
        
        # 3. å†³ç­–ï¼šç”Ÿæˆé€‰é¢˜
        _, analysis = final_decision(raw_data, client)
        
        # 4. ä¿å­˜æŠ¥å‘Š
        save_report(raw_data, analysis)
    
    print("\nâœ… é€‰é¢˜é›·è¾¾å®Œæˆï¼")

if __name__ == "__main__":
    main()
