"""
ğŸš€ å…¨ç½‘é€‰é¢˜é›·è¾¾ (Trend Hunter Agent) v2.0
"""
import sys, os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import json, httpx, time
from datetime import datetime
from bs4 import BeautifulSoup
from duckduckgo_search import DDGS
from openai import OpenAI
from config import DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL, PROXY_URL, REQUEST_TIMEOUT, get_topic_report_file, get_today_dir

def get_github_trending():
    print("ğŸ” [1/6] æ‰«æ GitHub Trending...")
    url = "https://github.com/trending/python?since=daily"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    try:
        with httpx.Client(proxy=PROXY_URL, timeout=15) as client:
            resp = client.get(url, headers=headers)
            resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')
        repos = soup.select('article.Box-row') or soup.select('.Box-row')
        results = []
        for repo in repos[:10]:
            name_tag = repo.select_one('h2 a') or repo.select_one('h1 a')
            if not name_tag: continue
            name = name_tag.get_text(strip=True).replace('\n', '').replace(' ', '')
            desc_tag = repo.select_one('p.col-9') or repo.select_one('p')
            desc = desc_tag.get_text(strip=True) if desc_tag else ""
            results.append(f"{name}: {desc}")
        return results if results else ["GitHub æš‚æ— æ•°æ®"]
    except Exception as e:
        return [f"GitHub æŠ“å–å¤±è´¥: {e}"]

def get_readhub_news():
    print("ğŸ” [2/6] æ‰«æ ReadHub ç§‘æŠ€æ–°é—»...")
    url = "https://api.readhub.cn/topic?lastCursor=&pageSize=15"
    try:
        with httpx.Client(proxy=PROXY_URL, timeout=10) as client:
            resp = client.get(url)
            data = resp.json()
        items = data.get('data', [])
        return [item.get('title', '') for item in items] or ["ReadHub æš‚æ— æ•°æ®"]
    except Exception as e:
        return [f"ReadHub æŠ“å–å¤±è´¥: {e}"]

def search_platform(site_domain, site_name, query="AI å·¥å…·"):
    print(f"ğŸ” æ‰«æ {site_name}...")
    try:
        with DDGS(proxy=PROXY_URL) as ddgs:
            search_query = f"site:{site_domain} {query}"
            results = [r.get('title', '') for r in ddgs.text(search_query, region='cn-zh', timelimit='w', max_results=8) if r.get('title')]
        return results if results else [f"{site_name} æš‚æ— æ•°æ®"]
    except Exception as e:
        return [f"{site_name} æœç´¢å¤±è´¥: {e}"]

def scan_all_sources():
    all_titles = []
    github_data = get_github_trending()
    all_titles.extend(github_data)
    readhub_data = get_readhub_news()
    all_titles.extend(readhub_data)
    print("ğŸ” [3/6] æ‰«æå°çº¢ä¹¦...")
    xiaohongshu_data = search_platform("xiaohongshu.com", "å°çº¢ä¹¦", "AIå·¥å…· æ•™ç¨‹")
    all_titles.extend(xiaohongshu_data)
    print("ğŸ” [4/6] æ‰«æå¾®åš...")
    weibo_data = search_platform("weibo.com", "å¾®åš", "AI äººå·¥æ™ºèƒ½")
    all_titles.extend(weibo_data)
    print("ğŸ” [5/6] æ‰«æå°‘æ•°æ´¾...")
    sspai_data = search_platform("sspai.com", "å°‘æ•°æ´¾", "AI æ•ˆç‡ å·¥å…·")
    all_titles.extend(sspai_data)
    return {"github": github_data, "readhub": readhub_data, "xiaohongshu": xiaohongshu_data, 
            "weibo": weibo_data, "sspai": sspai_data, "all_titles": all_titles}

KEYWORD_PROMPT = """ä½ æ˜¯çƒ­ç‚¹åˆ†æå¸ˆã€‚ä»ä»¥ä¸‹æ ‡é¢˜ä¸­æå–å½“å‰ AI åœˆæœ€ç«çš„ 5 ä¸ªæŠ€æœ¯åè¯æˆ–è¯é¢˜ã€‚
åªè¾“å‡ºå…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”ã€‚ä¾‹å¦‚ï¼šDeepSeek R1, Cursor, MCP, RAG, AI Agent"""

def extract_hot_keywords(all_titles, http_client):
    print("\nğŸ§  [6/6] DeepSeek æ­£åœ¨åˆ†æçƒ­è¯...\n")
    client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL, http_client=http_client)
    titles_text = "\n".join(all_titles[:50])
    try:
        response = client.chat.completions.create(
            model="deepseek-chat", messages=[{"role": "system", "content": KEYWORD_PROMPT},
            {"role": "user", "content": f"æ ‡é¢˜ï¼š\n{titles_text}"}], temperature=0.3)
        keywords = [kw.strip() for kw in response.choices[0].message.content.strip().split(',') if kw.strip()]
        print(f"ğŸ“Š ä»Šæ—¥çƒ­è¯ï¼š{keywords}\n")
        return keywords[:5]
    except Exception as e:
        print(f"çƒ­è¯æå–å¤±è´¥: {e}")
        return ["DeepSeek", "AI Agent", "æ•ˆç‡å·¥å…·"]

def search_wechat_by_keywords(keywords):
    print("ğŸ” æœç´¢å¾®ä¿¡å…¬ä¼—å·ç«å“...\n")
    all_results = []
    for kw in keywords:
        print(f"  â”œâ”€ å…³é”®è¯: {kw}")
        try:
            with DDGS(proxy=PROXY_URL) as ddgs:
                results = [f"    â€¢ {r.get('title', '')}" for r in ddgs.text(f"site:mp.weixin.qq.com {kw}", region='cn-zh', timelimit='w', max_results=3)]
                all_results.append(f"ã€{kw}ã€‘\n" + ("\n".join(results) if results else "    â€¢ æš‚æ— "))
        except Exception as e:
            all_results.append(f"ã€{kw}ã€‘\n    â€¢ æœç´¢å¤±è´¥: {e}")
        time.sleep(0.5)
    return "\n\n".join(all_results)

EDITOR_PROMPT = """ä½ å«"ç‹å¾€AI"ï¼Œä¸“æ³¨ AI å·¥ä½œæµçš„ç¡¬æ ¸æŠ€æœ¯åšä¸»ã€‚
æ ¹æ®æƒ…æŠ¥ç­›é€‰ 3 ä¸ªæœ€å€¼å¾—å†™çš„é€‰é¢˜ã€‚è¾“å‡ºæ ¼å¼ï¼š
### é€‰é¢˜ 1ï¼š[æ ‡é¢˜]
* **çƒ­åº¦æ¥æº**ï¼š[æ¥æº]
* **æ¨èç†ç”±**ï¼š[ç†ç”±]
* **æ ¸å¿ƒçœ‹ç‚¹**ï¼š[çœ‹ç‚¹]
---
## ä»Šæ—¥ä¸»æ¨
å‘Šè¯‰æˆ‘æœ€åº”è¯¥å†™å“ªä¸ªã€‚"""

def final_decision(scan_data, hot_keywords, wechat_data, http_client):
    print("\n" + "="*50 + "\nğŸ“ DeepSeek ä¸»ç¼–å®¡æ ¸ä¸­...\n" + "="*50 + "\n")
    client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL, http_client=http_client)
    full_report = f"""=== GitHub ===\n{chr(10).join(scan_data['github'])}
=== ReadHub ===\n{chr(10).join(scan_data['readhub'])}
=== å°çº¢ä¹¦ ===\n{chr(10).join(scan_data['xiaohongshu'])}
=== å¾®åš ===\n{chr(10).join(scan_data['weibo'])}
=== å°‘æ•°æ´¾ ===\n{chr(10).join(scan_data['sspai'])}
=== çƒ­è¯ ===\n{', '.join(hot_keywords)}
=== å…¬ä¼—å·ç«å“ ===\n{wechat_data}"""
    try:
        response = client.chat.completions.create(
            model="deepseek-reasoner", messages=[{"role": "system", "content": EDITOR_PROMPT},
            {"role": "user", "content": full_report}], stream=True)
        print("\n" + "="*20 + " é€‰é¢˜æŠ¥å‘Š " + "="*20 + "\n")
        collected = []
        for chunk in response:
            if chunk.choices[0].delta.content:
                c = chunk.choices[0].delta.content
                print(c, end="", flush=True)
                collected.append(c)
        print("\n\n" + "="*50 + "\n")
        return full_report, "".join(collected)
    except Exception as e:
        print(f"å†³ç­–å¤±è´¥: {e}")
        return full_report, f"å¤±è´¥: {e}"

def save_report(raw_data, hot_keywords, analysis):
    filename = get_topic_report_file()
    content = f"# ğŸš€ é€‰é¢˜é›·è¾¾æŠ¥å‘Š\n\n**æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n**ç›®å½•**: {get_today_dir()}\n\n## çƒ­è¯\n> {', '.join(hot_keywords)}\n\n## æƒ…æŠ¥\n```\n{raw_data}\n```\n\n## åˆ†æ\n{analysis}"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(content)
    print(f"ğŸ“ æŠ¥å‘Šå·²ä¿å­˜: {filename}")

def main():
    print("\n" + "="*60 + "\nğŸš€ å…¨ç½‘é€‰é¢˜é›·è¾¾ v2.0 - ç‹å¾€AI\n" + "="*60 + "\n")
    print("ğŸ“¡ Step 1/4: å¹¿åŸŸæ‰«æ...\n")
    scan_data = scan_all_sources()
    print("\nğŸ“¡ Step 2/4: çƒ­è¯è’¸é¦...")
    with httpx.Client(proxy=PROXY_URL, timeout=REQUEST_TIMEOUT) as http_client:
        hot_keywords = extract_hot_keywords(scan_data['all_titles'], http_client)
        print("\nğŸ“¡ Step 3/4: ç«å“éªŒè¯...")
        wechat_data = search_wechat_by_keywords(hot_keywords)
        print("\nğŸ“¡ Step 4/4: æœ€ç»ˆå†³ç­–...")
        raw_data, analysis = final_decision(scan_data, hot_keywords, wechat_data, http_client)
    save_report(raw_data, hot_keywords, analysis)
    print("\nâœ… é€‰é¢˜é›·è¾¾å®Œæˆï¼")

if __name__ == "__main__":
    main()
