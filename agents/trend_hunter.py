"""
ğŸš€ å…¨ç½‘é€‰é¢˜é›·è¾¾ (Trend Hunter Agent) v4.0 - ç¡¬æ ¸ä»·å€¼ç‰ˆ
æ ¸å¿ƒç­–ç•¥ï¼š
1. ä¸‰çº§å®¹é”™æœºåˆ¶ï¼šJina Primary -> Jina Backup (RSS) -> Tavily Searchï¼Œç¡®ä¿æ•°æ®æºç¨³å®šã€‚
2. éšæœºåŒ–æ‰«æï¼šBè·¯(æ•ˆç‡)ä¸Cè·¯(é¿å‘)é‡‡ç”¨éšæœºæŠ½å–ç­–ç•¥ï¼Œé¿å…é‡å¤ã€‚
3. ä¸¥æ ¼å»é‡ï¼šåŸºäºå†å²è®°å½•çš„è‡ªåŠ¨å»é‡ä¸æ–°è¯æ‰¶æŒæœºåˆ¶ã€‚
"""
import sys, os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import time
import json
import httpx
import random
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from openai import OpenAI
from config import (
    DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL, PROXY_URL, REQUEST_TIMEOUT,
    TAVILY_API_KEY, get_topic_report_file, get_today_dir,
    get_stage_dir, get_research_notes_file, get_history_file
)

# ================= å†å²è®°å½•ç®¡ç† =================

def load_history():
    """åŠ è½½æœ€è¿‘ 7 å¤©çš„å†å²é€‰é¢˜"""
    history_file = get_history_file()
    if not os.path.exists(history_file):
        return []
    
    try:
        with open(history_file, "r", encoding="utf-8") as f:
            history = json.load(f)
            
        # è¿‡æ»¤å‡ºæœ€è¿‘ 7 å¤©çš„
        recent_history = []
        today = datetime.now()
        for item in history:
            date_str = item.get("date")
            try:
                item_date = datetime.strptime(date_str, "%Y-%m-%d")
                if (today - item_date).days <= 7:
                    recent_history.append(item)
            except:
                continue
        return recent_history
    except Exception:
        return []

def save_topic_to_history(topic, angle):
    """ä¿å­˜é€‰ä¸­é€‰é¢˜åˆ°å†å²è®°å½•"""
    history_file = get_history_file()
    history = []
    if os.path.exists(history_file):
        try:
            with open(history_file, "r", encoding="utf-8") as f:
                history = json.load(f)
        except:
            pass
            
    new_entry = {
        "date": datetime.now().strftime("%Y-%m-%d"),
        "topic": topic,
        "angle": angle
    }
    history.append(new_entry)
    
    # åªä¿ç•™æœ€è¿‘ 30 æ¡
    if len(history) > 30:
        history = history[-30:]
        
    with open(history_file, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)
    print(f"   ğŸ’¾ å†å²è®°å½•å·²æ›´æ–°: {topic}")

# ================= é…ç½®åŒº =================

# é•¿æœŸå…³æ³¨çŸ©é˜µ (æµé‡åŸºæœ¬ç›˜)
WATCHLIST = [
    # é¡¶æµæ¨¡å‹ (å›½é™…)
    "DeepSeek V3", "Claude 3.5", "Gemini 2.0", "GPT-4o", "Llama 3",
    # å›½å†…å¤§å‚ (æ–°å¢)
    "æ™ºè°± AI", "AutoGLM", "é€šä¹‰åƒé—® Qwen", "è±†åŒ…", "Kimi", "ç§˜å¡”æœç´¢",
    # çƒ­é—¨æŠ€æœ¯
    "MCPåè®®", "AI Agent", "RAG", "AI ç¼–ç¨‹", "AI è§†é¢‘ç”Ÿæˆ", "æ‰‹æœºæ™ºèƒ½ä½“",
    # ç¼–ç¨‹ç¥å™¨
    "Cursor", "Windsurf", "Bolt.new", "Lovable",
    # æ•ˆç‡æ ‡æ†
    "Notion", "Obsidian", "Heptabase"
]

# è¿è¥é˜¶æ®µé…ç½®
OPERATIONAL_PHASE = "VALUE_HACKER" # ä»·å€¼é»‘å®¢

PHASE_CONFIG = {
    "VALUE_HACKER": {
        "name": "ä»·å€¼é»‘å®¢æ¨¡å¼",
        "weights": {"news": 1.5, "social": 2.0, "github": 1.0}, # å¹³è¡¡æƒé‡ï¼šæå‡æ–°é—»æƒé‡ï¼Œç¡®ä¿ä¸æ¼å¤§äº‹ä»¶
        "strategy": "åˆ©ç”¨å¿ƒç†å­¦é”šç‚¹(æ”¶ç›Š/æŸå¤±)ï¼ŒæŒ–æ˜èƒ½ç»™ç”¨æˆ·å¸¦æ¥'è·å¾—æ„Ÿ'çš„é€‰é¢˜ã€‚",
        "prompt_suffix": "âš ï¸ ç»å¯¹åŸåˆ™ï¼šåƒä¸€ä¸ª'ç”Ÿæ´»é»‘å®¢'ä¸€æ ·æ€è€ƒã€‚ä½†å¿…é¡»å¯¹'é‡å¤§æŠ€æœ¯æ›´æ–°'ä¿æŒæ•æ„Ÿï¼ˆå¦‚æ–°æ¨¡å‹å‘å¸ƒï¼‰ã€‚å¦‚æœæ˜¯å·¥å…·ï¼Œå¿…é¡»æ˜¯æ™®é€šäººæ‰‹æœº/ç”µè„‘èƒ½è£…çš„ï¼›å¦‚æœæ˜¯æ•™ç¨‹ï¼Œå¿…é¡»æ˜¯å°ç™½èƒ½çœ‹æ‡‚çš„ã€‚"
    }
}

CURRENT_CONFIG = PHASE_CONFIG[OPERATIONAL_PHASE]

# ================= Tavily æœç´¢å·¥å…· =================

class WebSearchTool:
    def __init__(self):
        self.api_key = TAVILY_API_KEY
        self.enabled = bool(self.api_key and len(self.api_key) > 10)
        if self.enabled:
            print("   âœ… Tavily Search API å·²å¯ç”¨")
    
    def search(self, query, max_results=5, include_answer=False, topic=None, days=3):
        """Tavily æœç´¢ï¼Œå¼ºåˆ¶åªè¿”å›æœ€è¿‘ N å¤©çš„æ–°é—»"""
        if not self.enabled: return []
        print(f"   ğŸ” Tavily (æœ€è¿‘{days}å¤©): {query}")
        url = "https://api.tavily.com/search"
        payload = {
            "api_key": self.api_key,
            "query": query,
            "search_depth": "advanced",  # ä½¿ç”¨ advanced ä»¥æ”¯æŒæ—¶é—´è¿‡æ»¤
            "max_results": max_results,
            "include_answer": include_answer,
            "days": days                  # åªçœ‹æœ€è¿‘ N å¤©çš„çƒ­ç‚¹
        }
        if topic:
            payload["topic"] = topic
            
        try:
            # Tavily éœ€è¦ä»£ç† (å¦‚æœé…ç½®äº† PROXY_URL)
            # ä½¿ç”¨ trust_env=False é˜²æ­¢è¯»å–ç³»ç»Ÿç¯å¢ƒå˜é‡å¯¼è‡´æ··ä¹±ï¼Œæ˜¾å¼æŒ‡å®š proxy
            proxies = PROXY_URL if PROXY_URL else None
            with httpx.Client(timeout=30, proxy=proxies) as client:
                resp = client.post(url, json=payload)
                resp.raise_for_status()
                data = resp.json()
                results = []
                if data.get('answer'):
                    results.append({"title": "AI Summary", "body": data['answer'], "url": ""})
                for r in data.get('results', []):
                    results.append({
                        "title": r.get('title', ''),
                        "body": r.get('content', ''),
                        "url": r.get('url', '')
                    })
                return results
        except Exception as e:
            print(f"      âŒ æœç´¢å¤±è´¥: {e}")
            return []

# ================= è¾…åŠ©å‡½æ•° =================

def get_github_trending():
    print("   ğŸ” GitHub Trending (Weekly)...")
    url = "https://github.com/trending?since=weekly" # å…¨è¯­è¨€ Weeklyï¼ŒèŒƒå›´æ›´å¹¿
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        with httpx.Client(proxy=PROXY_URL, timeout=15) as client:
            resp = client.get(url, headers=headers)
        soup = BeautifulSoup(resp.text, 'html.parser')
        repos = soup.select('article.Box-row')
        results = []
        for repo in repos[:5]:
            name_tag = repo.select_one('h1 a') or repo.select_one('h2 a')
            if not name_tag: continue
            name = name_tag.get_text(strip=True).replace('\n', '').replace(' ', '')
            desc = repo.select_one('p').get_text(strip=True) if repo.select_one('p') else ""
            # è¿‡æ»¤æ‰éAI/å·¥å…·ç±»çš„ä»“åº“(ç®€å•å…³é”®è¯è¿‡æ»¤)
            results.append(f"- {name}: {desc}")
        return results
    except Exception as e:
        return [f"- GitHub æŠ“å–å¤±è´¥: {e}"]

# ================= çƒ­æ¦œåŠ¨æ€æŠ“å– =================

def fetch_dynamic_trends(client, search_tool=None):
    """
    ä»çƒ­æ¦œç½‘ç«™æŠ“å–å®æ—¶å…³é”®è¯ï¼ˆä¸‰çº§å®¹é”™æœºåˆ¶ï¼‰
    1. Jina Primary -> 2. Jina Backup (RSS) -> 3. Tavily Search
    """
    print("   ğŸŒ [çƒ­æ¦œæŠ“å–] ä»å…¨ç½‘çƒ­æ¦œè·å–å®æ—¶è¶‹åŠ¿...")
    
    sources = [
        # === å›½é™…ç¡¬æ ¸æº ===
        {
            "name": "Hacker News",
            "tag": "ç¡¬æ ¸æŠ€æœ¯",
            "primary": "https://news.ycombinator.com",
            "backup": "https://news.ycombinator.com/rss"
        },
        {
            "name": "Product Hunt",
            "tag": "æ•ˆç‡å·¥å…·æ–°å“",
            "primary": "https://www.producthunt.com",
            "backup": "https://www.producthunt.com/feed"
        },
        
        # === å›½å†…å¤§ä¼—/å®æˆ˜æº ===
        {
            "name": "çŸ¥ä¹çƒ­æ¦œ-ç§‘æŠ€",
            "tag": "AIè§‚ç‚¹ä¸äº‰è®®",
            "primary": "https://www.zhihu.com/hot/technology",
            "backup": "https://rsshub.app/zhihu/hotlist"
        },
        {
            "name": "æ˜é‡‘-åç«¯/AI",
            "tag": "ç¨‹åºå‘˜å®æˆ˜",
            "primary": "https://juejin.cn/hot/articles",
            "backup": "https://rsshub.app/juejin/trending/all/weekly"
        },
        {
            "name": "36Kr-ç§‘æŠ€",
            "tag": "ç§‘æŠ€å¤§ä¼—åŒ–/è¡Œä¸šåŠ¨æ€",
            "primary": "https://36kr.com/information/technology",
            "backup": "https://36kr.com/feed"
        },
        {
            "name": "å¾®åšçƒ­æœ-ç§‘æŠ€",
            "tag": "å¤§ä¼—èˆ†æƒ…/çªå‘",
            "primary": "https://s.weibo.com/top/summary?cate=scitech",
            "backup": "https://rsshub.app/weibo/search/hot"
        },
        {
            "name": "å°‘æ•°æ´¾",
            "tag": "ç”Ÿæ´»é»‘å®¢/æ•ˆç‡æ–¹æ³•è®º",
            "primary": "https://sspai.com/tag/%E6%95%88%E7%8E%87/hot",
            "backup": "https://sspai.com/feed"
        },
        {
            "name": "CSDNçƒ­æ¦œ",
            "tag": "æŠ€æœ¯æ•™ç¨‹/æŠ¥é”™è§£å†³",
            "primary": "https://blog.csdn.net/rank/list",
            "backup": ""  # CSDN æ— ç¨³å®š RSSï¼Œç•™ç©ºä¾é  Jina å¼ºè¯»
        }
    ]
    
    all_keywords = []
    
    for source in sources:
        # ä¼ å…¥ search_tool ç”¨äº Tavily å…œåº•
        content = _fetch_with_fallback(
            source["primary"], 
            source["backup"], 
            source["name"],
            search_tool
        )
        if content:
            # å¯¹æ¯ä¸ªæºå•ç‹¬æå–å…³é”®è¯ï¼ˆå¸¦é™å™ª Promptï¼‰
            keywords = _extract_keywords_from_single_source(
                client, 
                content, 
                source["name"], 
                source["tag"]
            )
            all_keywords.extend(keywords)
    
    if not all_keywords:
        print("      âš ï¸ æ‰€æœ‰çƒ­æ¦œæºæå–å…³é”®è¯å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨")
        return []
    
    # å»é‡å¹¶é™åˆ¶æ•°é‡
    unique_keywords = list(dict.fromkeys(all_keywords))[:10]
    print(f"   ğŸ”¥ [çƒ­æ¦œæ±‡æ€»] å®æ—¶å…³é”®è¯: {unique_keywords}")
    return unique_keywords


def _fetch_with_fallback(primary_url, backup_url, source_name, search_tool=None):
    """
    ä¸‰çº§è·å–ç­–ç•¥ï¼šJina Primary -> Jina Backup -> Tavily Search
    """
    jina_base = "https://r.jina.ai/"
    
    # 1. å°è¯• Jina Primary
    content = _fetch_via_jina(jina_base + primary_url, source_name, "primary")
    if content and len(content) >= 500:
        return content
    
    # 2. å°è¯• Jina Backup (RSS)
    if backup_url:
        print(f"      ğŸ”„ [{source_name}] Primary å¤±è´¥ï¼Œå°è¯• Backup (RSS)...")
        content = _fetch_via_jina(jina_base + backup_url, source_name, "backup")
        if content and len(content) >= 500:
            return content

    # 3. å°è¯• Tavily ç»ˆææ•‘æ´
    if search_tool and search_tool.enabled:
        print(f"      ğŸ›¡ï¸ [{source_name}] å¯ç”¨ Tavily ç»ˆææ•‘æ´...")
        # æ„é€ æœç´¢è¯
        query = f"{source_name} çƒ­é—¨ AI ç§‘æŠ€å†…å®¹ {datetime.now().strftime('%Y-%m-%d')}"
        results = search_tool.search(query, max_results=3, days=3)
        if results:
            # æ‹¼æ¥ Tavily çš„æœç´¢ç»“æœä½œä¸ºä¼ªé€ çš„"ç½‘é¡µå†…å®¹"
            combined_text = "\n".join([f"Title: {r['title']}\nSnippet: {r['body']}" for r in results])
            print(f"      âœ… [{source_name}] Tavily æ•‘æ´æˆåŠŸ: æŠ“å– {len(results)} æ¡ç»“æœ")
            return combined_text
            
    print(f"      âŒ [{source_name}] æ‰€æœ‰é€šé“å‡å¤±è´¥")
    return None


def _fetch_via_jina(url, source_name, url_type):
    """
    é€šè¿‡ Jina Reader API è·å–ç½‘é¡µå†…å®¹
    """
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "x-no-cache": "true"  # å¼ºåˆ¶ Jina Reader æŠ“å–æœ€æ–°é¡µé¢ï¼Œä¸è¿”å›ç¼“å­˜
        }
        with httpx.Client(proxy=PROXY_URL, timeout=30) as client:
            resp = client.get(url, headers=headers)
            
            if resp.status_code != 200:
                print(f"      âš ï¸ [{source_name}] {url_type} çŠ¶æ€ç : {resp.status_code}")
                return None
            
            content = resp.text
            if len(content) < 500:
                print(f"      âš ï¸ [{source_name}] {url_type} å†…å®¹è¿‡çŸ­: {len(content)} å­—ç¬¦")
                return None
            
            print(f"      âœ… [{source_name}] {url_type} æˆåŠŸ: {len(content)} å­—ç¬¦")
            return content[:8000]  # é™åˆ¶é•¿åº¦ï¼Œé¿å… token è¿‡å¤š
            
    except httpx.TimeoutException:
        print(f"      âš ï¸ [{source_name}] {url_type} è¶…æ—¶")
        return None
    except Exception as e:
        print(f"      âš ï¸ [{source_name}] {url_type} å¼‚å¸¸: {e}")
        return None


def _extract_keywords_from_single_source(client, content, name, tag):
    """
    ä½¿ç”¨ LLM ä»å•ä¸ªçƒ­æ¦œæºä¸­æå–å…³é”®è¯ï¼ˆå¸¦ä¸¥æ ¼é™å™ªè¿‡æ»¤ï¼‰
    """
    if not content:
        return []
    
    # é™åˆ¶å†…å®¹é•¿åº¦ï¼ˆä¿ç•™è¾ƒå¤šå†…å®¹ä»¥è¦†ç›–çƒ­æ¦œå‰50åï¼‰
    content_truncated = content[:8000]
    
    prompt = f"""
è¿™æ˜¯ã€{name}ã€‘ä»Šå¤©çš„çƒ­æ¦œæˆ–æœç´¢æ‘˜è¦ã€‚
è¯·ä»ä¸­æå– 2-3 ä¸ªæœ€ç¬¦åˆ"{tag}"é¢†åŸŸçš„å…·ä½“æŠ€æœ¯åè¯æˆ–äº§å“åç§°ã€‚

âš ï¸ å…³é”®è¿‡æ»¤è§„åˆ™ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š
1. ğŸ”´ **ç»å¯¹æ’é™¤åº•å±‚æŠ€æœ¯**ï¼šä¸¥ç¦æå– åç«¯æ¡†æ¶(Spring Boot/Django)ã€æ•°æ®åº“(Redis/SQL)ã€è¿ç»´(K8s/Docker)ã€åº•å±‚é©±åŠ¨(CUDA/NATS)ã€ç¼–ç¨‹è¯­è¨€ç‰ˆæœ¬(Java 21/Vite 8)ã€‚**æˆ‘ä»¬åªè¦ç»™å°ç™½ç”¨çš„å·¥å…·ï¼**
2. ğŸŸ¢ **åªä¿ç•™åº”ç”¨å±‚**ï¼š
   - AI åº”ç”¨/å¤§æ¨¡å‹ (DeepSeek, Kimi, Claude 4.5, Sora)
   - æ•ˆç‡å·¥å…· (Notion, Cursor, Obsidian, Arcæµè§ˆå™¨)
   - è½åœ°ç©æ³• (AIåšPPT, æ™ºèƒ½ä½“å¼€å‘, æœ¬åœ°éƒ¨ç½²)
   - è¡Œä¸šçƒ­ç‚¹ (AIçœ¼é•œ, å…·èº«æ™ºèƒ½)
3. æ’é™¤å¨±ä¹æ˜æ˜Ÿå’Œç¤¾ä¼šæ–°é—»ã€‚
4. å¦‚æœé¡µé¢æ˜¯ RSS XML æ ¼å¼ï¼Œè¯·å¿½ç•¥ XML æ ‡ç­¾ï¼Œåªæå– Title ä¸­çš„æŠ€æœ¯åè¯ã€‚
5. è¿”å›æ ¼å¼ï¼šåªè¿”å›åè¯ï¼Œç”¨è‹±æ–‡é€—å·åˆ†éš”ã€‚å¦‚æœä¸ç¡®å®šæˆ–æ— ç›¸å…³å†…å®¹ï¼Œè¿”å› "NONE"ã€‚
6. ä¼˜å…ˆæå–**çŸ¥åç§‘æŠ€å…¬å¸**ï¼ˆå¦‚æ·±åº¦æ±‚ç´¢ï¼Œæ™ºè°±, å­—èŠ‚, è…¾è®¯ã€é˜¿é‡Œã€OpenAIï¼ŒGoogle ï¼ŒClaude ï¼ŒBing ï¼Œæœˆä¹‹æš—é¢ï¼Œè®¯é£ï¼Œç™¾åº¦ï¼Œå¾®è½¯ï¼Œè‹¹æœï¼Œå°çº¢ä¹¦ï¼‰å‘å¸ƒçš„**æ–°äº§å“åç§°**ï¼ˆå¦‚ AutoGLM, Soraï¼‰ï¼Œé™ä½å¯¹ä¸çŸ¥åå°å·¥å…·çš„æå–æƒé‡ã€‚

ç¤ºä¾‹ï¼š
âŒ é”™è¯¯ï¼šSpring Boot, MySQL, React Hooks
âœ… æ­£ç¡®ï¼šDeepSeek, Cursor, ç§˜å¡”æœç´¢
"""
    
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ•é”çš„æŠ€æœ¯è¶‹åŠ¿æ•æ‰‹ï¼Œæ“…é•¿ä»æ‚ä¹±çš„ç½‘é¡µå†…å®¹ä¸­æå–æœ‰ä»·å€¼çš„æŠ€æœ¯å…³é”®è¯ï¼Œå¹¶è¿‡æ»¤æ‰æ— å…³çš„å¨±ä¹å…«å¦ã€‚"},
                {"role": "user", "content": f"ã€{name} çƒ­æ¦œå†…å®¹ã€‘\n{content_truncated}\n\n{prompt}"}
            ],
            temperature=0.2
        )
        result = response.choices[0].message.content.strip()
        
        # å¤„ç† NONE æƒ…å†µ
        if result.upper() == "NONE" or "NONE" in result.upper():
            print(f"      â­ï¸ [{name}] æ— ç›¸å…³æŠ€æœ¯å†…å®¹ï¼Œè·³è¿‡")
            return []
        
        # æ¸…æ´—å¹¶è¿”å›
        keywords = [k.strip() for k in result.split(',') if k.strip() and len(k.strip()) < 30]
        keywords = keywords[:3]  # æ¯ä¸ªæºæœ€å¤š3ä¸ª
        
        if keywords:
            print(f"      ğŸ“Œ [{name}] æå–: {keywords}")
        return keywords
        
    except Exception as e:
        print(f"      âš ï¸ [{name}] å…³é”®è¯æå–å¤±è´¥: {e}")
        return []


def extract_hot_entities(client, search_results):
    """ä»æœç´¢ç»“æœä¸­æå– 2-3 ä¸ªçƒ­é—¨æŠ€æœ¯åè¯"""
    if not search_results: return []
    
    text = "\n".join([f"- {r['title']}" for r in search_results[:10]]) # é™åˆ¶è¾“å…¥é•¿åº¦
    prompt = """
    è¯·ä»ä¸Šè¿°æ–°é—»æ ‡é¢˜ä¸­ï¼Œæå– 2-3 ä¸ªå½“å‰æœ€ç«çš„ AI æŠ€æœ¯æˆ–äº§å“åç§°ã€‚
    è¦æ±‚ï¼š
    1. åªè¿”å›å…·ä½“åè¯ï¼Œå¦‚ "DeepSeek V3", "MCP", "Sora 2.0"ã€‚
    2. ä¸è¦è¿”å›é€šç”¨è¯ï¼ˆå¦‚ "AI", "LLM", "Technology"ï¼‰ã€‚
    3. è¾“å‡ºæ ¼å¼ï¼šç”¨è‹±æ–‡é€—å·åˆ†éš”ï¼Œä¸è¦å…¶ä»–åºŸè¯ã€‚
    """
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ•é”çš„æŠ€æœ¯è¶‹åŠ¿æ•æ‰‹ã€‚"},
                {"role": "user", "content": f"ã€æ–°é—»æ ‡é¢˜åˆ—è¡¨ã€‘\n{text}\n\n{prompt}"}
            ],
            temperature=0.1
        )
        content = response.choices[0].message.content.strip()
        # ç®€å•æ¸…ç†
        entities = [e.strip() for e in content.split(',') if e.strip() and len(e.strip()) < 20]
        return entities[:3]
    except Exception as e:
        print(f"      âš ï¸ çƒ­ç‚¹æå–å¤±è´¥: {e}")
        return []

# ================= æ ¸å¿ƒé€»è¾‘ =================

def get_plan_prompt(history_text=""):
    """åŠ¨æ€ç”Ÿæˆè§„åˆ’æç¤ºè¯ï¼Œæ³¨å…¥å½“å‰æ—¥æœŸå’Œå†å²è®°å½•"""
    today = datetime.now().strftime('%Y-%m-%d')
    return f"""
ğŸ“… ä»Šå¤©æ˜¯ {today}ã€‚ä½ å¿…é¡»åªå…³æ³¨æœ€è¿‘ 3-7 å¤©å†…å‘ç”Ÿçš„ AI åœˆæœ€æ–°å¤§äº‹ä»¶ã€‚
â— ç»å¯¹ç¦æ­¢æŠ¥é“ 2024 å¹´æˆ–æ›´æ—©çš„æ—§é—»ï¼ˆå¦‚ DeepSeek R1ã€GPT-4 å‘å¸ƒç­‰å†å²äº‹ä»¶ï¼‰ã€‚

ã€å†å²å‘æ–‡è®°å½• (æœ€è¿‘7å¤©)ã€‘
{history_text}
âš ï¸ æŸ¥é‡æŒ‡ä»¤ï¼šå¦‚æœä¸Šè¿°å†å²è®°å½•ä¸­å·²å­˜åœ¨ç›¸ä¼¼é€‰é¢˜ï¼Œè¯·å¿…é¡»è°ƒæ•´åˆ‡å…¥è§’åº¦ï¼ˆä¾‹å¦‚ï¼šä»"æ–°é—»æŠ¥é“"è½¬å‘"æ·±åº¦å®æµ‹"æˆ–"é¿å‘æŒ‡å—"ï¼‰ã€‚å¦‚æœæ— æ³•å·®å¼‚åŒ–ï¼Œè¯·ç›´æ¥ä¸¢å¼ƒè¯¥é€‰é¢˜ã€‚

ä½ æ˜¯â€œç‹å¾€AIâ€çš„é¦–å¸­å†…å®¹ç­–ç•¥å®˜ã€‚
è¯·åŸºäºã€å…¨ç½‘æƒ…æŠ¥ã€‘å’Œã€å¿ƒç†å­¦ç­–ç•¥ã€‘ï¼ŒæŒ–æ˜ 3 ä¸ªæœ€å…·â€œçˆ†æ¬¾æ½œè´¨â€çš„é€‰é¢˜æ–¹å‘ã€‚

å¿ƒç†å­¦ç­–ç•¥ï¼š
1. **Aè·¯ (é”šç‚¹æ•ˆåº”)**: å€ŸåŠ¿é¡¶æµ (DeepSeek/Kimi)ï¼Œå…³æ³¨å…¶"éšè—åŠŸèƒ½"æˆ–"æœ€æ–°ç©æ³•"ã€‚
2. **Bè·¯ (å³æ—¶æ»¡è¶³)**: å¯»æ‰¾"æ•ˆç‡ç¥å™¨"ã€"Life Hack"ï¼Œä¸»æ‰“"3åˆ†é’Ÿä¸Šæ‰‹"ã€"ä¸‹ç­æ—©èµ°1å°æ—¶"ã€‚
3. **Cè·¯ (æŸå¤±åŒæ¶)**: å¯»æ‰¾"é¿å‘æŒ‡å—"ã€"æ™ºå•†ç¨"ã€"å¹³æ›¿"ã€"ç¿»è½¦ç°åœº"ï¼Œå¼•å‘ç”¨æˆ·å±æœºæ„Ÿã€‚

è¾“å…¥æ•°æ®ï¼š
- é•¿æœŸå…³æ³¨å“ç±»åŠ¨æ€
- æœ¬å‘¨çƒ­é—¨å·¥å…·/æ•™ç¨‹
- ç”¨æˆ·åæ§½ä¸ç—›ç‚¹

å†³ç­–æ ‡å‡†ï¼š
- âœ… **ä¿ç•™**ï¼šDeepSeek è”ç½‘æœç´¢æ€ä¹ˆç”¨æ‰å‡†ã€Cursor å…è´¹é¢åº¦æ²¡äº†æ€ä¹ˆåŠã€å¤¸å…‹æ‰«æç‹å¯¹æ¯”ã€‚
- âŒ **å‰”é™¤**ï¼šOpenAI èèµ„æ¶ˆæ¯ã€Google å‘å¸ƒæ–°è®ºæ–‡ã€æŸæŸè¡Œä¸šå¤§æ¨¡å‹ç™½çš®ä¹¦ã€‚

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼ JSONï¼‰ï¼š
[
    {{
        "event": "é€‰é¢˜æ ¸å¿ƒè¯ (å¦‚: DeepSeek)",
        "angle": "åˆ‡å…¥è§’åº¦ (å¦‚: éšè—ç©æ³• / é¿å‘æŒ‡å—)",
        "news_query": "åŠŸèƒ½æ€§æœç´¢è¯ (å¦‚: DeepSeek V3 file upload)",
        "social_query": "æƒ…ç»ªæ€§æœç´¢è¯ (å¦‚: DeepSeek æŠ¥é”™ / DeepSeek ä¸å¥½ç”¨)"
    }},
    ...
]
"""

# ä¿ç•™å†å²å…¼å®¹æ€§
PLAN_PROMPT = get_plan_prompt()

def step1_broad_scan_and_plan(client, search_tool):
    """Step 1: å¹¿åŸŸä»·å€¼æ‰«æ (å¿ƒç†å­¦ä¸‰è·¯ç­–ç•¥ + å…¨ç½‘é›·è¾¾)"""
    print(f"\nğŸ“¡ [Step 1] å¹¿åŸŸä»·å€¼æ‰«æ (ç­–ç•¥: {CURRENT_CONFIG['name']})...")
    
    pre_scan_results = []
    
    # === Phase 0: å…¨ç½‘é›·è¾¾ (Global Radar) ===
    # ç ´é™¤ä¿¡æ¯èŒ§æˆ¿ï¼Œä¸»åŠ¨å—…æ¢ä¸åœ¨ WATCHLIST é‡Œçš„æ–°é»‘é©¬
    print(f"   ğŸŒ‘ [Phase 0] å…¨ç½‘é›·è¾¾æ‰«æ (å‘ç°æ–°ç‰©ç§)...")
    radar_queries = [
        "site:reddit.com/r/LocalLLaMA AI news today", # ç¡¬æ ¸ç¤¾åŒº
        "site:news.ycombinator.com AI launch",        # ç¡…è°·é£å‘æ ‡
        "site:huggingface.co/papers trending",        # å­¦æœ¯å‰æ²¿
        "AI technology breaking news today"           # å¤§ä¼—æ–°é—»
    ]
    for q in radar_queries:
        res = search_tool.search(q, max_results=2, topic="news", days=1) # åªçœ‹24å°æ—¶å†…
        pre_scan_results.extend(res)

    # === Phase 0.5: çƒ­ç‚¹æå– ===
    hot_entities = extract_hot_entities(client, pre_scan_results)
    if hot_entities:
        print(f"   ğŸ”¥ [é›·è¾¾é”å®š] çªå‘çƒ­ç‚¹: {hot_entities}")

    # === Phase 0.6: çƒ­æ¦œåŠ¨æ€è¶‹åŠ¿ ===
    fresh_keywords = []
    try:
        fresh_keywords = fetch_dynamic_trends(client, search_tool)
    except Exception as e:
        print(f"      âš ï¸ çƒ­æ¦œæŠ“å–å¼‚å¸¸ï¼Œè·³è¿‡: {e}")
    
    # === Aè·¯: é¡¶æµé”šç‚¹ (Watchlist + Hotspots + Fresh) ===
    # éšæœºé€‰ 3 ä¸ªé¡¶æµ
    targets = random.sample(WATCHLIST, 3)
    
    # å°†çƒ­æ¦œå…³é”®è¯åŠ å…¥ targets (æœ€é«˜ä¼˜å…ˆçº§)
    for fk in fresh_keywords:
        if not any(fk.lower() in t.lower() for t in targets):
            targets.insert(0, fk)
    
    # å°†çƒ­ç‚¹åŠ å…¥ targets (ä¼˜å…ˆä¾¦å¯Ÿ)
    for h in hot_entities:
        # ç®€å•å»é‡ï¼šå¦‚æœ target é‡Œæ²¡æœ‰ç±»ä¼¼çš„å­—ç¬¦ä¸²
        if not any(h.lower() in t.lower() for t in targets):
            targets.insert(0, h)
            
    # é™åˆ¶æ‰«ææ•°é‡ï¼Œé¿å…è¿‡è½½
    targets = targets[:6]

    print(f"   ğŸ¯ [Aè·¯-é”šç‚¹] æ‰«æç›®æ ‡: {targets}")
    for t in targets:
        # æ¿€æ´»åƒµå°¸å…³é”®è¯ï¼šåŒæ—¶æœ"éšè—åŠŸèƒ½"å’Œ"æœ€æ–°æ›´æ–°"
        queries = [
            f"{t} éšè—åŠŸèƒ½ ç©æ³• æ•™ç¨‹ 2025",
            f"{t} new features latest update" # è‹±æ–‡æœæ›´æ–°å¾€å¾€æ›´å‡†
        ]
        for q in queries:
            res = search_tool.search(q, max_results=1, topic="news", days=3)
            pre_scan_results.extend(res)
        
    # === Bè·¯: éšæœºæ”¶ç›Šåœºæ™¯ (Life Hack) ===
    print(f"   âš¡ [Bè·¯-æ”¶ç›Š] æ‰«ææ•ˆç‡ç¥å™¨...")
    efficiency_keywords = [
        "AI æ•´ç†å¾ˆå¤šæ–‡ä»¶", "AI è‡ªåŠ¨å†™å‘¨æŠ¥", "AI è¯»é•¿è®ºæ–‡", "AI åšæ¼‚äº®çš„PPT", 
        "Excel AI å…¬å¼", "Notion æ›¿ä»£å“", "Obsidian æ’ä»¶", "æµè§ˆå™¨ AI æ’ä»¶",
        "è‡ªåŠ¨åŒ–å·¥ä½œæµ Zapier", "AI å‰ªè¾‘è§†é¢‘", "AI å½•éŸ³è½¬æ–‡å­— å…è´¹"
    ]
    selected_efficiency = random.sample(efficiency_keywords, 3)
    print(f"      ğŸ² éšæœºæŠ½å–: {selected_efficiency}")
    for kw in selected_efficiency:
        # Bè·¯: å¼ºåˆ¶è¿½åŠ é«˜è´¨é‡ä¿¡æºï¼Œè¿‡æ»¤ SEO åƒåœ¾
        q = f"{kw} æ¨è site:sspai.com OR site:36kr.com OR site:v2ex.com OR site:zhihu.com"
        res = search_tool.search(q, max_results=2, days=3)
        pre_scan_results.extend(res)
        
    # === Cè·¯: éšæœºé¿å‘åœºæ™¯ (Pain Points) ===
    print(f"   ğŸ›¡ï¸ [Cè·¯-æŸå¤±] æ‰«æé¿å‘/åæ§½...")
    pain_keywords = [
        "AI å†™ä½œ æŸ¥é‡", "AI å¹»è§‰ ç¿»è½¦", "æ”¶è´¹ AI é¿å‘", "AI ç”Ÿæˆå›¾ç‰‡ ä¸‘",
        "DeepSeek æŠ¥é”™", "ChatGPT å°å·", "Cursor å¤ªè´µ", "Copilot ä¸å¥½ç”¨"
    ]
    selected_pain = random.sample(pain_keywords, 3)
    print(f"      ğŸ² éšæœºæŠ½å–: {selected_pain}")
    for kw in selected_pain:
        # Cè·¯: å¼ºåˆ¶è¿½åŠ ç¤¾åŒºä¿¡æº
        q = f"{kw} åæ§½ é¿å‘ site:v2ex.com OR site:reddit.com OR site:zhihu.com"
        res = search_tool.search(q, max_results=2, days=3)
        pre_scan_results.extend(res)
    
    pre_scan_text = "\n".join([f"- {r['title']}: {r['body'][:80]}" for r in pre_scan_results])
    
    # 2. æ™ºèƒ½ç­›é€‰ä¸è§„åˆ’
    print(f"   ğŸ“ æƒ…æŠ¥èšåˆå®Œæ¯•ï¼ŒDeepSeek æ­£åœ¨åº”ç”¨å¿ƒç†å­¦ç­–ç•¥é€‰é¢˜...")
    
    # åŠ è½½å†å²è®°å½•
    history = load_history()
    history_text = "\n".join([f"- {h['date']}: {h['topic']} ({h['angle']})" for h in history])
    if not history_text: history_text = "æ— ï¼ˆè¿™æ˜¯ç¬¬ä¸€ç¯‡ï¼‰"

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": get_plan_prompt(history_text)},
                {"role": "user", "content": f"ã€æ··åˆæƒ…æŠ¥æ± ã€‘\n{pre_scan_text}"}
            ],
            temperature=0.7,
            response_format={ "type": "json_object" }
        )
        content = response.choices[0].message.content
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        
        search_plan = json.loads(content)
        if isinstance(search_plan, dict) and "events" in search_plan:
            search_plan = search_plan["events"]
            
        print(f"   ğŸ§  é€‰é¢˜æ–¹å‘å·²é”å®š: {[i['event'] + '-' + i['angle'] for i in search_plan]}\n")
        return search_plan
    except Exception as e:
        print(f"   âŒ è§„åˆ’å¤±è´¥: {e}")
        return [{"event": "DeepSeek", "angle": "é¿å‘", "news_query": "DeepSeek V3", "social_query": "DeepSeek å¹»è§‰"}]

def step2_deep_scan(search_plan, search_tool):
    """Step 2: æ·±åº¦éªŒè¯ (é‡ç¤¾äº¤/ç—›ç‚¹)"""
    print("ğŸ“¡ [Step 2] å¯åŠ¨æ·±åº¦ä»·å€¼éªŒè¯...\n")
    all_results = []
    
    w_news = CURRENT_CONFIG['weights']['news']
    w_social = CURRENT_CONFIG['weights']['social']
    
    for item in search_plan:
        event = item.get("event", "æœªçŸ¥")
        angle = item.get("angle", "é€šç”¨")
        news_q = item.get("news_query", "")
        social_q = item.get("social_query", "")
        
        print(f"   ğŸ” æ­£åœ¨æ·±æŒ–: ã€{event}ã€‘ ({angle}æ–¹å‘)")
        event_data = [f"=== é€‰é¢˜: {event} ({angle}) ==="]
        
        # 1. ç¤¾äº¤/ç—›ç‚¹æœç´¢ (æ ¸å¿ƒ)
        if social_q:
            print(f"      ğŸ’¬ ç¤¾äº¤èˆ†æƒ… (æƒé‡ {w_social}): {social_q}")
            # å¢åŠ çŸ¥ä¹ã€Bç«™(site:bilibili.com)
            full_social_q = f"{social_q} site:mp.weixin.qq.com OR site:xiaohongshu.com OR site:zhihu.com OR site:bilibili.com"
            res = search_tool.search(full_social_q, max_results=4)
            if res:
                event_data.append(f"--- ç”¨æˆ·çœŸå®åé¦ˆ ({social_q}) ---")
                event_data.extend([f"- {r['title']}: {r['body'][:80]}..." for r in res])
                
        # 2. å®˜æ–¹éªŒè¯ (è¾…åŠ©)
        if news_q:
            print(f"      ğŸ”¥ å®˜æ–¹éªŒè¯ (æƒé‡ {w_news}): {news_q}")
            res = search_tool.search(news_q, max_results=2)
            if res:
                event_data.append(f"--- å®˜æ–¹ä¿¡æ¯ ({news_q}) ---")
                event_data.extend([f"- {r['title']}" for r in res])
        
        all_results.append("\n".join(event_data))
        print("")
        time.sleep(1)

    # GitHub è¡¥å…… (Weekly)
    print(f"   ğŸ’» GitHub Weekly Trending...")
    github_res = get_github_trending()
    all_results.append("=== GitHub Weekly Trending ===\n" + "\n".join(github_res))
    
    return "\n\n".join(all_results)

def step3_final_decision(scan_data, client, history_text="æ— ï¼ˆè¿™æ˜¯ç¬¬ä¸€ç¯‡ï¼‰"):
    """Step 3: å†³ç­–ï¼ˆå¸¦å»é‡å’Œæ–°è¯æ‰¶æŒï¼‰"""
    print("\n" + "="*50 + "\nğŸ“ DeepSeek ä¸»ç¼–å®¡æ ¸ä¸­...\n" + "="*50)
    
    prompt = f"""
    {EDITOR_PROMPT}
    
    âŒ **ä¸¥æ ¼å»é‡**ï¼šä»¥ä¸‹æ˜¯æœ€è¿‘å·²å†™è¿‡çš„é€‰é¢˜ï¼š
    {history_text}
    
    **ç»å¯¹ç¦æ­¢**å†æ¬¡é€‰æ‹©ä¸ä¸Šè¿°æå…¶ç›¸ä¼¼çš„é€‰é¢˜ï¼å¿…é¡»æ¢ä¸ªå·¥å…·æˆ–æ¢ä¸ªè§’åº¦ï¼
    
    âœ¨ **æ‰¶æŒæ–°è¯**ï¼šè¯·ä¼˜å…ˆå…³æ³¨æƒ…æŠ¥ä¸­æåˆ°çš„ã€ç”Ÿåƒ»æŠ€æœ¯åè¯ã€‘ï¼ˆå¦‚ AutoGLM, Dayflow ç­‰ï¼‰ï¼Œå¦‚æœå®ƒä»¬æœ‰ä»·å€¼ï¼Œä¼˜å…ˆå…¥é€‰ã€‚
    
    å½“å‰ç­–ç•¥ï¼šã€{CURRENT_CONFIG['name']}ã€‘
    {CURRENT_CONFIG['prompt_suffix']}
    """
    
    try:
        # å•æ¬¡æ‰«æç”¨ chat æ¨¡å‹ï¼ˆå¿«ã€ä¾¿å®œï¼‰ï¼Œç»¼åˆå†³ç­–æ‰ç”¨ reasoner
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": f"ã€æ·±åº¦éªŒè¯æƒ…æŠ¥ã€‘\n{scan_data}"}
            ],
            stream=True
        )
        
        print("\n" + "="*20 + " é€‰é¢˜æŠ¥å‘Š " + "="*20 + "\n")
        collected = []
        for chunk in response:
            if chunk.choices[0].delta.content:
                c = chunk.choices[0].delta.content
                print(c, end="", flush=True)
                collected.append(c)
        return "".join(collected)
    except Exception as e:
        print(f"âŒ å†³ç­–å¤±è´¥: {e}")
        return f"å¤±è´¥: {e}"

EDITOR_PROMPT = """
ä½ å«"ç‹å¾€AI"ï¼Œä¸“æ³¨ AI å·¥ä½œæµçš„ç¡¬æ ¸åšä¸»ã€‚
è¯·ç­›é€‰ 3 ä¸ªã€è·å¾—æ„Ÿæœ€é«˜ã€‘çš„é€‰é¢˜ã€‚

**è·å¾—æ„Ÿå…¬å¼** = (å¸®ç”¨æˆ·è§£å†³çš„ç—›ç‚¹ * èŠ‚çœçš„æ—¶é—´/é‡‘é’±) - é˜…è¯»é—¨æ§›

ğŸ›¡ï¸ **è´¨é‡è¿‡æ»¤çº¢çº¿**ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š
1. **æ‹’ç»é‡é¸¡å·¥å…·**ï¼šéå¤§å‚/éå¼€æºçš„å°ä¼—å·¥å…·ç›´æ¥å‰”é™¤ï¼Œå°¤å…¶æ˜¯é‚£äº›ä¸çŸ¥åçš„ä»˜è´¹å¥—å£³ç½‘ç«™ã€‚
2. **å¤§å‚æ–°åŠ¨ä½œä¼˜å…ˆ**ï¼šå¦‚æœæ™ºè°±ã€OpenAIã€DeepSeek æœ‰æ–°åŠ¨ä½œï¼Œä¼˜å…ˆçº§æœ€é«˜ã€‚
3. **å¼€æºä¼˜å…ˆ**ï¼šGitHub ä¸Šçš„é«˜æ˜Ÿå¼€æºé¡¹ç›®ä¼˜å…ˆçº§é«˜äºé—­æºä»˜è´¹å·¥å…·ã€‚

å†³ç­–é€»è¾‘ï¼š
1. **åªåšäººè¯**ï¼šæ‹’ç»æ‰€æœ‰æŠ€æœ¯é»‘è¯ï¼ŒæŠŠ"ä¸Šä¸‹æ–‡ç¼“å­˜"ç¿»è¯‘æˆ"è®©AIè®°ä½ä½ ä¸Šå‘¨è¯´äº†å•¥"ã€‚
2. **åªåšç—›ç‚¹**ï¼šä¼˜å…ˆé€‰"é¿å‘"ã€"å¹³æ›¿"ã€"ç™½å«–"ã€"ææ•ˆ"ç±»é€‰é¢˜ã€‚
3. **å…³è”çƒ­ç‚¹**ï¼šå¦‚æœæ¶‰åŠ WATCHLIST ä¸­çš„äº§å“ï¼ŒåŠ åˆ†ã€‚

è¾“å‡ºæ ¼å¼ï¼š
### é€‰é¢˜ 1ï¼š[æ ‡é¢˜] (éœ€æå…·å¸å¼•åŠ›ï¼Œå¦‚ï¼šDeepSeek å±…ç„¶è¿˜èƒ½è¿™ä¹ˆç©ï¼Ÿ)
* **è·å¾—æ„Ÿ**ï¼š[ç”¨æˆ·çœ‹å®Œèƒ½å¾—åˆ°ä»€ä¹ˆï¼Ÿçœé’±ï¼Ÿçœæ—¶ï¼Ÿ]
* **å¿ƒç†é”šç‚¹**ï¼š[åˆ©ç”¨äº†ä»€ä¹ˆå¿ƒç†ï¼Ÿè´ªä¾¿å®œï¼Ÿæ€•è½åï¼Ÿ]
* **æ ¸å¿ƒçœ‹ç‚¹**ï¼š[æ–‡ç« å¤§çº²ï¼ŒåŒ…å«å…·ä½“çš„å·¥å…·/æŠ€å·§]
---
## ä»Šæ—¥ä¸»æ¨
å‘Šè¯‰æˆ‘ä¸å†™ä¼šåæ‚”çš„é‚£ä¸ª (è·å¾—æ„Ÿæœ€å¼ºçš„)ã€‚
"""

def auto_init_workflow():
    """è‡ªåŠ¨åˆå§‹åŒ–åç»­å·¥ä½œæµæ–‡ä»¶å¤¹å’Œæ–‡ä»¶"""
    print("\nâš™ï¸ æ­£åœ¨åˆå§‹åŒ–åç»­å·¥ä½œæµ...")
    
    # 1. é¢„åˆ›å»ºæ‰€æœ‰é˜¶æ®µæ–‡ä»¶å¤¹
    from config import get_stage_dir, get_research_notes_file
    stages = ["research", "drafts", "publish", "assets"]
    for stage in stages:
        path = get_stage_dir(stage)
        print(f"   ğŸ“‚ ç›®å½•å°±ç»ª: {path}")
        
    # 2. åˆ›å»ºç©ºç™½ç ”ç©¶ç¬”è®°
    notes_file = get_research_notes_file()
    if not os.path.exists(notes_file):
        with open(notes_file, "w", encoding="utf-8") as f:
            f.write("# ç ”ç©¶ç¬”è®°\n\nè¯·å°† NotebookLM ç”Ÿæˆçš„ Briefing Doc ç²˜è´´åœ¨è¿™é‡Œ...\n")
        print(f"   ğŸ“„ ç¬”è®°æ–‡ä»¶å·²åˆ›å»º: {notes_file}")
    
    # 3. æç¤ºä¸‹ä¸€æ­¥
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥ï¼š")
    print("   - å¯ç»§ç»­è¿è¡Œ hunt è·å–æ›´å¤šé€‰é¢˜")
    print("   - æˆ–è¿è¡Œ `python run.py final` ç»¼åˆæ‰€æœ‰æŠ¥å‘Šï¼Œè·å¾— 3 ä¸ªæç¤ºè¯")

def save_report(raw_data, analysis):
    filename = get_topic_report_file()
    content = f"# ğŸš€ é€‰é¢˜é›·è¾¾æŠ¥å‘Š v7.0 ({CURRENT_CONFIG['name']})\n\n**æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n**ç­–ç•¥**: {CURRENT_CONFIG['strategy']}\n\n## æ·±åº¦éªŒè¯æƒ…æŠ¥\n{raw_data}\n\n## é€‰é¢˜åˆ†æ\n{analysis}"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(content)
    print(f"\n\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜: {filename}")
    
    # ä¿å­˜åè‡ªåŠ¨åˆå§‹åŒ–å·¥ä½œæµ
    auto_init_workflow()

def main():
    print("\n" + "="*60 + "\nğŸš€ å…¨ç½‘é€‰é¢˜é›·è¾¾ v7.0 (ä»·å€¼æŒ–æ˜ç‰ˆ) - ç‹å¾€AI\n" + "="*60 + "\n")
    
    search_tool = WebSearchTool()
    
    # DeepSeek å»ºè®®ç›´è¿ï¼Œä¸èµ°ä»£ç† (é™¤é api.deepseek.com è¢«å¢™)
    # è¿™é‡Œæˆ‘ä»¬å°† proxy è®¾ä¸º Noneï¼Œç¡®ä¿å®ƒä¸èµ° PROXY_URL
    with httpx.Client(proxy=None, timeout=REQUEST_TIMEOUT) as http_client:
        client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL, http_client=http_client)
        
        # åŠ è½½å†å²è®°å½•ç”¨äºå»é‡
        history = load_history()
        history_text = "\n".join([f"- {h['date']}: {h['topic']} ({h['angle']})" for h in history])
        if not history_text: history_text = "æ— ï¼ˆè¿™æ˜¯ç¬¬ä¸€ç¯‡ï¼‰"
        
        # 1. å¹¿åŸŸæ‰«æ (Watchlist + Trend + Pain)
        search_plan = step1_broad_scan_and_plan(client, search_tool)
        
        # 2. æ·±åº¦éªŒè¯
        raw_data = step2_deep_scan(search_plan, search_tool)
        
        # 3. å†³ç­–ï¼ˆä¼ å…¥å†å²è®°å½•ç”¨äºå»é‡ï¼‰
        analysis = step3_final_decision(raw_data, client, history_text)
        
        # 4. ä¿å­˜
        save_report(raw_data, analysis)
    
    print("\nâœ… é€‰é¢˜é›·è¾¾å®Œæˆï¼")

def final_summary():
    """ç»¼åˆå½“å¤©æ‰€æœ‰æŠ¥å‘Šï¼Œç»™å‡ºæœ€ç»ˆé€‰é¢˜æ¨èå’Œä¸‰ä¸ªæç¤ºè¯"""
    import glob
    from config import get_today_dir
    
    print("\n" + "="*60)
    print("ğŸ¯ ç»¼åˆé€‰é¢˜å†³ç­– - æ•´åˆä»Šæ—¥æ‰€æœ‰æŠ¥å‘Š")
    print("="*60 + "\n")
    
    # 1. è¯»å–å½“å¤©æ‰€æœ‰æŠ¥å‘Š
    topics_dir = os.path.join(get_today_dir(), "1_topics")
    reports = glob.glob(os.path.join(topics_dir, "report_*.md"))
    
    if not reports:
        print("âŒ ä»Šæ—¥æš‚æ— æŠ¥å‘Šï¼Œè¯·å…ˆè¿è¡Œ `python run.py hunt`")
        return
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(reports)} ä»½æŠ¥å‘Šï¼š")
    all_content = []
    for r in sorted(reports):
        print(f"   ğŸ“„ {os.path.basename(r)}")
        with open(r, "r", encoding="utf-8") as f:
            all_content.append(f"=== {os.path.basename(r)} ===\n{f.read()}")
    
    combined = "\n\n".join(all_content)
    
    # 2. DeepSeek ç»¼åˆåˆ†æ
    print("\nğŸ§  DeepSeek æ­£åœ¨ç»¼åˆåˆ†æ...")
    
    FINAL_PROMPT = """
ä½ æ˜¯"ç‹å¾€AI"ï¼Œä¸€ä¸ªæ“…é•¿ä»å¤šä»½æƒ…æŠ¥ä¸­æç‚¼æ ¸å¿ƒé€‰é¢˜çš„å…¬ä¼—å·ä¸»ç¼–ã€‚

ä½ çš„ä»»åŠ¡ï¼šç»¼åˆåˆ†æä»Šå¤©çš„æ‰€æœ‰é€‰é¢˜æŠ¥å‘Šï¼Œé€‰å‡ºã€1ä¸ªæœ€ç»ˆé€‰é¢˜ã€‘ï¼Œå¹¶è¾“å‡º3ä¸ªç»“æ„åŒ–æç¤ºè¯ã€‚

## é€‰é¢˜æ ‡å‡† (æŒ‰ä¼˜å…ˆçº§)
1. **å‡ºç°é¢‘ç‡é«˜**ï¼šå¤šæ¬¡å‡ºç°çš„é€‰é¢˜è¯´æ˜çƒ­åº¦æŒç»­ï¼Œå€¼å¾—æ·±æŒ–
2. **è·å¾—æ„Ÿå¼º**ï¼šèƒ½è®©è¯»è€…"çœé’±ã€çœæ—¶ã€å­¦ä¼šæ–°æŠ€èƒ½"çš„é€‰é¢˜ä¼˜å…ˆ
3. **ç—›ç‚¹å°–é”**ï¼šè§£å†³çš„é—®é¢˜è¶Šå…·ä½“ã€è¶Šç—›ï¼Œè¶Šæœ‰çˆ†æ¬¾æ½œè´¨

## è¾“å‡ºæ ¼å¼

### ğŸ† ä»Šæ—¥æœ€ç»ˆé€‰é¢˜
**æ ‡é¢˜**ï¼š[çˆ†æ¬¾æ ‡é¢˜ï¼Œ15-25å­—]
**ä¸€å¥è¯å–ç‚¹**ï¼š[ç”¨æˆ·çœ‹å®Œèƒ½å¾—åˆ°ä»€ä¹ˆï¼Ÿ]
**å…³é”®è¯**ï¼š[3-5ä¸ªæœç´¢å…³é”®è¯ï¼Œç”¨äºåç»­ç´ ææœé›†]

### ğŸ“¡ æç¤ºè¯ 1ï¼šFast Research (ç”¨äº NotebookLM æœç´¢ç´ æ)
```
[è¯·ç”¨ä¸­æ–‡ï¼Œå‘Šè¯‰ NotebookLM éœ€è¦æœç´¢å“ªäº›å…·ä½“å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š
- å®˜æ–¹æ–‡æ¡£/æ•™ç¨‹
- ç”¨æˆ·çœŸå®è¯„ä»·/é¿å‘ç»éªŒ
- åŒç±»å·¥å…·å¯¹æ¯”
- æœ€æ–°æ›´æ–°/ç‰ˆæœ¬å˜åŒ–
æ ¼å¼è¦æ±‚ï¼šåˆ†æ¡åˆ—å‡ºï¼Œæ¯æ¡ä¸€ä¸ªæ˜ç¡®çš„æœç´¢ä»»åŠ¡]
```

### ğŸ¨ æç¤ºè¯ 2ï¼šè§†è§‰è„šæœ¬ (ç”¨äºé…å›¾æ–¹æ¡ˆ)
**ä½¿ç”¨æ–¹æ³•**ï¼šå¤åˆ¶åˆ° NotebookLM Chatï¼Œç„¶åç‚¹å‡»å³ä¾§ Studio â†’ **Infographic** ç”Ÿæˆä¿¡æ¯å›¾
```
[è¯·ç”¨ä¸­æ–‡ï¼Œå»ºè®®éœ€è¦å‡†å¤‡çš„é…å›¾ï¼ŒåŒ…æ‹¬ï¼š
- å…³é”®æˆªå›¾ (å“ªä¸ªç•Œé¢ã€å“ªä¸ªæ­¥éª¤)
- å¯¹æ¯”å›¾ (ä»€ä¹ˆ vs ä»€ä¹ˆ)
- æµç¨‹å›¾ (å¦‚æœæœ‰å¤æ‚æµç¨‹)
- å°é¢å›¾é£æ ¼å»ºè®®
- ä¿¡æ¯å›¾è¦ç‚¹ (é€‚åˆç”¨ Infographic ç”Ÿæˆçš„æ•°æ®/å¯¹æ¯”)]
```

### ğŸ¨ è§†è§‰é…å›¾æŒ‡å— (Visual Guide)
**è¯´æ˜**ï¼šè¯·ä¸ºäººå·¥é…å›¾æä¾›è¯¦ç»†çš„ç”»é¢å»ºè®®ï¼Œå¸®åŠ©åšä¸»å¿«é€Ÿäº§å‡ºé«˜è´¨é‡ç´ æã€‚
[è¯·ç”¨ä¸­æ–‡åˆ—å‡ºä¸å°‘äº 3 å¼ å…³é”®é…å›¾çš„å»ºè®®ï¼š

å°é¢å›¾ï¼š[ç”»é¢æè¿°ï¼Œå¦‚ï¼šDeepSeek Logo ä¸ Excel å›¾æ ‡å¯¹æ’ï¼Œç§‘æŠ€æ„Ÿï¼Œæ©™è“é…è‰²]

ç—›ç‚¹å›¾ï¼š[æè¿°ä¸€å¼ èƒ½å±•ç¤º"æ—§æ–¹æ³•å¾ˆéº»çƒ¦"çš„æˆªå›¾æˆ–æ¢—å›¾]

æ•ˆæœå›¾ï¼š[æè¿°ä¸€å¼ å±•ç¤º"æ–°æ–¹æ³•å¤ªçˆ½äº†"çš„å¯¹æ¯”å›¾æˆ–æœ€ç»ˆæ•ˆæœ]

ä¿¡æ¯å›¾/æµç¨‹å›¾ï¼š[å¦‚æœæœ‰å¤æ‚æ­¥éª¤ï¼Œå»ºè®®ç”»ä¸€å¼ ä»€ä¹ˆæ ·çš„æµç¨‹å›¾] ]


"""

    with httpx.Client(proxy=None, timeout=REQUEST_TIMEOUT) as http_client:
        client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL, http_client=http_client)
        
        try:
            response = client.chat.completions.create(
                model="deepseek-reasoner",
                messages=[
                    {"role": "system", "content": FINAL_PROMPT},
                    {"role": "user", "content": f"ä»¥ä¸‹æ˜¯ä»Šæ—¥çš„æ‰€æœ‰é€‰é¢˜æŠ¥å‘Šï¼Œè¯·ç»¼åˆåˆ†æåç»™å‡ºæœ€ç»ˆæ¨èï¼š\n\n{combined}"}
                ],
                stream=True
            )
            
            print("\n" + "="*60)
            print("ğŸ† æœ€ç»ˆé€‰é¢˜æ¨è")
            print("="*60 + "\n")
            
            collected = []
            for chunk in response:
                if chunk.choices[0].delta.content:
                    c = chunk.choices[0].delta.content
                    print(c, end="", flush=True)
                    collected.append(c)
            
            # ä¿å­˜ç»¼åˆæŠ¥å‘Š
            final_report = os.path.join(topics_dir, "FINAL_DECISION.md")
            content_str = ''.join(collected)
            with open(final_report, "w", encoding="utf-8") as f:
                f.write(f"# ğŸ† ä»Šæ—¥æœ€ç»ˆé€‰é¢˜å†³ç­–\n\n**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n**ç»¼åˆæŠ¥å‘Šæ•°**: {len(reports)}\n\n{content_str}")
            
            print(f"\n\nğŸ“ ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: {final_report}")

            # === è‡ªåŠ¨æ›´æ–°å†å²è®°å½• (Memory Update) ===
            try:
                import re
                # ä¼˜åŒ–æ­£åˆ™ï¼šå…¼å®¹ä¸­è‹±æ–‡å†’å·ã€å¿½ç•¥å‰åç©ºæ ¼ã€å¤šè¡ŒåŒ¹é…
                # æ¨¡å¼1: **æ ‡é¢˜**: xxx
                title_pattern1 = r'\*\*æ ‡é¢˜\*\*\s*[:ï¼š]\s*(.+)'
                # æ¨¡å¼2: ### é€‰é¢˜ 1ï¼šxxx
                title_pattern2 = r'###\s*é€‰é¢˜\s*\d+\s*[:ï¼š]\s*(.+)'
                
                final_topic = None
                
                # å°è¯•åŒ¹é…
                match1 = re.search(title_pattern1, content_str)
                if match1:
                    final_topic = match1.group(1).strip()
                else:
                    match2 = re.search(title_pattern2, content_str)
                    if match2:
                        final_topic = match2.group(1).strip()
                
                if final_topic:
                    save_topic_to_history(final_topic, "ç»¼åˆå†³ç­–")
                else:
                    # Fallback: å°è¯•æå–ç¬¬ä¸€è¡Œæœ‰æ•ˆæ–‡æœ¬
                    lines = [l.strip() for l in content_str.split('\n') if l.strip() and not l.startswith('#')]
                    if lines:
                        fallback_title = lines[0][:50]  # å–å‰50å­—ç¬¦
                        save_topic_to_history(fallback_title, "ç»¼åˆå†³ç­–")
                        print(f"âš ï¸ ä½¿ç”¨ Fallback æ ‡é¢˜: {fallback_title}")
                    else:
                        print("âš ï¸ è­¦å‘Š: æ— æ³•ä»æŠ¥å‘Šä¸­æå–æœ€ç»ˆé€‰é¢˜æ ‡é¢˜ï¼Œå†å²è®°å½•æœªæ›´æ–°ã€‚")
                        print(f"   è°ƒè¯•ä¿¡æ¯: å†…å®¹å‰200å­— -> {content_str[:200].replace(chr(10), ' ')}")
            
            except Exception as e:
                 print(f"âš ï¸ å†å²è®°å½•æ›´æ–°å¤±è´¥: {e}")
            
        except Exception as e:
            print(f"âŒ ç»¼åˆåˆ†æå¤±è´¥: {e}")

    print("\nâœ… ç»¼åˆé€‰é¢˜å®Œæˆï¼")

if __name__ == "__main__":
    main()
